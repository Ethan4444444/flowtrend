# Ingredients for building the `flowmix_tf()` method

\def\E{\mathbb{E}}
\def\R{\mathbb{R}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}


## Trend filtering, briefly

Trend filtering \citep{steidl2006splines, kim2009ell_1} is a tool for
non-parametric regression on a sequence of output points $y = (y_1,..., y_T)$
observed at locations $x = (x_1, ..., x_T)$. It is usually assumed that $x_1,
..., x_T$ are evenly spaced points, though this can be relaxed. The trend
filtering estimate of order $l$ of the time series $\mu_t = \E(y_t), t \in x$ is
obtained by calculating $$\hat{\mu} = \argmin_{\mu \in \mathbb{R}^T}
\frac{1}{2}\| \mu - y\|_2^2 + \lambda \| D^{(l+1)} \mu\|_1$$, where $\lambda$ is
a tuning parameter and $D^{(l+1)} \in \R^{T-l}$ is the $(l+1)^\text{th}$ order
discrete differencing matrix. 

## Differencing matrix

We first need to be able to construct the trend filtering "differencing matrix"
used for smoothing the mixture parameters over time. The general idea of the
trend filtering is explained masterfully in [Ryan's paper, Section 6 and
equation (41)].

The differencing matrix is formed by recursion, starting with $D^{(1)}$.

\begin{equation*}
    D^{(1)} = 
    \begin{bmatrix}
    -1 & 1 & 0 & \cdots & 0 & 0 \\
    0 & -1 & 1 & \cdots & 0  & 0\\
    \vdots & & & & \\
    0 & 0 & 0 & \cdots & -1 & 1
    \end{bmatrix}.
\end{equation*}

For $l>1$, the differencing matrox $D^{(l+1)}$ is defined recursively as
$D^{(l+1)} = D^{(1)} D^{(l)}$, starting with $D^{(1)}$.

```{r}
#' Generating Difference Matrix of Specified Order
#'
#' @param n length of vector to be differenced
#' @param l order of differencing
#' @param x optional. Spacing of input points.
#'
#' @return A n by n-l-1 matrix
#' @export
#'
#' @examples
gen_diff_mat <- function(n, l, x = NULL){

  ## Basic check
  if(!is.null(x))  stopifnot(length(x) == n)

  get_D1 <- function(t) {do.call(rbind, lapply(1:(t-1), FUN = function(x){
    v <- rep(0, t)
    v[x] <- -1
    v[x+1] <- 1
    v
  }))}

  if(is.null(x)){
    if(l == 0){
      return(diag(rep(1,n)))
    }
    if(l == 1){
      return(get_D1(n))
    }
    if(l > 1){
      D <- get_D1(n)
      for(k in 1:(l-1)){
        D <- get_D1(n-k) %*% D
      }
      return(D)
    }
  }
  else{
    if(l == 0){
      return(diag(rep(1,n)))
    }
    if(l == 1){
      return(get_D1(n))
    }
    if(l > 1){
      D <- get_D1(n)
      for(k in 1:(l-1)){
        D <- get_D1(n-k) %*% diag(k/diff(x, lag = k)) %*% D
      }
      return(D)
    }
  }
}
```

For equally spaced inputs:

```{r, eval = TRUE}
devtools::load_all("~/repos/flowsmooth/flowsmooth")
TT = 10

l = 1
Dl = gen_diff_mat(n = 10, l = l+1, x = NULL)
print(Dl)

l = 1
Dl = gen_diff_mat(n = 10, l = l+1, x = NULL)
print(Dl)

l = 2
Dl = gen_diff_mat(n = 10, l = l+1, x = NULL)
print(Dl)
```

When the inputs have a gap in it:

```{r, eval = FALSE}
x = (1:10)[-(4:6)]
l = 2
TT = length(x)
Dl = gen_diff_mat(n = TT, l = l+1, x = x)
print(Dl)
```


Next, here's a function to build a lasso regressor matrix $H$ that can be used
to solve an equivalent problem as the trend filtering of the \code{k}'th degree.
(This is stated in Lemma 4, equation (25) from Tibshirani (2014))

```{r}
#' A lasso regressor matrix H that can be used to solve an equivalent problem as the trend filtering of the \code{k}'th degree.
#'
#' @param n Total number of time points.
#' @param k Degree of trend filtering for cluster probabilities.
#' @param x Time points
#'
#' @return $n$ by $n$ matrix.
#' 
#' @export
gen_tf_mat <- function(n, k, x = NULL){

  if(is.null(x) ){
    x = (1:n)/n
  }
  if(!is.null(x)){
    stopifnot(length(x) == n)
  }

  ## For every i,j'th entry, use this helper function (from eq 25 of Tibshirani
  ## (2014)).
  gen_ij <- function(i,j){
    xi <- x[i]
    if(j %in% 1:(k+1)){
      return(xi^(j-1))
    }
    if(j %in% (k+2):n){
      return(prod(xi - x[(j-k):(j-1)]) * ifelse(xi >= x[(j-1)], 1, 0))
    }
  }

  ## Generate the H matrix, entry-wise.
  H <- matrix(nrow = n, ncol = n)
  for(i in 1:n){
    for(j in 1:n){
      H[i,j] <- gen_ij(i,j)
    }
  }
  return(H)
}
```

Here's a simple test.

```{r}
testthat::test_that("Trend filtering regression matrix is created correctly.", {
  H1 <- gen_tf_mat(10, 1)
  H2 <- gen_tf_mat(10, 1, x=(1:10)/10)
  testthat::expect_equal(H1, H2)
  testthat::expect_equal(dim(H1), c(10,10))
})
```


## Objective (data log-likelihood)

First, `loglik_tt()` calculates the log-likelihood of one cytogram, which is:

$$\sum_{i=1}^{n_t} C_i^{(t)} \log\left( \sum_{k=1}^K \pi_{itk} \phi(y_i^{(t)}; \mu_{kt}, \Sigma_k) \right) $$

```{r}
#' @param mu Cluster means.
#' @param prob Cluster probabilities.
#' @param prob Cluster variances.
#' @param ylist Data.
#' @param tt Time point.
loglik_tt <- function(ylist, tt, mu, sigma, prob, dimdat = NULL, countslist = NULL, numclust){

  ## One particle's log likelihood (weighted density)
  weighted.densities = sapply(1:numclust, function(iclust){
    if(dimdat == 1){
      return(prob[tt,iclust] * dnorm(ylist[[tt]], mu[tt,,iclust], sqrt(sigma[iclust,,])))
    }
    if(dimdat > 1){
    return(prob[tt,iclust] * dmvnorm_arma_fast(ylist[[tt]], mu[tt,,iclust], as.matrix(sigma[iclust,,]), FALSE))
    }
  })
  nt = nrow(ylist[[tt]])
  counts = (if(!is.null(countslist)) countslist[[tt]] else rep(1, nt))

  sum_wt_dens = rowSums(weighted.densities)
  sum_wt_dens = sum_wt_dens %>% pmax(1E-100)

  return(sum(log(sum_wt_dens) * counts))
}
```

Next, here is the function that calculates the entire objective from all
cytograms, given model parameter `mu`, `prob`, and `sigma`.

```{r}
#' Evaluating the penalized data log-likelihood on all data \code{ylist} given parameters \code{mu}, \code{prob}, and \code{sigma}.
#'
#' @param mu
#' @param prob
#' @param prob_link
#' @param sigma
#' @param ylist
#' @param Dl
#' @param l
#' @param lambda
#' @param l_prob
#' @param Dl_prob
#' @param lambda_prob
#' @param alpha
#' @param beta
#' @param denslist_by_clust
#' @param countslist
#' @param unpenalized if TRUE, return the unpenalized out-of-sample fit.
#'
#' @return
#' @export
#'
#' @examples
objective <- function(mu, prob, prob_link = NULL, sigma,
                      ## TT, N, dimdat, numclust,
                      ylist,
                      Dl, l = NULL,
                      lambda = 0,
                      l_prob = NULL,
                      Dl_prob = NULL,
                      lambda_prob = 0,
                      alpha = NULL, beta = NULL,
                      denslist_by_clust = NULL,
                      countslist = NULL,
                      unpenalized = FALSE){

  ## Set some important variables
  TT = dim(mu)[1]
  numclust = dim(mu)[3]
  if(is.null(countslist)){
    ntlist = sapply(ylist, nrow)
  } else {
    ntlist = sapply(countslist, sum)
  }
  N = sum(ntlist)
  dimdat = ncol(ylist[[1]])

  ## Calculate the log likelihood
  loglik = sapply(1:TT, function(tt){
    if(is.null(denslist_by_clust)){
      return(loglik_tt(ylist, tt, mu, sigma, prob, countslist, numclust = numclust, dimdat = dimdat))
    } else {
      return(loglik_tt_precalculate(ylist, tt, denslist_by_clust, prob, countslist, numclust))
      ## TODO: This doesn't exist yet, but might need to, since.. speed!
    }
  })

  if(unpenalized){
    obj =  -1/N * sum(unlist(loglik)) 
    return(obj)
  } else {

    ## Return penalized likelihood
    mu.splt <- asplit(mu, MARGIN = 3)
    diff_mu <- sum(unlist(lapply(mu.splt, FUN = function(m) sum(abs(Dl %*% m)))))
    ## diff_prob <- sum(abs(Dl_prob %*% log(prob * sapply(countslist, sum))))
    diff_prob <- sum(abs(Dl_prob %*% prob_link))
    obj =  -1/N * sum(unlist(loglik)) + lambda * diff_mu + lambda_prob * diff_prob
    return(obj)
  }
}

```

Here's a helper to check numerical convergence of the EM algorithm.

```{r}
#' Checks numerical improvement in objective value. Returns TRUE if |old-new|/|old| is smaller than tol.
#'
#' @param old Objective value from previous iteration.
#' @param new Objective value from current iteration.
#' @param tol Numerical tolerance.
check_converge_rel <- function(old, new, tol=1E-6){
  return(abs((old-new)/old) < tol )
}
```

Here's also a helper function to do the softmax-ing of $\alpha_t \in
\mathbb{R}^K$.

```{r softmax}
#' Converts the Xbeta to softmax(Xbeta), so to speak. Xbeta is the linear functional of X from a multinomial regression; in our notation, it's alpha.
#' 
#' @param prob_link alpha, which is a (T x K) matrix.
#' 
#' @return exp(alpha)/rowSum(exp(alpha)). A (T x K) matrix.
softmax <- function(prob_link){
  exp_prob_link = exp(prob_link)
  prob = exp_prob_link / rowSums(exp_prob_link)
}

testthat::test_that("Test for softmax",{
  link = runif(100, min = -10, max = 10) %>% matrix(nrow = 10, ncol = 10)
  testthat::expect_true(all(abs(rowSums(softmax(link)) - 1) < 1E-13))
})
```


## Initial parameter values for EM algorithm

The EM algorithm requires some initial values for $\mu$, $\pi$ and $\Sigma$. 

Initializing $\pi$ is one line.  `prob = matrix(1/numclust, nrow = TT, ncol =
numclust)`, which sets everything to $1/K$.
  
For $\mu$ and $\Sigma$, we write some functions. Essentially, initial means
$\mu$ are *jittered* versions of a $K$ means that are drawn from a downsampled
version of $ylist$ (downsampling is done because $ylist$ can have a large number
of particles). $\Sigma$ is $d\times d$ identity matrices, with `fac=1` diagonal
by default. 

TODO: better downsampling for binned data.
TODO: think hard about best way to design sigma
  * TODO: this could be a good student project; best initialization
    techniques. How much does it take to get to the optimum?

```{r init}
#' Initialize the cluster centers.
#'
#' @param ylist  A T-length list of (nt  by 3) datasets.  There should  be T of
#'   such datasets. 3 is actually \code{mulen}.
#' @param numclust Number of clusters (M).
#' @param TT total number of (training) time points.
#'
#' @return An array of dimension (T x dimdat x M).
#' @export
init_mn <- function(ylist, numclust, TT, dimdat, countslist = NULL, seed=NULL){

  if(!is.null(seed)){
    assertthat::assert_that(all((seed %>% sapply(., class)) == "integer"))
    assertthat::assert_that(length(seed) == 7)
    .Random.seed <<- seed
  }

  if(!is.null(countslist)){

    ## Initialize the means by (1) collapsing to one cytogram (2) random
    ## sampling from this distribution, after truncation,
    TT = length(ylist)
    ylist_downsampled <- lapply(1:TT, function(tt){

      y = ylist[[tt]]
      counts = countslist[[tt]]

      ## Sample so that, in total, we get mean(nt)*30 sized sample. In the case
      ## of binned data, nt is the number of bins.
      if(nrow(y) > 500) nsize = nrow(y) / TT * 30 else nsize = nrow(y)
      some_rows = sample(1:nrow(y), size = nsize, prob = counts/sum(counts))
      y[some_rows,, drop=FALSE]
    })

    yy = do.call(rbind, ylist_downsampled)
    new_means = yy[sample(1:nrow(yy), numclust),, drop=FALSE]
    jitter_sd = apply(yy, 2, sd) / 100
    jitter_means = MASS::mvrnorm(n = nrow(new_means),
                                 mu = rep(0, dimdat),
                                 Sigma = diag(jitter_sd, ncol = dimdat))
    new_means = new_means + jitter_means

    ## Repeat TT times.
    mulist = lapply(1:TT, function(tt){ new_means })

  } else {

    TT = length(ylist)
    ylist_downsampled <- lapply(1:TT, function(tt){
      y = ylist[[tt]]
      counts = countslist[[tt]]
      nsize = pmin(nrow(y) / TT * 30, nrow(y))
      y[sample(1:nrow(y), size = nsize),, drop=FALSE]
    })

    ## Combine all the particles
    yy = do.call(rbind, ylist_downsampled)

    ## Get K new means from these
    inds = sample(1:nrow(yy), numclust)
    new_means = yy[inds,, drop=FALSE]
    mulist = lapply(1:TT, function(tt){ new_means })

  }

  ## New (T x dimdat x numclust) array is created.
  muarray = array(NA, dim=c(TT, dimdat, numclust))
  for(tt in 1:TT){
    muarray[tt,,] = as.matrix(mulist[[tt]])
  }
  return(muarray)
}


#' Initialize the covariances.
#'
#' @param data The (nt by 3) datasets. There should be T of them.
#' @param numclust Number of clusters.
#' @param fac Value to use for the diagonal of the (dimdat x dimdat) covariance
#'   matrix.
#'
#' @return An (K x dimdat x dimdat) array containing the (dimdat by dimdat)
#'   covariances.
#' @export
init_sigma <- function(data, numclust, fac = 1){

  ndat = nrow(data[[1]])
  pdat = ncol(data[[1]])
  sigmas = lapply(1:numclust, function(iclust){
    onesigma = diag(fac * rep(1, pdat))
    if(pdat==1) onesigma = as.matrix(fac)
    colnames(onesigma) = paste0("datcol", 1:pdat)
    rownames(onesigma) = paste0("datcol", 1:pdat)
    return(onesigma)
  })
  sigmas = abind::abind(sigmas, along=0)
  return(sigmas)
}
```

Here's a helper function for printing the progress.

```{r print_progress}
#' A helper function to print the progress of a loop or simulation.
#'
#' @param isim Replicate number.
#' @param nsim Total number of replicates.
#' @param type Type of job you're running. Defaults to "simulation".
#' @param lapsetime Lapsed time, in seconds (by default).
#' @param lapsetimeunit "second".
#' @param start.time start time.
#' @param fill Whether or not to fill the line.
#'
#' @return No return
print_progress <- function(isim, nsim,
                           type = "simulation", lapsetime = NULL,
                           lapsetimeunit = "seconds", start.time = NULL,
                           fill = FALSE){

  ## If lapse time is present, then use it
  if(fill) cat(fill = TRUE)
  if(is.null(lapsetime) & is.null(start.time)){
    cat("\r", type, " ", isim, "out of", nsim)
  } else {
    if(!is.null(start.time)){
      lapsetime = round(difftime(Sys.time(), start.time,
                                 units = "secs"), 0)
      remainingtime = round(lapsetime * (nsim-isim)/isim,0)
      endtime = Sys.time() + remainingtime
    }
    cat("\r", type, " ", isim, "out of", nsim, "with lapsed time",
        lapsetime, lapsetimeunit, "and remaining time", remainingtime,
        lapsetimeunit, "and will finish at", strftime(endtime))
  }
  if(fill) cat(fill = TRUE)
}
```

# E step

```{r Estep}
#' E step, which updates the "responsibilities", which are posterior membership probabilities of each particle.
#'
#' @param mn
#' @param sigma
#' @param prob
#' @param ylist
#' @param numclust
#' @param denslist_by_clust
#' @param first_iter
#' @param countslist
#'
#' @return
#' @export
#'
Estep <- function(mn, sigma, prob, ylist = NULL, numclust, denslist_by_clust = NULL,
                  first_iter = FALSE, countslist = NULL){
  ## Basic setup
  TT = length(ylist)
  ntlist = sapply(ylist, nrow)
  resp = list()
  dimdat = dim(mn)[2]
  assertthat::assert_that(dim(mn)[1] == length(ylist))

  ## Helper to calculate Gaussian density for each \code{N(y_{t,k},mu_{t,k} and
  ## Sigma_k)}.
  calculate_dens <- function(iclust, tt, y, mn, sigma, denslist_by_clust,
                             first_iter) {
    mu <- mn[tt, , iclust]
    if (dimdat == 1) {
      dens = dnorm(y, mu, sd = sqrt(sigma[iclust, , ]))
    } else {
       dens = dmvnorm_arma_fast(y, mu, sigma[iclust,,], FALSE)
    }
    return(dens)
  }

  ## Calculate posterior probability of membership of $y_{it}$.
  ncol.prob = ncol(prob)
  for (tt in 1:TT) {
    ylist_tt = ylist[[tt]]
    densmat <- sapply(1:numclust, calculate_dens, tt, ylist_tt,
                      mn, sigma, denslist_by_clust, first_iter)
    wt.densmat <- matrix(prob[tt, ], nrow = ntlist[tt],
                         ncol = ncol.prob, byrow = TRUE) * densmat
    wt.densmat = wt.densmat + 1e-10
    wt.densmat <- wt.densmat/rowSums(wt.densmat)
    resp[[tt]] <- wt.densmat
  }

  ## Weight the responsibilities by $C_{it}$.
  if (!is.null(countslist)) {
    resp <- Map(function(myresp, mycount) {
      myresp * mycount
    }, resp, countslist)
  }
  return(resp)
}
```

The E step should return a list of exactly the same size and format as `ylist`,
which is a $T$ -length list of matrices of size $n_t \times d$.

```{r test-Estep, eval = FALSE}
testthat::test_that("E step returns appropriately sized responsibilities.",{
  
  ## Generate some fake data
  TT = 100
  ylist = lapply(1:TT, function(tt){ runif(90) %>% matrix(ncol = 3, nrow = 30)})
  numclust = 3
  dimdat = 3

  ## Initialize a few parameters, not carefully
  sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat))
  mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist)
  prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K.

  ## Calculate responsibility
  ## TODO: the code fails here. why?
  resp = Estep(mn = mn, sigma = sigma, prob = prob, ylist = ylist, numclust = numclust)

  ## Check these things
  testthat::expect_equal(length(resp), length(ylist))
  testthat::expect_equal(sapply(resp, dim), sapply(ylist, dim))
})
```

# M step 

The M step of the EM algorithm has three steps -- one each for $\mu$, $\pi$, and
$\Sigma$.

## M step for $\pi$

```{r Mstep_prob}
#' The M step for the cluster probabilities
#'
#' @param resp Responsibilities.
#' @param H_tf Trend filtering matrix.
#' @param countslist Particle multiplicities.
#' @param lambda_prob Regularization. 
#' @param l_prob Trend filtering degree.
#'
#' @return (T x k) matrix containing the alphas, for \code{prob = exp(alpha)/
#'         rowSums(exp(alpha))}.
#' @export
#'
Mstep_prob <- function(resp, H_tf, countslist = NULL,
                       lambda_prob = NULL, l_prob = NULL, x = NULL){

  ## Basic setup
  TT <- length(resp)

  ## Basic checks
  stopifnot(is.null(l_prob) == is.null(lambda_prob))

  ## If glmnet isn't actually needed, don't use it.
  if(is.null(l_prob) & is.null(lambda_prob)){

    ## Calculate the average responsibilities, per time point.
    if(is.null(countslist)){
      resp.avg <- lapply(resp, colMeans) %>% do.call(rbind, .)
    } else {
      resp.avg <- lapply(1:TT, FUN = function(ii){
        colSums(resp[[ii]])/sum(countslist[[ii]])
      }) %>% do.call(rbind, .)
    }
    return(resp.avg) 

  ## If glmnet is actually needed, use it.
  } else {
    penalty.facs <- c(rep(0, l_prob+1), rep(1, nrow(H_tf) - l_prob - 1))
    resp.predict <- do.call(rbind, lapply(resp, colSums))
    glmnet_obj <- glmnet(x = H_tf, y = resp.predict, family = "multinomial",
                         penalty.factor = penalty.facs, maxit = 1e7,
                         lambda =  mean(penalty.facs)*lambda_range(lambda_prob),
                         standardize = F, intercept = FALSE) ## todo: replicate the parameters.
    pred_link <- predict(glmnet_obj, newx = H_tf, type = "link", s = mean(penalty.facs)*lambda_prob)[,,1]
    return(pred_link)
  }
}
```

This should return a $T$ by $K$ matrix, which we'll test here:

```{r test-Mstep_prob, eval = FALSE}
testthat::test_that("Mstep of pi returns a (T x K) matrix.", {

  ## Generate some fake responsibilities and trend filtering matrix
  TT = 100
  numclust = 3
  nt = 10
  resp = lapply(1:TT, function(tt){
    oneresp = runif(nt*numclust) %>% matrix(ncol=numclust)
    oneresp = oneresp/rowSums(oneresp)
  })
  H_tf <- gen_tf_mat(n = TT, k = 0)

  ## Check the size
  pred_link = Mstep_prob(resp, H_tf, l_prob = 0, lambda_prob = 1E-3)
  testthat::expect_equal(dim(pred_link), c(TT, numclust))
  pred_link = Mstep_prob(resp, H_tf)
  testthat::expect_equal(dim(pred_link), c(TT, numclust))

  ## Check the correctness
  pred_link = Mstep_prob(resp, H_tf)
})
```

Each row of this matrix should contain the fitted values $\alpha_k \in
\mathbb{R}^3$ where $\alpha_{kt} = h_t^T w_{k}$, for..

- $h_t$ that are rows of the trend filtering matrix $H \in \mathbb{R}^{T \times
  T}$.

- $w_k \in \mathbb{R}^{n}$ that are the regression coefficients estimated by
  `glmnet()`.


Here is a test for the correctness of the M step for $\pi$.

```{r test-correctness-mstep-prob, eval = FALSE}
testthat::test_that("Test the M step of \pi against CVXR", {})
```


## M step for $\Sigma$

```{r Mstep_sigma}
#' M step for cluster covariance (sigma).
#'
#' @param resp Responsibility.
#' @param ylist Data.
#' @param mn Means
#' @param numclust Number of clusters.
#'
#' @return (K x d x d) array containing K (d x d) covariance matrices.
#' @export
#'
#' @examples
Mstep_sigma <- function(resp, ylist, mn, numclust){

  ## Find some sizes
  TT = length(ylist)
  ntlist = sapply(ylist, nrow)
  dimdat = ncol(ylist[[1]])
  cs = c(0, cumsum(ntlist))

  ## Set up empty residual matrix (to be reused)
  cs = c(0, cumsum(ntlist))
  vars <- vector(mode = "list", numclust)
  ylong = do.call(rbind, ylist)
  ntlist = sapply(ylist, nrow)
  irows = rep(1:nrow(mn), times = ntlist)

  for(iclust in 1:numclust){
    resp.thisclust = lapply(resp, function(myresp) myresp[,iclust, drop = TRUE])
    resp.long = do.call(c, resp.thisclust)
    mnlong = mn[irows,,iclust]
    if(is.vector(mnlong)) mnlong = mnlong %>% cbind()
    ## browser()
    ## vars[[iclust]] = estepC(ylong, mnlong, sqrt(resp.long), sum(resp.long))
    ## TODO: see if this could be sped up.
    resid <- ylong - mnlong
    resid_weighted <- resp.long * resid
    sig_temp <- t(resid_weighted) %*% resid/sum(resp.long)
    vars[[iclust]] <- sig_temp
  }

  ## Make into an array
  sigma_array = array(NA, dim=c(numclust, dimdat, dimdat))
  for(iclust in 1:numclust){
    sigma_array[iclust,,] = vars[[iclust]]
  }

  ## Basic check
  stopifnot(all(dim(sigma_array) == c(numclust, dimdat, dimdat)))
  return(sigma_array)
}
```


## M step for $\mu$

This is a big one. It uses the ADMM written in section OO, reproduced briefly here.

We need a convergence checker for the outer layer of LA-ADMM:

```{r}
#' LA-ADMM requires an convergence check for the outer layer.
#' 
#' @param objectives Objectives of the outer layer.
#'
#' @return TRUE if relative improvement is smaller than 1E-5 in the last four
#'   outer iterations' objective.
#' @export
outer_converge <- function(objectives){
  consec = 4
  if(length(objectives) < consec){
    return(FALSE)
  } else {
    mytail = utils::tail(objectives, consec)
    rel_diffs = mytail[1:(consec-1)]/mytail[2:consec]
    return(all(abs(rel_diffs) - 1 < 1E-5))
  }
}
```

Next, we define the main function `Mstep_mu()`.


```{r mstep_admm_tf}
devtools::load_all("~/repos/FlowTF")
#' Computes the M step for mu. TODO: use templates for the argument. As shown
#' here:
#' https://stackoverflow.com/questions/15100129/using-roxygen2-template-tags
#' 
#' @param resp Responsbilities of each particle.
#' @param ylist 
#' @param lambda
#' @param l
#' @param sigma
#' @param sigma_eig_by_clust
#' @param Dlm1
#' @param Dl
#' @param TT
#' @param N
#' @param dimdat
#' @param first_iter
#' @param mus
#' @param Zs
#' @param Ws
#' @param uws
#' @param uzs
#' @param maxdev
#' @param x
#' @param niter
#' @param err_rel
#' @param err_abs
#' @param zerothresh
#' @param local_adapt
#' @param local_adapt_niter
#' @param space
#'
#' @return
#' @export
#'
#' @examples
Mstep_mu <- function(resp,
                     ylist,
                     lambda = 0.5,
                     l = 3,
                     sigma,
                     sigma_eig_by_clust = NULL,
                     Dlm1sqrd,
                     Dlm1, Dl,  TT, N, dimdat,
                     first_iter = TRUE,
                     e_mat,

                     ## Warm startable variables
                     mus = NULL,
                     Zs = NULL,
                     Ws = NULL,
                     uws = NULL,
                     uzs = NULL,
                     ## End of warm startable variables

                     maxdev = NULL,
                     x = NULL,
                     niter = (if(local_adapt) 1e3 else 1e4),
                     err_rel = 1E-3,
                     err_abs = 0,
                     zerothresh = 1E-6,
                     local_adapt = FALSE,
                     local_adapt_niter = 10,
                     space = 50){

  ####################
  ## Preliminaries ###
  ####################
  TT = length(ylist)
  numclust = ncol(resp[[1]])
  dimdat = ncol(ylist[[1]])
  ntlist = sapply(ylist, nrow)
  resp.sum = lapply(resp, colSums) %>% do.call(rbind, .)
  N = sum(unlist(resp.sum)) ## NEW (make more efficient, later)

  # starting rho for LA-ADMM
  if(local_adapt){
    rho.init = 1e-3
  }
  else{
    if(!is.null(x)){
      rho.init = lambda*((max(x) - min(x))/length(x))^l
      #print(rho.init)
      rho.init = lambda
    }else{
    rho.init = lambda
    }
  }


  ## Other preliminaries
  schur_syl_A_by_clust = schur_syl_B_by_clust = term3list = list()
  ybarlist = list()
  ycentered_list = Xcentered_list = yXcentered_list = list()
  Qlist = list()
  sigmainv_list = list()
  convergences = list()
  for(iclust in 1:numclust){

    ## Retrieve sigma inverse from pre-computed SVD, if necessary
    if(is.null(sigma_eig_by_clust)){
      sigmainv = solve(sigma[iclust,,])
    } else {
      sigmainv = sigma_eig_by_clust[[iclust]]$sigma_inv
    }

    resp.iclust <- lapply(resp, FUN = function(r) matrix(r[,iclust]))

    ## Center y and X
   # obj <- weight_ylist(iclust, resp, resp.sum, ylist)
    #ycentered <- obj$ycentered

    ## Form the Sylvester equation coefficients in AX + XB + C = 0
    # syl_A = rho * sigma[iclust,,]
    # Q = 1/N * t(Xcentered) %*% D %*% Xcentered
    # syl_B = Q %*% Xinv

    AB <- get_AB_mats(y = y, resp = resp.iclust, Sigma_inv = sigmainv, e_mat = e_mat, N = N, Dl = Dl,
                      Dlm1 = Dlm1, Dlm1sqrd = Dlm1sqrd, rho = rho.init, z = NULL, w = NULL, uz = NULL, uw = NULL)

    ## Store the Schur decomposition
    schur_syl_A_by_clust[[iclust]] = myschur(AB$A)
    schur_syl_B_by_clust[[iclust]] = myschur(AB$B)

    ## Calculate coefficients for objective value  calculation
    # Qlist[[iclust]] = Q

    ## ## Also calculate some things for the objective value
    ## ylong = sweep(do.call(rbind, ylist), 2, obj$ybar)
    ## longwt = do.call(c, lapply(1:TT, function(tt){ resp[[tt]][,iclust]})) %>% sqrt()
    ## wt.long = longwt * ylong
    ## wt.ylong = longwt * ylong
    ## crossprod(wt.ylong, wt.ylong)

    ## Store the third term
    # term3list[[iclust]] =  1 / N * sigmainv %*% yXcentered
    # ybarlist[[iclust]] = obj$ybar
    #
    ycentered <- NULL
     ycentered_list[[iclust]] = ycentered
     #print(ycentered)
    # Xcentered_list[[iclust]] = Xcentered
    # yXcentered_list[[iclust]] = yXcentered
    sigmainv_list[[iclust]] = sigmainv
  }

  ##########################################
  ## Run ADMM separately on each cluster ##
  #########################################
  yhats = admm_niters = admm_inner_iters = vector(length = numclust, mode = "list")
  if(first_iter) mus = vector(length = numclust, mode = "list")
 # if(first_iter){
    Zs <- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat) )
    Ws <- lapply(1:numclust, function(x) matrix(0, nrow = dimdat, ncol = TT - l))
    uzs <- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat) )
    uws <- lapply(1:numclust, function(x) matrix(0, nrow = dimdat, ncol = TT - l))

   # Zs =  Ws =  Us  = vector(length = numclust, mode = "list")
 # }

  fits = matrix(NA, ncol = numclust, nrow = ceiling(niter / space))

  #browser()

  ## For every cluster, run LA-ADMM
  resid_mat_list = list()
  start.time = Sys.time()
  for(iclust in 1:numclust){
    resp.iclust <- lapply(resp, FUN = function(r) matrix(r[,iclust]))
    resp.sum.iclust <- lapply(resp.sum, FUN = function(r) matrix(r[iclust]))

    ## Possibly locally adaptive ADMM, for now just running with rho == lambda
    res = la_admm_oneclust(K = (if(local_adapt) local_adapt_niter else 1),
                           local_adapt = local_adapt,
                           iclust = iclust,
                           niter = niter,
                           TT = TT, N = N, dimdat = dimdat, maxdev = maxdev,
                           schurA = schur_syl_A_by_clust[[iclust]],
                           schurB = schur_syl_B_by_clust[[iclust]],
                           #term3 = term3list[[iclust]],
                           sigmainv = sigmainv_list[[iclust]],
                           # Xinv = Xinv,
                           # Xaug = Xaug,
                           # Xa = Xa,
                           rho = rho.init,
                           rhoinit = rho.init,
                           sigma = sigma,
                           lambda = lambda,
                           resp = resp.iclust,
                           l = l,
                           Dl = Dl,
                           Dlm1 = Dlm1,
                          #resp.sum = resp.sum.iclust,
                           y = ylist,
                           err_rel = err_rel,
                           err_abs = err_abs,
                           zerothresh = zerothresh,
                           sigma_eig_by_clust = sigma_eig_by_clust,
                           space = space,
                           objective = F, norms = F,

                           ## Warm starts from previous *EM* iteration
                           first_iter = first_iter,
                          # beta = betas[[iclust]],
                           #mu = mus[[iclust]],
                           uw = uws[[iclust]],
                           uz = uzs[[iclust]],
                           z = Zs[[iclust]],
                           w = Ws[[iclust]]
    )

    ## Store the results
    mus[[iclust]] = res$mu
    yhats[[iclust]] = t(res$yhat)
    ## fits[,iclust] = res$fits ## TODO: revive this, for testing? We'll see.
    admm_niters[[iclust]] = res$kk
    admm_inner_iters[[iclust]] = res$inner.iter

    ## Store other things for for warmstart
    Zs[[iclust]] = res$Z
    uzs[[iclust]] = res$uz
    uws[[iclust]] = res$uw
    Ws[[iclust]] = res$W

    ## The upper triangular matrix remains the same.
    ## The upper triangular matrix remains the same.

    resid_mat_list[[iclust]] = res$resid_mat ## temporary
    convergences[[iclust]] = res$converge
   # print(res$converge)
  }

  ## Aggregate the yhats into one array
  yhats_array = array(NA, dim = c(TT, dimdat, numclust))
  for(iclust in 1:numclust){ yhats_array[,,iclust] = yhats[[iclust]] }

  ## Each are lists of length |numclust|.
  return(list(mns = yhats_array,
              ## fits = fits,
              resid_mat_list = resid_mat_list,
              convergences = convergences,
              admm_niters = admm_niters, ## Temporary: Seeing the number of
              ## outer iterations it took to
              ## converge.
              admm_inner_iters = admm_inner_iters,

              ## For warmstarts
              Zs = Zs,
              Ws = Ws,
              uws = uws,
              uzs = uzs,
              N = N,

              ## For using in the Sigma M step
              ycentered_list = ycentered_list,
              Xcentered_list = Xcentered_list,
              yXcentered_list = yXcentered_list,
              Qlist = Qlist
  ))
}
```


TODO:

- Right now, `sigma_eig_by_clust` is not used. When speeding up the code, do
  this first.
- Likewise, `ycentered_list` isn't used, while it is clearly useful in Sigma M step

```{r Mstep_mu_cvxr}
#' Testing against \code{Mstep_mu()}, for ONE cluster.
#' @param ylist
#' @param resp
#' @param lambda
#' @param l
#' @param Sigma_inv inverse of Sigma
Mstep_mu_cvxr <- function(ylist,
                          resp,
                          lambda,
                          l,
                          Sigma_inv, 
                          thresh = 1E-8,
                          maxdev = NULL,
                          dimdat,
                          N,
                          ecos_thresh = 1E-8,
                          scs_eps = 1E-5){
  
  ## Define dimensions
  TT = length(ylist)
  
  ## Responsibility Weighted Data
  ytildes <- lapply(1:TT, FUN = function(tt){
    yy <- ylist[[tt]]
    g <- resp[[tt]]
    yy <- apply(yy, MARGIN = 2, FUN = function(x) x * g)
    colSums(yy)
  })  %>% bind_rows() %>% as.matrix()
  
  
  ## Auxiliary term, needed to make the objective interpretable
  aux.y <- Reduce("+", lapply(1:TT, FUN = function(tt){
    yy <- ylist[[tt]]
    g <- sqrt(resp[[tt]])
    yy <- apply(yy, MARGIN = 2, FUN = function(x) x * g)
    sum(diag(yy %*% Sigma_inv %*% t(yy)))
  }))
  
  ## Mu, d x T matrix
  mumat <- CVXR::Variable(cols=dimdat, rows=TT)
  
  ## Summed sqrt responsibilities - needed in the objective.
  resp.sum.sqrt <- lapply(resp, FUN = function(x) sqrt(sum(x)))
  
  ## Differencing Matrix, TT-l + 1 x TT 
  Dl <- gen_diff_mat(n = TT, l = l)
  
  ## Forming the objective
  obj = 1/(2*N) *( Reduce("+", lapply(1:TT, FUN = function(tt) CVXR::quad_form(resp.sum.sqrt[[tt]]*mumat[tt,], Sigma_inv))) -2 * Reduce("+", lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% mumat[tt,])) + aux.y) + lambda * sum(CVXR::sum_entries(abs(Dl %*% mumat), axis = 1))
  
  ## Putting together the ball constraint
  rowmns <- matrix(rep(1, TT^2), nrow = TT)/TT
  mu_dotdot <- rowmns %*% mumat
  constraints = list()
  if(!is.null(maxdev)){
    constraints = list(CVXR::sum_entries(CVXR::square(mumat - mu_dotdot), axis = 2) <= rep(maxdev^2, TT) )
  }
  
  ## Try all two CVXR solvers.
  prob <- CVXR::Problem(CVXR::Minimize(obj), constraints)
  result = NULL
  result <- tryCatch({
    CVXR::solve(prob, solver="ECOS",
          FEASTOL = ecos_thresh, RELTOL = ecos_thresh, ABSTOL = ecos_thresh)
  }, error=function(err){
    err$message = paste(err$message,
                        "\n", "Lasso solver using ECOS has failed." ,sep="")
    cat(err$message, fill=TRUE)
    return(NULL)
  })
  
  ## If anything is wrong, flag to use SCS solver
  scs = FALSE
  if(is.null(result)){
    scs = TRUE
  } else {
    if(result$status != "optimal") scs = TRUE
  }
  
  ## Use the SCS solver
  if(scs){
    result = CVXR::solve(prob, solver="SCS", eps = scs_eps)
    if(any(is.na(result$getValue(mumat)))){ ## A clumsy way to check.
      stop("Lasso solver using both ECOS and SCS has failed.", sep="")
    }
  }
  
  ## Record Interesting Parameters
  num_iters <- result$num_iters
  status <- result$status
  mumat <- result$getValue(mumat)
  val <- result$value
  
  return(list(mu = mumat, value = val, status = status, num_iters = num_iters))
}
```

This function solves the following problem:

\usepackage{amsmath}
\usepackage[long]{optidef}

$$
\begin{align*}
&\text{minimize}_{\mu}
{\frac{1}{2N}
    \sum_{t=1}^T \sum_{i=1}^{n_t} \hat{\gamma}_{it} (y_i^{(t)} - \mu_{\cdot t})^\top \hat{\Sigma}^{-1} ( y_i^{(t)} - \mu_{\cdot t})
  + \lambda \sum_{j=1}^d \|D^{(l)}\mu_{j\cdot }\|_1}\\
&\text{subject to}\;\; {\| \mu_{\cdot t} - \bar{\mu}_{\cdot \cdot}\|_2 \le r \;\;\forall t=1,\cdots, T, }
\end{align*}
$$


and is directly equivalent to `Mstep_mu()`. The resulting solution and the
objective value should be the same. Let's check that.

First, set up some objects to run `Mstep_mu()` and `Mstep_mu_cvxr()`.

```{r, eval = TRUE}
numclust = 3
TT = 100
dimdat = 1
set.seed(0)
dt = gendat_1d(TT = TT, ntlist = rep(TT, 100))
ylist = dt %>% dt2ylist()
sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat))
mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist)
prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K.
resp = Estep(mn, sigma, prob, ylist = ylist, numclust)
lambda = .01
l = 1
x = 1:TT
Dl = gen_diff_mat(n = TT, l = l+1, x = x)
Dlm1 = gen_diff_mat(n = TT, l = l, x = x)
Dlm1sqrd <- t(Dlm1) %*% Dlm1
maxdev = NULL
```

Then, compare the result of the two implementations. They should look identical.

```{r, eval = TRUE, fig.width = 7, fig.height = 5}
## overall ADMM
res1 = Mstep_mu(resp, ylist, lambda, l=l, sigma=sigma, Dlm1sqrd = Dlm1sqrd, Dlm1=Dlm1, Dl=Dl, TT=TT, N=N, dimdat=dimdat, e_mat=etilde_mat(TT = TT),
                maxdev = maxdev)
mn1 = res1$mns

## CVXR just ONE cluster
res2list = lapply(1:numclust, function(iclust){
  Sigma_inv_oneclust = solve(sigma[iclust,,])
  resp_oneclist = lapply(resp, function(resp_onetime){resp_onetime[,iclust, drop=FALSE]})
  N = sum(unlist(resp))
  res2 = Mstep_mu_cvxr(ylist, resp_oneclist, lambda, l+1,  Sigma_inv_oneclust, thresh = 1E-8, maxdev = maxdev, dimdat, N) 
  res2$mu
})
mn2 = array(NA, dim=c(100, dimdat, 3))
for(iclust in 1:numclust){
  mn2[,,iclust] = res2list[[iclust]] %>%  as.matrix()
}

  
## Plot!
plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = 'admm vs cvxr', ylab = "", xlab = "time")
mn1[,1,] %>% matlines(lwd = 1, lty = 1)
mn2[,1,] %>% matlines(lwd = 3, lty = 3)
```

TODO: We just need to bundle this into `testthat` style tests, and make sure to test several lambda values.

TODO: maybe do this for unevenly spaced inputs. CVXR needs to take a different D matrix.

```{r test-correctness-mstep-mu, eval = FALSE}
testthat::test_that("Test the M step of \mu against CVXR", {})
```



# `flowsmooth()`

Now we've assembled all ingredients we need, we'll build the main function
`flowsmooth_once()` to estimate a flowsmooth model. Here goes:

```{r flowsmooth_once}
#' Estimate flowsmooth model once.
#'
#' @param ylist Data.
#' @param countslist Counts corresponding to multiplicities.
#' @param x Times, if points are not evenly spaced. Defaults to NULL, in which
#'   case the value becomes \code{1:T}, for the $T==length(ylist)$.
#' @param numclust Number of clusters.
#' @param niter Maximum number of EM iterations.
#' @param l Degree of differencing for the mean trend filtering
#' @param l_prob Degree of differencing for the probability trend filtering
#' @param mn Initial value for cluster means. Defaults to NULL, in which case
#'   initial values are randomly chosen from the data.
#' @param lambda Smoothing parameter for means
#' @param lambda_prob Smoothing parameter for probabilities
#' @param verbose Loud or not? EM iteration progress is printed.
#' @param tol_em Relative numerical improvement of the objective value at which
#'   to stop the EM algorithm
#' @param maxdev Maximum deviation of cluster means across time..
#' @param countslist_overwrite
#' @param admm_err_rel
#' @param admm_err_abs
#' @param admm_local_adapt
#' @param admm_local_adapt_niter
#'
#' @return List object with flowsmooth model estimates.
#' @export
#'
#' @examples
flowsmooth_once <- function(ylist,
                       countslist = NULL,
                       x = NULL,
                       numclust, niter = 1000, l, l_prob = NULL,
                       mn = NULL, lambda = 0, lambda_prob = NULL, verbose = FALSE,
                       tol_em = 1E-4,
                       maxdev = NULL,
                       countslist_overwrite = NULL,
                       ## beta Mstep (ADMM) settings
                       admm = TRUE,
                       admm_err_rel = 1E-3,
                       admm_err_abs = 1E-4,
                       ## Mean M step (Locally Adaptive ADMM) settings
                       admm_local_adapt = FALSE,
                       admm_local_adapt_niter = if(admm_local_adapt) 10 else 1){

  ## Basic checks
  if(!is.null(maxdev)){
    assertthat::assert_that(maxdev!=0)
  } else {
    maxdev = 1E10
  }
  assertthat::assert_that(numclust > 1)
  assertthat::assert_that(niter > 1)
  if(is.null(countslist)){
    ntlist = sapply(ylist, nrow)
    countslist = lapply(ntlist, FUN = function(nt) rep(1, nt))
  }

  ## Setup for EM algorithm
  TT = length(ylist)
  dimdat = ncol(ylist[[1]])
  if(is.null(x)) x <- 1:TT
  Dl = gen_diff_mat(n = TT, l = l+1, x = x)
  Dlm1 = gen_diff_mat(n = TT, l = l, x = x)
  Dlm1sqrd <- t(Dlm1) %*% Dlm1
  e_mat <- etilde_mat(TT = TT) # needed to generate B
  Dl_prob = gen_diff_mat(n = TT, l = l_prob+1, x = x)
  H_tf <- gen_tf_mat(n = length(countslist), k = l_prob, x = x)
  if(is.null(mn)) mn = init_mn(ylist, numclust, TT, dimdat, countslist = countslist)
  ntlist = sapply(ylist, nrow)
  N = sum(ntlist)

  ## Initialize some objects
  prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K.
  denslist_by_clust <- NULL
  objectives = c(+1E20, rep(NA, niter-1))
  sigma_fac <- diff(range(do.call(rbind, ylist)))/8
  sigma = init_sigma(ylist, numclust, sigma_fac) ## (T x numclust x (dimdat x dimdat))
  sigma_eig_by_clust = NULL
  zero.betas = zero.alphas = list()

  ## The least elegant solution I can think of.. used only for blocked cv
  if(!is.null(countslist_overwrite)) countslist = countslist_overwrite
  #if(!is.null(countslist)) check_trim(ylist, countslist)

  vals <- vector(length = niter)
  start.time = Sys.time()
  for(iter in 2:niter){
    if(verbose){
      print_progress(iter-1, niter-1, "EM iterations.", start.time = start.time)
    }
    resp <- Estep(mn, sigma, prob, ylist = ylist, numclust = numclust,
                  denslist_by_clust = denslist_by_clust,
                  first_iter = (iter == 2), countslist = countslist)

    ## M step (three parts)

    ## 1. Means
    res.mu = Mstep_mu(resp, ylist,
                      lambda = lambda,
                      first_iter = (iter == 2),
                      l = l, Dl = Dl, Dlm1 = Dlm1,
                      Dlm1sqrd = Dlm1sqrd,
                      sigma_eig_by_clust = sigma_eig_by_clust,
                      sigma = sigma, maxdev = maxdev,
                      e_mat = e_mat,
                      Zs = NULL,
                      Ws = NULL,
                      uws = NULL,
                      uzs =  NULL,
                      x = x,
                      err_rel = admm_err_rel,
                      err_abs = admm_err_abs,
                      local_adapt = admm_local_adapt,
                      local_adapt_niter = admm_local_adapt_niter)
    mn = res.mu$mns

    ## 2. Sigma
    sigma = Mstep_sigma(resp, ylist, mn, numclust)
    # sigma_eig_by_clust <- eigendecomp_sigma_array(sigma)
    # denslist_by_clust <- make_denslist_eigen(ylist, mn, TT, dimdat, numclust,
    #                                          sigma_eig_by_clust,
    #                                          countslist)

    ## 3. Probabilities
    prob_link = Mstep_prob(resp, countslist = countslist, H_tf = H_tf,
                           lambda_prob = lambda_prob, l_prob = l_prob, x = x)
    prob = softmax(prob_link)

    objectives[iter] = objective(ylist = ylist, mu = mn, sigma = sigma, prob = prob, prob_link = prob_link,
                                 lambda = lambda, Dl = Dl, l = l, countslist = countslist,
                                 Dl_prob = Dl_prob,
                                 l_prob = l_prob,
                                 lambda_prob = lambda_prob)

    ## Check convergence
    if(check_converge_rel(objectives[iter-1], objectives[iter], tol = tol_em)) break
  }

  return(structure(list(mn = mn,
                        prob = prob,
                        sigma = sigma,
                        objectives = objectives[2:iter],
                        final.iter = iter,
                        resp = resp,
                        ## Above is output, below are data/algorithm settings.
                        dimdat = dimdat,
                        TT = TT,
                        N = N,
                        l = l,
                        x = x,
                        numclust = numclust,
                        lambda = lambda,
                        maxdev = maxdev,
                        niter = niter
  ), class = "flowsmooth"))
}
```

Next, `flowsmooth()` is the main user-facing function.


```{r flowsmooth}
#' Main function. Repeats the EM algorithm (\code{flowsmooth_once()}) with |nrep| restarts (5 by default).
#'
#' @param nrestart : number of random restarts
#' @param ... : arguments for \code{flowsmooth_once()}
#'
#' @return
#' @export
#'
#' @examples
flowsmooth <- function(nrestart = 10, ...){
  out.models <- lapply(1:nrestart, FUN = function(x){
    model.temp <- flowsmooth_once(...)
    model.obj <- tail(model.temp$objectives, n = 1)
    return(list(model = model.temp, objective = model.obj))
  })
  objectives <- sapply(out.models, FUN = function(x) x$objective)
  best.model <- which.min(objectives)
  return(out.models[[best.model]][["model"]])
}
```

