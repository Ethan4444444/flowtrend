[["index.html", "Creating the flowsmooth R package 1 Introduction", " Creating the flowsmooth R package Sangwon Hyun 2022-12-01 1 Introduction This package implements flowsmooth, a model used for smooth estimation of mixture models across time. The documentation and package are both created using one simple command: litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook()) "],["package-setup.html", "2 Package setup", " 2 Package setup The DESCRIPTION file is created using this code. usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;flowsmooth&quot;, Description = &quot;Time-smooth mixture modeling for flow cytometry data.&quot;, `Authors@R` = person( given = &quot;Sangwon&quot;, family = &quot;Hyun&quot;, email = &quot;sangwonh@ucsc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;Sangwon Hyun&quot;) The following is what will show up when someone types package?flowsmooth in the console. #&#39; flowsmooth #&#39; #&#39; This package implements the `flowsmooth` method for automatic gating of flow cytometry data over time and space. #&#39; #&#39; @docType package This package will have some dependancies: library(tidyverse) library(ggplot2) usethis::use_package(&quot;tidyverse&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_package(&quot;clue&quot;) ## Temporary loading of the old package "],["generating-1d-data.html", "3 Generating 1d data", " 3 Generating 1d data This function generates synthetic 1-dimensional data, and returns it in a “long” format matrix, with columns time, y, mu, and cluster. The latter two are the true underlying parameters. #&#39; Generates some synthetic 1-dimensional data with three clusters. Returns a #&#39; data frame with (1) time, (2) Y (3) mu (4) cluster assignments. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; @param die_off_time When the cluster probability dies off, in the #&#39; middle. For instance, 0.45 means it dies off at time \\code{.45*TT}, then #&#39; lives again at \\code{(1 - .45)*TT}. #&#39; @param return_model If true, return true cluster means and probabilities #&#39; instead of data. #&#39; #&#39; @return long matrix with data or model. #&#39; @export gendat_1d &lt;- function(TT, ntlist, die_off_time = 0.45, return_model = FALSE){ ## Make cluster probabilities, by time probs &lt;- sapply(1:TT, FUN = function(tt){ if(TT * die_off_time &lt; tt &amp; tt &lt; (1-die_off_time) * TT){ cluster_prob &lt;- c(0.05, 0.95*rep(1/(3-1), 3-1)) } else { cluster_prob &lt;- rep(1/3, 3) } return(cluster_prob) }) %&gt;% t() colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) ## Make cluster means, by time means &lt;- matrix(NA, TT, 3) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1] &lt;- tt/TT + 0.5 means[tt, 2] &lt;- sin(seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 3] &lt;- -3+sin(seq(-1, 1, length.out = TT)[tt]*6.282) } } colnames(means) = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ## mutate(cluster = as.numeric(cluster)) if(return_model) return(model) ys &lt;- lapply(1:TT, FUN = function(tt){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, sd = 0.4)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 2){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, sd = .5)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 3){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, sd = .35)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } } Y &lt;- unlist(Y) mu &lt;- unlist(mu) cluster &lt;- rep(1:3, times = clusters_count) one_df = tibble(time = tt, Y = Y, mu = mu, cluster = cluster) return(one_df) }) %&gt;% bind_rows() return(ys) } dt2ylist() is a helper that takes the output generated from gendat_1d(), and splits it by the time column to create a ylist object, which is a \\(T\\)-length list of \\(n_t \\times d\\) matrices. #&#39; Converting to a list of matrices, \\code{ylist}, to input to \\code{flowsmooth()}. #&#39; #&#39; @param dt Output from \\code{gendat_1d()}. #&#39; #&#39; @return List of matrices #&#39; @export dt2ylist &lt;- function(dt){ dt%&gt;% select(time, Y) %&gt;% arrange(time) %&gt;% group_by(time) %&gt;% group_split(.keep = FALSE) %&gt;% lapply(as.matrix) } Next, we’ll make some plotting functions 1d model and data. "],["building-the-flowmix_tf-method.html", "4 Building the flowmix_tf() method 4.1 Trend filtering, briefly 4.2 Differencing matrix 4.3 Objective (data log-likelihood)", " 4 Building the flowmix_tf() method 4.1 Trend filtering, briefly Trend filtering is a tool for non-parametric regression on a sequence of output points \\(y = (y_1,..., y_T)\\) observed at locations \\(x = (x_1, ..., x_T)\\). It is usually assumed that \\(x_1, ..., x_T\\) are evenly spaced points, though this can be relaxed. The trend filtering estimate of order \\(l\\) of the time series \\(\\mu_t = \\mathbb{E}(y_t), t \\in x\\) is obtained by calculating \\[\\hat{\\mu} = \\mathop{\\mathrm{argmin}}_{\\mu \\in \\mathbb{R}^T} \\frac{1}{2}\\| \\mu - y\\|_2^2 + \\lambda \\| D^{(l+1)} \\mu\\|_1\\], where \\(\\lambda\\) is a tuning parameter and \\(D^{(l+1)} \\in \\mathbb{R}^{T-l}\\) is the \\((l+1)^\\text{th}\\) order discrete differencing matrix. 4.2 Differencing matrix We first need to be able to construct the trend filtering “differencing matrix” used for smoothing the mixture parameters over time. The general idea of the trend filtering is explained masterfully in [Ryan’s paper, Section 6 and equation (41)]. The differencing matrix is formed by recursion, starting with \\(D^{(1)}\\). \\[\\begin{equation*} D^{(1)} = \\begin{bmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0\\\\ \\vdots &amp; &amp; &amp; &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{bmatrix}. \\end{equation*}\\] For \\(l&gt;1\\), the differencing matrox \\(D^{(l+1)}\\) is defined recursively as \\(D^{(l+1)} = D^{(1)} D^{(l)}\\), starting with \\(D^{(1)}\\). #&#39; Generating Difference Matrix of Specified Order #&#39; #&#39; @param n length of vector to be differenced #&#39; @param l order of differencing #&#39; @param x optional. Spacing of input points. #&#39; #&#39; @return A n by n-l-1 matrix #&#39; @export #&#39; #&#39; @examples gen_diff_mat &lt;- function(n, l, x = NULL){ ## Basic check if(!is.null(x)) stopifnot(length(x) == n) get_D1 &lt;- function(t) {do.call(rbind, lapply(1:(t-1), FUN = function(x){ v &lt;- rep(0, t) v[x] &lt;- -1 v[x+1] &lt;- 1 v }))} if(is.null(x)){ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% D } return(D) } } else{ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% diag(k/diff(x, lag = k)) %*% D } return(D) } } } For equally spaced inputs: devtools::load_all(&quot;~/repos/flowsmooth/flowsmooth&quot;) TT = 10 l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) l = 2 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) ## ℹ Loading flowsmooth ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 -2 1 0 0 0 0 0 0 0 ## [2,] 0 1 -2 1 0 0 0 0 0 0 ## [3,] 0 0 1 -2 1 0 0 0 0 0 ## [4,] 0 0 0 1 -2 1 0 0 0 0 ## [5,] 0 0 0 0 1 -2 1 0 0 0 ## [6,] 0 0 0 0 0 1 -2 1 0 0 ## [7,] 0 0 0 0 0 0 1 -2 1 0 ## [8,] 0 0 0 0 0 0 0 1 -2 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 -2 1 0 0 0 0 0 0 0 ## [2,] 0 1 -2 1 0 0 0 0 0 0 ## [3,] 0 0 1 -2 1 0 0 0 0 0 ## [4,] 0 0 0 1 -2 1 0 0 0 0 ## [5,] 0 0 0 0 1 -2 1 0 0 0 ## [6,] 0 0 0 0 0 1 -2 1 0 0 ## [7,] 0 0 0 0 0 0 1 -2 1 0 ## [8,] 0 0 0 0 0 0 0 1 -2 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -1 3 -3 1 0 0 0 0 0 0 ## [2,] 0 -1 3 -3 1 0 0 0 0 0 ## [3,] 0 0 -1 3 -3 1 0 0 0 0 ## [4,] 0 0 0 -1 3 -3 1 0 0 0 ## [5,] 0 0 0 0 -1 3 -3 1 0 0 ## [6,] 0 0 0 0 0 -1 3 -3 1 0 ## [7,] 0 0 0 0 0 0 -1 3 -3 1 When the inputs have a gap in it: x = (1:10)[-(4:6)] l = 2 TT = length(x) Dl = gen_diff_mat(n = TT, l = l+1, x = x) print(Dl) Next, here’s a function to build a lasso regressor matrix \\(H\\) that can be used to solve an equivalent problem as the trend filtering of the ’th degree. (This is stated in Lemma 4, equation (25) from Tibshirani (2014)) #&#39; A lasso regressor matrix H that can be used to solve an equivalent problem as the trend filtering of the \\code{k}&#39;th degree. #&#39; #&#39; @param n Total number of time points. #&#39; @param k Degree of trend filtering for cluster probabilities. #&#39; @param x Time points #&#39; #&#39; @return $n$ by $n$ matrix. #&#39; #&#39; @export gen_tf_mat &lt;- function(n, k, x = NULL){ if(is.null(x) ){ x = (1:n)/n } if(!is.null(x)){ stopifnot(length(x) == n) } ## For every i,j&#39;th entry, use this helper function (from eq 25 of Tibshirani ## (2014)). gen_ij &lt;- function(i,j){ xi &lt;- x[i] if(j %in% 1:(k+1)){ return(xi^(j-1)) } if(j %in% (k+2):n){ return(prod(xi - x[(j-k):(j-1)]) * ifelse(xi &gt;= x[(j-1)], 1, 0)) } } ## Generate the H matrix, entry-wise. H &lt;- matrix(nrow = n, ncol = n) for(i in 1:n){ for(j in 1:n){ H[i,j] &lt;- gen_ij(i,j) } } return(H) } Here’s a simple test. testthat::test_that(&quot;Trend filtering regression matrix is created correctly.&quot;, { H1 &lt;- gen_tf_mat(10, 1) H2 &lt;- gen_tf_mat(10, 1, x=(1:10)/10) testthat::expect_equal(H1, H2) testthat::expect_equal(dim(H1), c(10,10)) }) ## Test passed 😀 4.3 Objective (data log-likelihood) First, loglik_tt() calculates the log-likelihood of one cytogram, which is: \\[\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right) \\] #&#39; @param mu Cluster means. #&#39; @param prob Cluster probabilities. #&#39; @param prob Cluster variances. #&#39; @param ylist Data. #&#39; @param tt Time point. loglik_tt &lt;- function(ylist, tt, mu, sigma, prob, dimdat = NULL, countslist = NULL, numclust){ ## One particle&#39;s log likelihood (weighted density) weighted.densities = sapply(1:numclust, function(iclust){ if(dimdat == 1){ return(prob[tt,iclust] * dnorm(ylist[[tt]], mu[tt,,iclust], sqrt(sigma[iclust,,]))) } if(dimdat &gt; 1){ return(prob[tt,iclust] * dmvnorm_arma_fast(ylist[[tt]], mu[tt,,iclust], as.matrix(sigma[iclust,,]), FALSE)) } }) nt = nrow(ylist[[tt]]) counts = (if(!is.null(countslist)) countslist[[tt]] else rep(1, nt)) sum_wt_dens = rowSums(weighted.densities) sum_wt_dens = sum_wt_dens %&gt;% pmax(1E-100) return(sum(log(sum_wt_dens) * counts)) } Next, here is the function that calculates the entire objective from all cytograms, given model parameter mu, prob, and sigma. #&#39; Evaluating the penalized data log-likelihood on all data \\code{ylist} given parameters \\code{mu}, \\code{prob}, and \\code{sigma}. #&#39; #&#39; @param mu #&#39; @param prob #&#39; @param prob_link #&#39; @param sigma #&#39; @param ylist #&#39; @param Dl #&#39; @param l #&#39; @param lambda #&#39; @param l_prob #&#39; @param Dl_prob #&#39; @param lambda_prob #&#39; @param alpha #&#39; @param beta #&#39; @param denslist_by_clust #&#39; @param countslist #&#39; @param unpenalized if TRUE, return the unpenalized out-of-sample fit. #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples objective &lt;- function(mu, prob, prob_link = NULL, sigma, ## TT, N, dimdat, numclust, ylist, Dl, l = NULL, lambda = 0, l_prob = NULL, Dl_prob = NULL, lambda_prob = 0, alpha = NULL, beta = NULL, denslist_by_clust = NULL, countslist = NULL, unpenalized = FALSE){ ## Set some important variables TT = dim(mu)[1] numclust = dim(mu)[3] if(is.null(countslist)){ ntlist = sapply(ylist, nrow) } else { ntlist = sapply(countslist, sum) } N = sum(ntlist) dimdat = ncol(ylist[[1]]) ## Calculate the log likelihood loglik = sapply(1:TT, function(tt){ if(is.null(denslist_by_clust)){ return(loglik_tt(ylist, tt, mu, sigma, prob, countslist, numclust = numclust, dimdat = dimdat)) } else { return(loglik_tt_precalculate(ylist, tt, denslist_by_clust, prob, countslist, numclust)) ## TODO: This doesn&#39;t exist yet, but might need to, since.. speed! } }) if(unpenalized){ obj = -1/N * sum(unlist(loglik)) return(obj) } else { ## Return penalized likelihood mu.splt &lt;- asplit(mu, MARGIN = 3) diff_mu &lt;- sum(unlist(lapply(mu.splt, FUN = function(m) sum(abs(Dl %*% m))))) ## diff_prob &lt;- sum(abs(Dl_prob %*% log(prob * sapply(countslist, sum)))) diff_prob &lt;- sum(abs(Dl_prob %*% prob_link)) obj = -1/N * sum(unlist(loglik)) + lambda * diff_mu + lambda_prob * diff_prob return(obj) } } Here’s a helper to check numerical convergence of the EM algorithm. #&#39; Checks numerical improvement in objective value. Returns TRUE if |old-new|/|old| is smaller than tol. #&#39; #&#39; @param old Objective value from previous iteration. #&#39; @param new Objective value from current iteration. #&#39; @param tol Numerical tolerance. check_converge_rel &lt;- function(old, new, tol=1E-6){ return(abs((old-new)/old) &lt; tol ) } Here’s also a helper function to do the softmax-ing of \\(\\alpha_t \\in \\mathbb{R}^K\\). #&#39; Converts the Xbeta to softmax(Xbeta), so to speak. Xbeta is the linear functional of X from a multinomial regression; in our notation, it&#39;s alpha. #&#39; #&#39; @param prob_link alpha, which is a (T x K) matrix. #&#39; #&#39; @return exp(alpha)/rowSum(exp(alpha)). A (T x K) matrix. softmax &lt;- function(prob_link){ exp_prob_link = exp(prob_link) prob = exp_prob_link / rowSums(exp_prob_link) } testthat::test_that(&quot;Test for softmax&quot;,{ link = runif(100, min = -10, max = 10) %&gt;% matrix(nrow = 10, ncol = 10) testthat::expect_true(all(abs(rowSums(softmax(link)) - 1) &lt; 1E-13)) }) ## Test passed 😸 "],["m-step.html", "5 M step 5.1 M step for \\(\\pi\\) 5.2 M step for \\(\\Sigma\\) 5.3 M step for \\(\\mu\\)", " 5 M step The M step of the EM algorithm has three steps, one each for \\(\\mu\\), \\(\\pi\\), and \\(\\Sigma\\). 5.1 M step for \\(\\pi\\) #&#39; The M step for the cluster probabilities #&#39; #&#39; @param resp Responsibilities. #&#39; @param H_tf Trend filtering matrix. #&#39; @param countslist Particle multiplicities. #&#39; @param lambda_prob Regularization. #&#39; @param l_prob Trend filtering degree. #&#39; #&#39; @return (T x k) matrix containing the alphas, for \\code{prob = exp(alpha)/ #&#39; rowSums(exp(alpha))}. #&#39; @export #&#39; Mstep_prob &lt;- function(resp, H_tf, countslist = NULL, lambda_prob = NULL, l_prob = NULL, x = NULL){ ## Basic setup TT &lt;- length(resp) ## Basic checks stopifnot(is.null(l_prob) == is.null(lambda_prob)) ## If glmnet isn&#39;t actually needed, don&#39;t use it. if(is.null(l_prob) &amp; is.null(lambda_prob)){ ## Calculate the average responsibilities, per time point. if(is.null(countslist)){ resp.avg &lt;- lapply(resp, colMeans) %&gt;% do.call(rbind, .) } else { resp.avg &lt;- lapply(1:TT, FUN = function(ii){ colSums(resp[[ii]])/sum(countslist[[ii]]) }) %&gt;% do.call(rbind, .) } return(resp.avg) ## If glmnet is actually needed, use it. } else { penalty.facs &lt;- c(rep(0, l_prob+1), rep(1, nrow(H_tf) - l_prob - 1)) resp.predict &lt;- do.call(rbind, lapply(resp, colSums)) glmnet_obj &lt;- glmnet(x = H_tf, y = resp.predict, family = &quot;multinomial&quot;, penalty.factor = penalty.facs, maxit = 1e7, lambda = mean(penalty.facs)*lambda_range(lambda_prob), standardize = F, intercept = FALSE) ## todo: replicate the parameters. pred_link &lt;- predict(glmnet_obj, newx = H_tf, type = &quot;link&quot;, s = mean(penalty.facs)*lambda_prob)[,,1] return(pred_link) } } This should return a \\(T\\) by \\(K\\) matrix, which we’ll test here: testthat::test_that(&quot;Mstep of pi returns a (T x K) matrix.&quot;, { ## Generate some fake responsibilities and trend filtering matrix TT = 100 numclust = 3 nt = 10 resp = lapply(1:TT, function(tt){ oneresp = runif(nt*numclust) %&gt;% matrix(ncol=numclust) oneresp = oneresp/rowSums(oneresp) }) H_tf &lt;- gen_tf_mat(n = TT, k = 0) ## Check the size pred_link = Mstep_prob(resp, H_tf, l_prob = 0, lambda_prob = 1E-3) testthat::expect_equal(dim(pred_link), c(TT, numclust)) pred_link = Mstep_prob(resp, H_tf) testthat::expect_equal(dim(pred_link), c(TT, numclust)) ## Check the correctness pred_link = Mstep_prob(resp, H_tf) }) Each row of this matrix should contain the fitted values \\(\\alpha_k \\in \\mathbb{R}^3\\) where \\(\\alpha_{kt} = h_t^T w_{k}\\), for.. \\(h_t\\) that are rows of the trend filtering matrix \\(H \\in \\mathbb{R}^{T \\times T}\\). \\(w_k \\in \\mathbb{R}^{n}\\) that are the regression coefficients estimated by glmnet(). Here is a test for the correctness of the M step for \\(\\pi\\). testthat::test_that(&quot;Test the M step of \\pi against CVXR&quot;, {}) 5.2 M step for \\(\\Sigma\\) #&#39; M step for cluster covariance (sigma). #&#39; #&#39; @param resp Responsibility. #&#39; @param ylist Data. #&#39; @param mn Means #&#39; @param numclust Number of clusters. #&#39; #&#39; @return (K x d x d) array containing K (d x d) covariance matrices. #&#39; @export #&#39; #&#39; @examples Mstep_sigma &lt;- function(resp, ylist, mn, numclust){ ## Find some sizes TT = length(ylist) ntlist = sapply(ylist, nrow) dimdat = ncol(ylist[[1]]) cs = c(0, cumsum(ntlist)) ## Set up empty residual matrix (to be reused) cs = c(0, cumsum(ntlist)) vars &lt;- vector(mode = &quot;list&quot;, numclust) ylong = do.call(rbind, ylist) ntlist = sapply(ylist, nrow) irows = rep(1:nrow(mn), times = ntlist) for(iclust in 1:numclust){ resp.thisclust = lapply(resp, function(myresp) myresp[,iclust, drop = TRUE]) resp.long = do.call(c, resp.thisclust) mnlong = mn[irows,,iclust] if(is.vector(mnlong)) mnlong = mnlong %&gt;% cbind() ## browser() ## vars[[iclust]] = estepC(ylong, mnlong, sqrt(resp.long), sum(resp.long)) ## TODO: see if this could be sped up. resid &lt;- ylong - mnlong resid_weighted &lt;- resp.long * resid sig_temp &lt;- t(resid_weighted) %*% resid/sum(resp.long) vars[[iclust]] &lt;- sig_temp } ## Make into an array sigma_array = array(NA, dim=c(numclust, dimdat, dimdat)) for(iclust in 1:numclust){ sigma_array[iclust,,] = vars[[iclust]] } ## Basic check stopifnot(all(dim(sigma_array) == c(numclust, dimdat, dimdat))) return(sigma_array) } 5.3 M step for \\(\\mu\\) This is a big one. It uses the ADMM written in OO, reproduced briefly here. #&#39; Computes the M step for mu. TODO: use templates for the argument. As shown #&#39; here: #&#39; https://stackoverflow.com/questions/15100129/using-roxygen2-template-tags #&#39; #&#39; @param resp Responsbilities of each particle. #&#39; @param ylist #&#39; @param lambda #&#39; @param l #&#39; @param sigma #&#39; @param sigma_eig_by_clust #&#39; @param Dlm1 #&#39; @param Dl #&#39; @param TT #&#39; @param N #&#39; @param dimdat #&#39; @param first_iter #&#39; @param mus #&#39; @param Zs #&#39; @param Ws #&#39; @param uws #&#39; @param uzs #&#39; @param maxdev #&#39; @param x #&#39; @param niter #&#39; @param err_rel #&#39; @param err_abs #&#39; @param zerothresh #&#39; @param local_adapt #&#39; @param local_adapt_niter #&#39; @param space #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples Mstep_mu &lt;- function(resp, ylist, lambda = 0.5, l = 3, sigma, sigma_eig_by_clust = NULL, Dlm1sqrd, Dlm1, Dl, TT, N, dimdat, first_iter = TRUE, e_mat, ## Warm startable variables mus = NULL, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, ## End of warm startable variables maxdev = NULL, x = NULL, niter = (if(local_adapt) 1e3 else 1e4), err_rel = 1E-3, err_abs = 0, zerothresh = 1E-6, local_adapt = FALSE, local_adapt_niter = 10, space = 50){ #################### ## Preliminaries ### #################### TT = length(ylist) numclust = ncol(resp[[1]]) dimdat = ncol(ylist[[1]]) ntlist = sapply(ylist, nrow) resp.sum = lapply(resp, colSums) %&gt;% do.call(rbind, .) N = sum(unlist(resp.sum)) ## NEW (make more efficient, later) # starting rho for LA-ADMM if(local_adapt){ rho.init = 1e-3 } else{ if(!is.null(x)){ rho.init = lambda*((max(x) - min(x))/length(x))^l #print(rho.init) rho.init = lambda }else{ rho.init = lambda } } ## Other preliminaries schur_syl_A_by_clust = schur_syl_B_by_clust = term3list = list() ybarlist = list() ycentered_list = Xcentered_list = yXcentered_list = list() Qlist = list() sigmainv_list = list() convergences = list() for(iclust in 1:numclust){ ## Retrieve sigma inverse from pre-computed SVD, if necessary if(is.null(sigma_eig_by_clust)){ sigmainv = solve(sigma[iclust,,]) } else { sigmainv = sigma_eig_by_clust[[iclust]]$sigma_inv } resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) ## Center y and X # obj &lt;- weight_ylist(iclust, resp, resp.sum, ylist) #ycentered &lt;- obj$ycentered ## Form the Sylvester equation coefficients in AX + XB + C = 0 # syl_A = rho * sigma[iclust,,] # Q = 1/N * t(Xcentered) %*% D %*% Xcentered # syl_B = Q %*% Xinv AB &lt;- get_AB_mats(y = y, resp = resp.iclust, Sigma_inv = sigmainv, e_mat = e_mat, N = N, Dl = Dl, Dlm1 = Dlm1, Dlm1sqrd = Dlm1sqrd, rho = rho.init, z = NULL, w = NULL, uz = NULL, uw = NULL) ## Store the Schur decomposition schur_syl_A_by_clust[[iclust]] = myschur(AB$A) schur_syl_B_by_clust[[iclust]] = myschur(AB$B) ## Calculate coefficients for objective value calculation # Qlist[[iclust]] = Q ## ## Also calculate some things for the objective value ## ylong = sweep(do.call(rbind, ylist), 2, obj$ybar) ## longwt = do.call(c, lapply(1:TT, function(tt){ resp[[tt]][,iclust]})) %&gt;% sqrt() ## wt.long = longwt * ylong ## wt.ylong = longwt * ylong ## crossprod(wt.ylong, wt.ylong) ## Store the third term # term3list[[iclust]] = 1 / N * sigmainv %*% yXcentered # ybarlist[[iclust]] = obj$ybar # ycentered &lt;- NULL ycentered_list[[iclust]] = ycentered #print(ycentered) # Xcentered_list[[iclust]] = Xcentered # yXcentered_list[[iclust]] = yXcentered sigmainv_list[[iclust]] = sigmainv } ########################################## ## Run ADMM separately on each cluster ## ######################################### yhats = admm_niters = admm_inner_iters = vector(length = numclust, mode = &quot;list&quot;) if(first_iter) mus = vector(length = numclust, mode = &quot;list&quot;) # if(first_iter){ Zs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat) ) Ws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = dimdat, ncol = TT - l)) uzs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat) ) uws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = dimdat, ncol = TT - l)) # Zs = Ws = Us = vector(length = numclust, mode = &quot;list&quot;) # } fits = matrix(NA, ncol = numclust, nrow = ceiling(niter / space)) #browser() ## For every cluster, run LA-ADMM resid_mat_list = list() start.time = Sys.time() for(iclust in 1:numclust){ resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) resp.sum.iclust &lt;- lapply(resp.sum, FUN = function(r) matrix(r[iclust])) ## Possibly locally adaptive ADMM, for now just running with rho == lambda res = la_admm_oneclust(K = (if(local_adapt) local_adapt_niter else 1), local_adapt = local_adapt, iclust = iclust, niter = niter, TT = TT, N = N, dimdat = dimdat, maxdev = maxdev, schurA = schur_syl_A_by_clust[[iclust]], schurB = schur_syl_B_by_clust[[iclust]], #term3 = term3list[[iclust]], sigmainv = sigmainv_list[[iclust]], # Xinv = Xinv, # Xaug = Xaug, # Xa = Xa, rho = rho.init, rhoinit = rho.init, sigma = sigma, lambda = lambda, resp = resp.iclust, l = l, Dl = Dl, Dlm1 = Dlm1, #resp.sum = resp.sum.iclust, y = ylist, err_rel = err_rel, err_abs = err_abs, zerothresh = zerothresh, sigma_eig_by_clust = sigma_eig_by_clust, space = space, objective = F, norms = F, ## Warm starts from previous *EM* iteration first_iter = first_iter, # beta = betas[[iclust]], #mu = mus[[iclust]], uw = uws[[iclust]], uz = uzs[[iclust]], z = Zs[[iclust]], w = Ws[[iclust]] ) ## Store the results mus[[iclust]] = res$mu yhats[[iclust]] = t(res$yhat) ## fits[,iclust] = res$fits admm_niters[[iclust]] = res$kk admm_inner_iters[[iclust]] = res$inner.iter ## Store other things for for warmstart Zs[[iclust]] = res$Z uzs[[iclust]] = res$uz uws[[iclust]] = res$uw Ws[[iclust]] = res$W ## The upper triangular matrix remains the same. ## The upper triangular matrix remains the same. resid_mat_list[[iclust]] = res$resid_mat ## temporary convergences[[iclust]] = res$converge # print(res$converge) } ## Aggregate the yhats into one array yhats_array = array(NA, dim = c(TT, dimdat, numclust)) for(iclust in 1:numclust){ yhats_array[,,iclust] = yhats[[iclust]] } ## Each are lists of length |numclust|. return(list(mus = mus, mns = yhats_array, fits = fits, resid_mat_list = resid_mat_list, convergences = convergences, admm_niters = admm_niters, ## Temporary: Seeing the number of ## outer iterations it took to ## converge. admm_inner_iters = admm_inner_iters, ## For warmstarts Zs = Zs, Ws = Ws, uws = uws, uzs = uzs, N = N, ## For using in the Sigma M step ycentered_list = ycentered_list, Xcentered_list = Xcentered_list, yXcentered_list = yXcentered_list, Qlist = Qlist )) } Here’s a convergence checker for the outer layer of LA-ADMM. # Outer convergence check ------------------------------------------------- outer_converge &lt;- function(objectives){ consec = 4 if(length(objectives) &lt; consec){ return(FALSE) } else { mytail = utils::tail(objectives, consec) rel_diffs = mytail[1:(consec-1)]/mytail[2:consec] return(all(abs(rel_diffs) - 1 &lt; 1E-5)) } } TODO: Right now, sigma_eig_by_clust is not used. When speeding up the code, do this first. Likewise, ycentered_list isn’t used, while it is clearly useful in Sigma M step Comparing it against CVXR (TODO: not written yet). testthat::test_that(&quot;Test the M step of \\mu against CVXR&quot;, {}) "],["e-step.html", "6 E step", " 6 E step #&#39; E step, which updates the &quot;responsibilities&quot;, which are posterior membership probabilities of each particle. #&#39; #&#39; @param mn #&#39; @param sigma #&#39; @param prob #&#39; @param ylist #&#39; @param numclust #&#39; @param denslist_by_clust #&#39; @param first_iter #&#39; @param countslist #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples Estep &lt;- function (mn, sigma, prob, ylist = NULL, numclust, denslist_by_clust = NULL, first_iter = FALSE, countslist = NULL){ ## Basic setup TT = length(ylist) ntlist = sapply(ylist, nrow) resp = list() dimdat = dim(mn)[2] assertthat::assert_that(dim(mn)[1] == length(ylist)) ## Helper to calculate Gaussian density for each \\code{N(y_{t,k},mu_{t,k} and ## Sigma_k)}. calculate_dens &lt;- function(iclust, tt, y, mn, sigma, denslist_by_clust, first_iter) { mu &lt;- mn[tt, , iclust] if (dimdat == 1) { dens = dnorm(y, mu, sd = sqrt(sigma[iclust, , ])) } else { dens = dmvnorm_arma_fast(y, mu, sigma[iclust,,], FALSE) } return(dens) } ## Calculate posterior probability of membership of $y_{it}$. ncol.prob = ncol(prob) for (tt in 1:TT) { ylist_tt = ylist[[tt]] densmat &lt;- sapply(1:numclust, calculate_dens, tt, ylist_tt, mn, sigma, denslist_by_clust, first_iter) wt.densmat &lt;- matrix(prob[tt, ], nrow = ntlist[tt], ncol = ncol.prob, byrow = TRUE) * densmat wt.densmat = wt.densmat + 1e-10 wt.densmat &lt;- wt.densmat/rowSums(wt.densmat) resp[[tt]] &lt;- wt.densmat } ## Weight the responsibilities by $C_{it}$. if (!is.null(countslist)) { resp &lt;- Map(function(myresp, mycount) { myresp * mycount }, resp, countslist) } return(resp) } The E step should return a list of exactly the same size and format as ylist, which is a \\(T\\) -length list of matrices of size \\(n_t \\times d\\). testthat::test_that(&quot;E step returns appropriately sized object.&quot;,{ ## Generate some fake data TT = 100 ylist = lapply(1:TT, function(tt){ runif(90) %&gt;% matrix(ncol = 3, nrow = 30)}) numclust = 3 dimdat = 3 ## Initialize a few parameters, not carefully sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. ## Calculate responsibility resp = Estep(mn, sigma, prob, ylist, numclust) ## Check these things testthat::expect_equal(length(resp), length(ylist)) testthat::expect_equal(sapply(resp, dim), sapply(ylist, dim)) }) "],["main-flowsmooth-function.html", "7 Main flowsmooth function", " 7 Main flowsmooth function Now we’ve assembled all ingredients we need, we’ll build the main function flowsmooth_once() to estimate a flowsmooth model. Here goes: #&#39; Estimate flowsmooth model once. #&#39; #&#39; @param ylist Data. #&#39; @param countslist Counts corresponding to multiplicities. #&#39; @param x Times, if points are not evenly spaced. Defaults to NULL, in which #&#39; case the value becomes \\code{1:T}, for the $T==length(ylist)$. #&#39; @param numclust Number of clusters. #&#39; @param niter Maximum number of EM iterations. #&#39; @param l Degree of differencing for the mean trend filtering #&#39; @param l_prob Degree of differencing for the probability trend filtering #&#39; @param mn Initial value for cluster means. Defaults to NULL, in which case #&#39; initial values are randomly chosen from the data. #&#39; @param lambda Smoothing parameter for means #&#39; @param lambda_prob Smoothing parameter for probabilities #&#39; @param verbose Loud or not? EM iteration progress is printed. #&#39; @param tol_em Relative numerical improvement of the objective value at which #&#39; to stop the EM algorithm #&#39; @param maxdev Maximum deviation of cluster means across time.. #&#39; @param countslist_overwrite #&#39; @param admm_err_rel #&#39; @param admm_err_abs #&#39; @param admm_local_adapt #&#39; @param admm_local_adapt_niter #&#39; #&#39; @return List object with flowsmooth model estimates. #&#39; @export #&#39; #&#39; @examples flowsmooth_once &lt;- function(ylist, countslist = NULL, x = NULL, numclust, niter = 1000, l, l_prob = NULL, mn = NULL, lambda = 0, lambda_prob = NULL, verbose = FALSE, tol_em = 1E-4, maxdev = NULL, countslist_overwrite = NULL, ## beta Mstep (ADMM) settings admm = TRUE, admm_err_rel = 1E-3, admm_err_abs = 1E-4, ## Mean M step (Locally Adaptive ADMM) settings admm_local_adapt = FALSE, admm_local_adapt_niter = if(admm_local_adapt) 10 else 1){ ## Basic checks if(!is.null(maxdev)){ assertthat::assert_that(maxdev!=0) } else { maxdev = 1E10 } assertthat::assert_that(numclust &gt; 1) assertthat::assert_that(niter &gt; 1) if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } ## Setup for EM algorithm TT = length(ylist) dimdat = ncol(ylist[[1]]) if(is.null(x)) x &lt;- 1:TT Dl = gen_diff_mat(n = TT, l = l+1, x = x) Dlm1 = gen_diff_mat(n = TT, l = l, x = x) Dlm1sqrd &lt;- t(Dlm1) %*% Dlm1 e_mat &lt;- etilde_mat(TT = TT) # needed to generate B Dl_prob = gen_diff_mat(n = TT, l = l_prob+1, x = x) H_tf &lt;- gen_tf_mat(n = length(countslist), k = l_prob, x = x) if(is.null(mn)) mn = init_mn(ylist, numclust, TT, dimdat, countslist = countslist) ntlist = sapply(ylist, nrow) N = sum(ntlist) ## Initialize some objects prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. denslist_by_clust &lt;- NULL objectives = c(+1E20, rep(NA, niter-1)) sigma_fac &lt;- diff(range(do.call(rbind, ylist)))/8 sigma = init_sigma(ylist, numclust, sigma_fac) ## (T x numclust x (dimdat x dimdat)) sigma_eig_by_clust = NULL zero.betas = zero.alphas = list() ## The least elegant solution I can think of.. used only for blocked cv if(!is.null(countslist_overwrite)) countslist = countslist_overwrite #if(!is.null(countslist)) check_trim(ylist, countslist) vals &lt;- vector(length = niter) start.time = Sys.time() for(iter in 2:niter){ if(verbose){ print_progress(iter-1, niter-1, &quot;EM iterations.&quot;, start.time = start.time) } resp &lt;- Estep(mn, sigma, prob, ylist = ylist, numclust = numclust, denslist_by_clust = denslist_by_clust, first_iter = (iter == 2), countslist = countslist) ## M step (three parts) ## 1. Means res.mu = Mstep_mu(resp, ylist, lambda = lambda, first_iter = (iter == 2), l = l, Dl = Dl, Dlm1 = Dlm1, Dlm1sqrd = Dlm1sqrd, sigma_eig_by_clust = sigma_eig_by_clust, sigma = sigma, maxdev = maxdev, e_mat = e_mat, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, x = x, err_rel = admm_err_rel, err_abs = admm_err_abs, local_adapt = admm_local_adapt, local_adapt_niter = admm_local_adapt_niter) mn = res.mu$mns ## 2. Sigma sigma = Mstep_sigma(resp, ylist, mn, numclust) # sigma_eig_by_clust &lt;- eigendecomp_sigma_array(sigma) # denslist_by_clust &lt;- make_denslist_eigen(ylist, mn, TT, dimdat, numclust, # sigma_eig_by_clust, # countslist) ## 3. Probabilities prob_link = Mstep_prob(resp, countslist = countslist, H_tf = H_tf, lambda_prob = lambda_prob, l_prob = l_prob, x = x) prob = softmax(prob_link) objectives[iter] = objective(ylist = ylist, mu = mn, sigma = sigma, prob = prob, prob_link = prob_link, lambda = lambda, Dl = Dl, l = l, countslist = countslist, Dl_prob = Dl_prob, l_prob = l_prob, lambda_prob = lambda_prob) ## Check convergence if(check_converge_rel(objectives[iter-1], objectives[iter], tol = tol_em)) break } return(structure(list(mn = mn, prob = prob, sigma = sigma, objectives = objectives[2:iter], final.iter = iter, resp = resp, ## Above is output, below are data/algorithm settings. dimdat = dimdat, TT = TT, N = N, l = l, x = x, numclust = numclust, lambda = lambda, maxdev = maxdev, niter = niter ), class = &quot;flowsmooth&quot;)) } Next, flowsmooth() is the main user-facing function. #&#39; Main function. Repeats the EM algorithm (\\code{flowsmooth_once()}) with |nrep| restarts (5 by default). #&#39; #&#39; @param nrestart : number of random restarts #&#39; @param ... : arguments for \\code{flowsmooth_once()} #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples flowsmooth &lt;- function(nrestart = 10, ...){ out.models &lt;- lapply(1:nrestart, FUN = function(x){ model.temp &lt;- flowsmooth_once(...) model.obj &lt;- tail(model.temp$objectives, n = 1) return(list(model = model.temp, objective = model.obj)) }) objectives &lt;- sapply(out.models, FUN = function(x) x$objective) best.model &lt;- which.min(objectives) return(out.models[[best.model]][[&quot;model&quot;]]) } "],["plotting-for-1d-data.html", "8 Plotting for 1d data", " 8 Plotting for 1d data Given 1d data ylist and an estimated model object obj, we want to plot both in a single plot. plot_1d() lets you do this. #&#39; Makes 1d plot of data and model #&#39; #&#39; @param ylist Data. #&#39; @param obj flowsmooth object. Defaults to NULL. #&#39; @param x time points. Defaults to NULL. #&#39; #&#39; @return ggplot object with data, and optionally, a flowsmooth model overlaid. #&#39; @export plot_1d &lt;- function(ylist, obj=NULL, x = NULL){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } ## make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() ## plot long matrix gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y), alpha = .1) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) ## theme(legend.position = &#39;none&#39;) if(is.null(obj)) return(gg) ## Add the model ## TT = length(ylist) numclust = obj$numclust mnmat = obj$mn %&gt;% .[,1,] %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) est_long = full_join(mn_long, prob_long) gg = gg + geom_line(aes(x = time, y = mean, size = prob, group = cluster, color = cluster), data = est_long) ## TODO: make it ignore the missing values at the gaps; currently this is not coded as NAs. ## Add the estimated 95% probability regions for data. stdev = obj$sigma %&gt;% .[,,1] %&gt;% sqrt() band_long = mn_long %&gt;% mutate(upper = case_when(cluster == &quot;1&quot; ~ mean + 1.96 * stdev[1], cluster == &quot;2&quot; ~ mean + 1.96 * stdev[2], cluster == &quot;3&quot; ~ mean + 1.96 * stdev[3]), lower = case_when(cluster == &quot;1&quot; ~ mean - 1.96 * stdev[1], cluster == &quot;2&quot; ~ mean - 1.96 * stdev[2], cluster == &quot;3&quot; ~ mean - 1.96 * stdev[3])) gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(1), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(1), alpha = .5) } The plotting function plot_1d() will be even more useful when we have a model, but can simply plot the data. Let’s try this out. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist, NULL, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model, linetype = &quot;dashed&quot;, size=2, alpha = .7) The following few sections show examples of how to use these plotting functions. Also, we will want to plot the estimated cluster probabilities, from obj. #&#39; Makes cluster probability plot (lines over time). #&#39; #&#39; @param obj Estimated model (from \\code{flowsmooth(ylist)}) #&#39; #&#39; @export plot_prob &lt;- function(obj, x = NULL){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } numclust = obj$numclust probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) prob_long %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster)) + xlab(&quot;Estimated cluster probability&quot;) } Here’s a useful function to reorder the clusters of an estimated model obj. This comes in handy when plotting and comparing model parameters. #&#39; Reorder the results of one object so that cluster 1 through #&#39; \\code{numclust} is in a particular order. The default is decreasing order of #&#39; the averages (over time) of the cluster means. #&#39; #&#39; @param res Model object. #&#39; @param ord Defaults to NULL. Use if you have an ordering in mind. #&#39; #&#39; @return Same object, but with clusters reordered. #&#39; #&#39; @export reorder_clust &lt;- function(res, ord = NULL){ ## Find an order by sums (averages) if(is.null(ord)) ord = res$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) if(!is.null(ord)) all(sort(ord) == 1:res$numclust) ## Reorder mean res$mn = res$mn[,,ord, drop=FALSE] ## Reorder sigma res$sigma = res$sigma[ord,,,drop=FALSE] ## Reorder prob res$prob = res$prob[,ord, drop=FALSE] ## Reorder the responsibilities if(&#39;resp&#39; %in% res){ resp_temp = list() for(tt in 1:TT){ rep_temp[[tt]] = res$resp[[tt]][,ord] } } return(res) } It’s useful to be able to reorder, or permute, one model’s cluster labels (cluster 1,2,.. of newres which are arbitrary) to that of another model origres. The function reorder_kl() does this by taking the posterior probabilities of the particles in ylist_particle (row-binded to be a \\(\\sum_t n_t \\times K\\) matrix), and then using a Hungarian algorithm to best match the two elongated matrices \\(A\\) and \\(B\\) by measuring the (symmetric? TODO check) KL divergence between all permutations of the \\(K\\) columns TODO: right now this is just copy-pasted from flowmix. But actually, this is an example of a function that can be directly borrowed; all the work is done in reorder_clust(). So deal with that!! #&#39; Reorder the cluster numbers for a new flowsmooth object \\code{newres}; the best #&#39; permutation (reordering) is to match the original flowmix object #&#39; \\code{origres}. #&#39; #&#39; @param newres New flowsmooth object to reorder. #&#39; @param origres Original flowsmooth object. #&#39; @param ylist_particle The particle-level data. #&#39; @param fac Defaults to 100, to take 1/100&#39;th of the particles from each time point. #&#39; @param verbose Loud or not? #&#39; #&#39; @return Reordered res #&#39; #&#39; @export reorder_kl &lt;- function(newres, origres, ylist_particle, fac = 100, verbose = FALSE){ ## Randomly sample 1/100 of the original particles (mainly for memory reasons) TT = length(ylist_particle) N = sapply(ylist_particle, nrow) %&gt;% sum() ntlist = sapply(ylist_particle, nrow) indlist = lapply(1:TT, function(tt){ nt = ntlist[[tt]] ind = sample(1:nt, round(nt / fac), replace=FALSE) }) ## Sample responsibilities ylist_particle_small = Map(function(ind, y){ y[ind,,drop = FALSE] }, indlist, ylist_particle) ## Calculate new responsibilities resp_orig_small &lt;- Estep(origres$mn, origres$sigma, origres$prob, ylist = ylist_particle_small, numclust = origres$numclust, first_iter = TRUE) resp_new_small &lt;- Estep(newres$mn, newres$sigma, newres$prob, ylist = ylist_particle_small, numclust = newres$numclust, first_iter = TRUE) assertthat::assert_that(all(sapply(resp_orig_small, dim) == sapply(resp_new_small, dim))) ## Get best ordering (using symm. KL divergence and Hungarian algorithm for ## matching) best_ord &lt;- get_best_match_from_kl(resp_new_small, resp_orig_small) if(verbose) cat(&quot;New order is&quot;, best_ord, fill=TRUE) newres_reordered_kl = newres %&gt;% reorder_clust(ord = best_ord) ## Return the reordered object return(newres_reordered_kl) } This function uses get_best_match_from_kl(), which takes two lists containing responsibilities (posterior probabilities of particles) – one from each model – and returns the cluster ordering to apply to the model that produced resp_new. We define this function and a couple of helper functions next. #&#39; Compute KL divergence from responsibilities between two models&#39; #&#39; responsibilities \\code{resp_new} and \\code{resp_old}. #&#39; #&#39; @param resp_new New responsibilities #&#39; @param resp_orig Original responsiblities. #&#39; #&#39; @return Calculate reordering \\code{o} of the clusters in model represented #&#39; by \\code{resp_new}. To be clear, \\code{o[i]} of new model is the best #&#39; match with the i&#39;th cluster of the original model. #&#39; #&#39; @export #&#39; @importFrom clue solve_LSAP get_best_match_from_kl &lt;- function(resp_new, resp_orig){ ## Basic checks . = NULL ## Fixing check() assertthat::assert_that(all(sapply(resp_new, dim) == sapply(resp_orig , dim))) ## Row-bind all the responsibilities to make a long matrix distmat = form_symmetric_kl_distmat(resp_orig %&gt;% do.call(rbind,.), resp_new %&gt;% do.call(rbind,.)) ## Use Hungarian algorithm to solve. fit &lt;- clue::solve_LSAP(distmat) o &lt;- as.numeric(fit) ## Return the ordering return(o) } ##&#39; From two probability matrices, form a (K x K) distance matrix of the ##&#39; (n)-vectors. The distance between the vectors is the symmetric KL ##&#39; divergence. ##&#39; ##&#39; @param mat1 Matrix 1 of size (n x K). ##&#39; @param mat2 Matrix 2 of size (n x K). ##&#39; ##&#39; @return K x K matrix containing symmetric KL divergence of each column of ##&#39; \\code{mat1} and \\code{mat2}. form_symmetric_kl_distmat &lt;- function(mat1, mat2){ ## Manually add some small, in case some columns are all zero mat1 = (mat1 + 1E-10) %&gt;% pmin(1) mat2 = (mat2 + 1E-10) %&gt;% pmin(1) ## Calculate and return distance matrix. KK1 = ncol(mat1) KK2 = ncol(mat2) distmat = matrix(NA, ncol=KK2, nrow=KK1) for(kk1 in 1:KK1){ for(kk2 in 1:KK2){ mydist = symmetric_kl(mat1[,kk1, drop=TRUE], mat2[,kk2, drop=TRUE]) distmat[kk1, kk2] = mydist } } stopifnot(all(!is.na(distmat))) return(distmat) } ##&#39; Symmetric KL divergence, of two probability vectors. ##&#39; ##&#39; @param vec1 First probability vector. ##&#39; @param vec2 Second prbability vector. ##&#39; ##&#39; @return Symmetric KL divergence (scalar). symmetric_kl &lt;- function(vec1, vec2){ stopifnot(all(vec1 &lt;= 1) &amp; all(vec1 &gt;= 0)) stopifnot(all(vec2 &lt;= 1) &amp; all(vec2 &gt;= 0)) kl &lt;- function(vec1, vec2){ sum(vec1 * log(vec1 / vec2)) } return((kl(vec1, vec2) + kl(vec2, vec1))/2) } Here’s an example of how to use it. TODO: this code block incurs an error because flowsmooth is not fully available to use yet, at this stage of the code. Fix this! devtools::load_all(&quot;~/repos/FlowTF&quot;) set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(2) objlist &lt;- lapply(1:2, function(isim){ flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 1, verbose = TRUE)}) newres = objlist[[1]] origres = objlist[[2]] newres_reordered = reorder_kl(newres, origres, ylist, fac = 100, verbose = FALSE) plot_1d(ylist, newres, x = x) + ggtitle(&quot;before reordering, model 1&quot;) plot_1d(ylist, origres, x = x) + ggtitle(&quot;model 2&quot;) plot_1d(ylist, newres_reordered, x = x) + ggtitle(&quot;reordered model 1&quot;) "],["tuning-the-regularization-parameters-for-flowsmooth.html", "9 Tuning the regularization parameters for flowsmooth 9.1 Predicting and evaluating on new time points 9.2 Evaluating data fit (by likelihood) in an out-of-sample measurement. 9.3 1d example with ends cut off 9.4 1d example with cross-validation 9.5 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test 9.6 Cross-validation", " 9 Tuning the regularization parameters for flowsmooth We’re going to take a huge leap, and assume the flowsmooth() function has been built. We need to build up quite a few functions before we’re able to do cross-validation. These include: Predicting out-of-sample, using predict_flowsmooth(). Evaluating data fit (by likelihood) in an out-of-sample measurement, using objective(..., unpenalized = TRUE). Numerically estimating the maximum regularization values to test, using get_max_lambda(). Making data splits, using make_cv_tf_folds(). 9.1 Predicting and evaluating on new time points First, let’s write a couple of functions interpolate_mn() and interpolate_prob() which linearly interpolate the means and probabilities at new time points. #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param mn length(x) by dimdat by numclust matrix. #&#39; #&#39; @return A dimdat-length vector. interpolate_mn &lt;- function(x, tt, iclust, mn){ ## Basic checks stopifnot(length(x) == dim(mn)[1]) stopifnot(iclust &lt;= dim(mn)[3]) if(tt %in% x) return(mn[which(x==tt),,iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation mn_t &lt;- mn[ceiling_t_ind,,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + mn[floor_t_ind,,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) ## Basic checks stopifnot(length(mn_t) == dim(mn)[2]) return(mn_t) } #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param prob length(x) by numclust array or matrix. #&#39; #&#39; @return One probability. interpolate_prob &lt;- function(x, tt, iclust, prob){ ## Basic checks numdat = dim(prob)[1] numclust = dim(prob)[2] stopifnot(length(x) == numdat) stopifnot(iclust &lt;= numclust) if(tt %in% x) return(prob[which(x == tt),iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation prob_t &lt;- prob[ceiling_t_ind,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + prob[floor_t_ind,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t ## Basic checks stopifnot(length(prob_t) == 1) stopifnot(0 &lt;= prob_t &amp; prob_t &lt;= 1) return(prob_t) } Next, let’s build a prediction function predict_flowsmooth() which takes the model object obj, and the new time points newtimes, and produces. #&#39; Prediction: Given new timepoints in the original time interval,generate a set #&#39; of means and probs (and return the same Sigma). #&#39; #&#39; @param obj Object returned from covariate EM flowmix(). #&#39; @param newtimes New times at which to make predictions. #&#39; #&#39; @return List containing mean, prob, and sigma, and x. #&#39; #&#39; @export #&#39; predict_flowsmooth &lt;- function(obj, newtimes = NULL){ ## Check the dimensions newx &lt;- newtimes if(is.null(newtimes)){ newx = obj$x } ## Check if the new times are within the time range of the original data (why is this important)? if(FALSE) stopifnot(all(sapply(newx, FUN = function(t) t &gt;= min(obj$x) &amp; t &lt;= max(obj$x)))) ## Setup some things x &lt;- obj$x TT_new = length(newx) numclust = obj$numclust dimdat = obj$dimdat ## Predict the means (manually). newmn_array = array(NA, dim = c(TT_new, dimdat, numclust)) for(iclust in 1:numclust){ newmn_oneclust &lt;- lapply(newx, function(tt){ interpolate_mn(x, tt, iclust, obj$mn) }) %&gt;% do.call(rbind, . ) newmn_array[,,iclust] = newmn_oneclust } ## Predict the probs. newprob = array(NA, dim = c(TT_new, numclust)) for(iclust in 1:numclust){ newprob_oneclust &lt;- lapply(newx, function(tt){ interpolate_prob(x, tt, iclust, obj$prob) }) %&gt;% do.call(c, .) newprob[,iclust] = newprob_oneclust } ## Basic checks stopifnot(all(dim(newprob) == c(TT_new,numclust))) stopifnot(all(newprob &gt;= 0)) stopifnot(all(newprob &lt;= 1)) ## Return the predictions return(list(mn = newmn_array, prob = newprob, sigma = obj$sigma, x = newx)) } Here’s a quick test (no new data) to make sure this function returns a list containing: the mean, probability, covariance, and new times. testthat::test_that(&quot;The prediction function returns the right things&quot;, { ## Generate data ## ll() ## devtools::load_all(&#39;~/repos/FlowTF&#39;) set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() obj &lt;- flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1, niter = 3) predobj = predict_flowsmooth(obj) testthat::expect_named(predobj, c(&quot;mn&quot;, &quot;prob&quot;, &quot;sigma&quot;, &quot;x&quot;)) }) Now, we try to make predictions at new held-out time points held_out=25:35, from a model that is estimated without those time points. testthat::test_that(&quot;prediction function returns the right things&quot;, { ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() obj &lt;- flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) predobj = predict_flowsmooth(obj, newtimes = held_out) ## Check a few things testthat::expect_equal(predobj$x, held_out) testthat::expect_equal(rowSums(predobj$prob), rep(1, length(held_out))) testthat::expect_equal(dim(predobj$mn), c(length(held_out), 1, 3)) }) Plot the predicted means and probabilities, with purple points at the interpolated means. We can see that it works as expected. g = plot_1d(ylist, obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = held_out, cluster = iclust) }) %&gt;% bind_rows() g + geom_point(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;purple&#39;, alpha = .8) The estimated probabilities are shown here, with purple points showing the interpolation. It works as expected. plot_prob(obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_point(aes(x = time, y = prob), data = preds, col = &#39;purple&#39;) 9.2 Evaluating data fit (by likelihood) in an out-of-sample measurement. predobj = predict_flowsmooth(obj, newtimes = held_out) ## Use the predicted (interpolated) model parameters obj_pred = objective(mu = predobj$mn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) truemn = array(NA, dim = dim(predobj$mn)) truemn[,1,] = dt_model %&gt;% select(time, cluster, mean) %&gt;% pivot_wider(names_from = cluster, values_from = mean) %&gt;% subset(time %in% held_out) %&gt;% select(-time) %&gt;% as.matrix() ## Use the true mean obj_better = objective(mu = truemn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) ## Make sure using the true mean is better (lower is better) stopifnot(obj_better &lt; obj_pred) obj$objectives %&gt;% plot(type = &#39;o&#39;) ## matplot(truemn[,1,], type = &#39;l&#39;) ## matlines(predobj$mn[,1,], lwd = 2) 9.3 1d example with ends cut off set.seed(100) dt &lt;- gendat_1d(TT, ntlist, die_off_time = 0.45) dt1 = dt %&gt;% subset(time &lt;= 50) dt2 = dt %&gt;% subset(time &gt; 50) 9.4 1d example with cross-validation TODO: Thoroughly test the selected lambda. TT = length(ylist) nfold = 3 make_cv_tf_folds(ylist, nfold, TT) TT 9.5 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test What should the maximum value of regularization parameters to use? It’s useful to be able to calculate the smallest value of regularization parameters that result in fully flat \\(\\mu\\) and \\(\\pi\\) over time, in all clusters. Call these \\(\\lambda_\\mu^{\\text{max}}\\) and \\(\\lambda_{\\pi}&amp;{\\text{max}}\\). Then, as candidates for cross-validation, we can use a grid of logarithmically-spaced pairs of values between (0,0) and \\((\\lambda_{\\mu}&amp;{\\text{max}}, \\lambda_{\\pi}&amp;{\\text{max}})\\). The function get_max_lambda() numerically estimates this maximum pair \\((\\lambda_{\\mu}&amp;{\\text{max}}, \\lambda_{\\pi}&amp;{\\text{max}})\\). It proceeds by first running flowsmooth() on a very large pair \\((\\lambda_\\mu, \\lambda_\\pi)\\), then sequentially halving both values while checking if the resulting estimated \\(\\mu\\) and \\(\\pi\\) are all flat over time. As soon as they cease to be flat, stop and take the previous pair of values of \\((\\lambda_\\mu, \\lambda_\\pi)\\). get_max_lambda() is a wrapper around the workhorse calc_max_lambda(). It obtains the value and saves it to a maxres_file (which defaults to maxres.Rdata) in the destin directory. ##&#39; A wrapper for \\code{calc_max_lambda}. Saves the two maximum lambda values in ##&#39; a file. ##&#39; ##&#39; @param destin Where to save the output (A two-lengthed list called ##&#39; &quot;maxres&quot;). ##&#39; @param maxres_file Filename for output. Defaults to maxres.Rdata. ##&#39; @param ... Additional arguments to \\code{flowmix()}. ##&#39; @inheritParams calc_max_lambda ##&#39; ##&#39; @return No return ##&#39; ##&#39; @export get_max_lambda &lt;- function(destin, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist, X, numclust, maxdev, max_prob_lambda, max_mean_lambda, ...){ if(file.exists(file.path(destin, maxres_file))){ load(file.path(destin, maxres_file)) cat(&quot;Maximum regularization values are loaded.&quot;, fill=TRUE) return(maxres) } else { print(Sys.time()) cat(&quot;Maximum regularization values being calculated.&quot;, fill = TRUE) cat(&quot;with initial lambdas values (alpha and beta):&quot;, fill = TRUE) print(c(max_prob_lambda, max_mean_lambda)); maxres = calc_max_lambda(ylist = ylist, countslist = countslist, X = X, numclust = numclust, maxdev = maxdev, ## This function&#39;s settings max_prob_lambda = max_prob_lambda, max_mean_lambda = max_mean_lambda, ...) save(maxres, file = file.path(destin, maxres_file)) cat(&quot;file was written to &quot;, file.path(destin, maxres_file), fill=TRUE) cat(&quot;maximum regularization value calculation done.&quot;, fill = TRUE) print(Sys.time()) return(maxres) } } The aforementioned workhorse calc_max_lambda() is here. ##&#39; Estimate maximum lambda values by brute force. First starts with a large ##&#39; initial value \\code{max_mean_lambda} and \\code{max_prob_lambda}, and runs ##&#39; the EM algorithm on decreasing set of values (sequentially halved). This ##&#39; stops once you see any non-zero coefficients, and returns the *smallest* ##&#39; regularization (lambda) value pair that gives full sparsity. Note that the ##&#39; \\code{zero_stabilize=TRUE} option is used in \\code{flowmix()}, which ##&#39; basically means the EM algorithm runs only until the zero pattern ##&#39; stabilizes. ##&#39; ##&#39; @param ylist List of responses. ##&#39; @param X Covariates. ##&#39; @param numclust Number of clusters. ##&#39; @param max_mean_lambda Defaults to 4000. ##&#39; @param max_prob_lambda Defaults to 1000. ##&#39; @param iimax Maximum value of x for 2^{-x} factors to try. ##&#39; @param ... Other arguments to \\code{flowmix_once()}. ##&#39; @return list containing the two maximum values to use. ##&#39; @examples ##&#39; \\dontrun{ ##&#39; ## Generate and bin data ##&#39; obj = generate_data_generic(p=5, TT=50, fac=1, nt=7000, dimdat=3) ##&#39; ylist = obj$ylist ##&#39; X = obj$X ##&#39; dat.gridsize = 50 ##&#39; dat.grid = make_grid(ylist, gridsize = dat.gridsize) ##&#39; obj = bin_many_cytograms(ylist, dat.grid, mc.cores=4, verbose=TRUE) ##&#39; ybin_list = obj$ybin_list ##&#39; counts_list = obj$counts_list ##&#39; ##&#39; numclust = 4 ##&#39; maxres = calc_max_lambda(ybin_list, counts_list, X, numclust, verbose=TRUE, ##&#39; nrep = 4, ##&#39; ## Function settings ##&#39; parallelize = FALSE, ##&#39; iimax = 20, ##&#39; niter = 1000, ##&#39; max_prob_lambda = 10000, ##&#39; tol = 1E-3 ## This doesn&#39;t need to be so low here. ##&#39; ) ##&#39; ##&#39; } calc_max_lambda &lt;- function(ylist, countslist = NULL, numclust, max_lambda = 4000, max_lambda_pi = 1000, verbose = FALSE, iimax = 16, ...){ ## Get range of regularization parameters. facs = sapply(1:iimax, function(ii) 2^(-ii+1)) ## DECREASING order print(&quot;running the models once&quot;) for(ii in 1:iimax){ ## print_progress(ii, iimax, &quot;regularization values&quot;, fill = TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) cat(&quot;#### lambda_alpha = &quot;, max_prob_lambda * facs[ii], &quot; and lambda_beta = &quot;, max_mean_lambda * facs[ii], &quot;being tested. &quot;, fill=TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) res = flowsmooth_once(ylist = ylist, countslist = countslist, numclust = numclust, prob_lambda = max_lambda_pi * facs[ii], mean_lambda = max_lambda * facs[ii], verbose = verbose, zero_stabilize = TRUE, ...) ## Check zero-ness toler = 0 sum_nonzero_alpha = sum(res$alpha[,-1] &gt; toler) sum_nonzero_beta = sum(unlist(lapply(res$beta, function(cf){ sum(cf[-1,] &gt; toler) }))) ## If there are *any* nonzero values, do one of the following if(sum_nonzero_alpha + sum_nonzero_beta != 0){ ## If there are *any* nonzero values at the first iter, prompt a restart ## with higher initial lambda values. if(ii==1){ stop(paste0(&quot;Max lambdas: &quot;, max_lambda, &quot; and &quot;, max_lambda_pi, &quot; were too small as maximum reg. values. Go up and try again!!&quot;)) ## If there are *any* nonzero values, return the immediately preceding ## lambda values -- these were the smallest values we had found that gives ## full sparsity. } else { ## Check one more time whether the model was actually zero, by fully running it; res = flowsmooth_once(ylist = ylist, countslist = countslist, X = X, numclust = numclust, lambda_pi = max_lambda_pi * facs[ii], lambda = max_lambda * facs[ii], zero_stabilize = FALSE, ...) toler = 0 sum_nonzero_alpha = sum(res$alpha[,-1] &gt; toler) sum_nonzero_beta = sum(unlist(lapply(res$beta, function(cf){ sum(cf[-1,] &gt; toler) }))) ## If there are *any* nonzero values, do one of the following if(sum_nonzero_alpha + sum_nonzero_beta != 0){ return(list(beta = max_lambda * facs[ii-1], alpha = max_lambda_pi *facs[ii-1])) } ## Otherwise, just proceed to the next iteration. } } cat(fill=TRUE) } } 9.6 Cross-validation Finally, we build the immediate elements needed for cross-validation. make_cv_tf_folds() makes the cross-validation “folds”, which are the \\(K\\) (nfold) list of data indices. These are not times! They simply split of 1:length(ylist). ##&#39; Define the time folds cross-validation. ##&#39; ##&#39; @param nfold Number of folds. ##&#39; @return List of fold indices. ##&#39; @export ##&#39; make_cv_tf_folds &lt;- function(ylist=NULL, nfold, TT=NULL){ ## Make hour-long index list if(is.null(TT)) TT = length(ylist) folds &lt;- rep(1:nfold, ceiling( (TT-2)/nfold))[1:(TT-2)] inds &lt;- lapply(1:nfold, FUN = function(k) (2:(TT-1))[folds == k]) names(inds) = paste0(&quot;Fold&quot;, 1:nfold) return(inds) } We can visualize how the data is to be split. In the following plot, vertical lines mark data indices in each fold using different colors. For nfold = 5, the first fold is every 5th point starting at 2, \\(\\{2,7,\\dots\\}\\), and the second fold is \\(\\{3,8,\\dots\\}\\), and so forth. The first index \\(1\\) and the last \\(TT\\) are intentionally left out and assumed available to folds. This is a small detail required because, otherwise, we cannot predictions at the either end points. TODO: explanation is clunky nfold = 5 TT = 100 inds = make_cv_tf_folds(nfold = nfold, TT = TT) print(inds) plot(NA, xlim = c(0,TT), ylim=1:2, ylab = &quot;&quot;, xlab = &quot;Data index of ylist&quot;, yaxt = &quot;n&quot;, xaxt=&quot;n&quot;) axis(1, at = c(1, seq(10, 100,10))) for(ifold in 1:nfold){ abline(v = inds[[ifold]], col = ifold, lwd = 2) } ## $Fold1 ## [1] 2 7 12 17 22 27 32 37 42 47 52 57 62 67 72 77 82 87 92 97 ## ## $Fold2 ## [1] 3 8 13 18 23 28 33 38 43 48 53 58 63 68 73 78 83 88 93 98 ## ## $Fold3 ## [1] 4 9 14 19 24 29 34 39 44 49 54 59 64 69 74 79 84 89 94 99 ## ## $Fold4 ## [1] 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 ## ## $Fold5 ## [1] 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 "],["testing-the-flowsmooth-method.html", "10 Testing the flowsmooth method 10.1 1d example 10.2 1d example with gap", " 10 Testing the flowsmooth method We’re going to take a huge leap, and assume the flowsmooth() function has been built. We’re going to test it now. library(tidyverse) ll &lt;- function(){ litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook()) devtools::load_all(&quot;~/repos/FlowTF&quot;) devtools::load_all(&#39;~/repos/flowsmooth/flowsmooth&#39;) } ll() 10.1 1d example ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 3) nrestart = 1) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) The objective value (that is, the penalized log likelihood) should be monotone across EM algorithm iterations. testthat::test_that(&quot;Objective value decreases over EM iterations.&quot;,{ devtools::load_all(&quot;~/repos/FlowTF&quot;) for(iseed in 1:5){ ## Generate increasingly noisy data set.seed(iseed*100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.2) dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, ## lambda_prob = .5, lambda_prob = 0.05, nrestart = 1) ## Test objective monotonicity testthat::expect_true(all(diff(obj$objective) &lt; 0)) } }) While the slight rise in objective value is not egregious, I would like to get to the bottom of this. The next code block shows a self-contained example of the objective value rising. I didn’t stop the algorithm (allowing the full niter=200 iterations) to see if it increases. Now I wonder – is it due to glmnet? If we use CVXR, will it go away? iseed = 1 set.seed(iseed * 100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.2) dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = 0.05, nrestart = 1, niter = 200, verbose = TRUE) ## TODO: remember to comment out the convergence check!! objectives = obj$objective for(iter in 2:200){ if(check_converge_rel(objectives[iter-1], objectives[iter], tol = 1E-4)) break } stopping_iter = iter print(range(diff(obj$objective))) plot(obj$objective, type =&#39;l&#39;, xlim = c(0,50)) ## Ok did it rise? abline(v=iter, lty = &#39;dotted&#39;) abline(h=min(obj$objective), lty = &#39;dotted&#39;, col = &#39;blue&#39;) plot(diff(obj$objective), type =&#39;l&#39;, ylim = c(-0.001, 0.0005)) ## Ok did it rise? abline(h=0, col = &#39;blue&#39;, lty = &#39;dotted&#39;) The data and estimated model are shown here. plot_1d(ylist, obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) The estimated probabilities are shown here. plot_prob(obj) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) 10.2 1d example with gap Repeating this exercise with data that has a gap (between times 25 and 35). held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowsmooth(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 2) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) The model estimates (the solid colored lines) at the gap (between 25 and 35) are automatically generated by ggplot::geom_line(), and no model estimates are being made here. But actually, this is what we want to do when making predictions at new time points. There’s more about this shortly. plot_1d(ylist, obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
