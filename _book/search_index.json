[["index.html", "Creating the flowtrend R package 1 Introduction", " Creating the flowtrend R package Sangwon Hyun 2023-04-08 1 Introduction This package implements flowtrend, a model used for smooth estimation of mixture models across time. The documentation and package are both created using one simple command: litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook()) "],["package-setup.html", "2 Package setup", " 2 Package setup The DESCRIPTION file is created using this code. usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;flowtrend&quot;, Description = &quot;Time-smooth mixture modeling for flow cytometry data.&quot;, `Authors@R` = person( given = &quot;Sangwon&quot;, family = &quot;Hyun&quot;, email = &quot;sangwonh@ucsc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;Sangwon Hyun&quot;) The following is what will show up when someone types package?flowtrend in the console. #&#39; flowtrend #&#39; #&#39; This package implements the `flowtrend` method for automatic gating of flow cytometry data using trend filtering. #&#39; #&#39; @docType package This package will have some dependancies: library(tidyverse) library(ggplot2) usethis::use_package(&quot;tidyverse&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_package(&quot;clue&quot;) "],["generating-1d-data.html", "3 Generating 1d data", " 3 Generating 1d data This function generates synthetic 1-dimensional data, and returns it in a “long” format matrix, with columns time, y, mu, and cluster. The latter two are the true underlying parameters. #&#39; Generates some synthetic 1-dimensional data with three clusters. Returns a #&#39; data frame with (1) time, (2) Y (3) mu (4) cluster assignments. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; @param die_off_time When the cluster probability dies off, in the #&#39; middle. For instance, 0.45 means it dies off at time \\code{.45*TT}, then #&#39; lives again at \\code{(1 - .45)*TT}. #&#39; @param return_model If true, return true cluster means and probabilities #&#39; instead of data. #&#39; #&#39; @return long matrix with data or model. #&#39; @export gendat_1d &lt;- function(TT, ntlist, die_off_time = 0.45, return_model = FALSE){ ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time probs &lt;- sapply(1:TT, FUN = function(tt){ if(TT * die_off_time &lt; tt &amp; tt &lt; (1-die_off_time) * TT){ cluster_prob &lt;- c(0.05, 0.95*rep(1/(3-1), 3-1)) } else { cluster_prob &lt;- rep(1/3, 3) } return(cluster_prob) }) %&gt;% t() colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) ## Make cluster means, by time means &lt;- matrix(NA, TT, 3) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1] &lt;- tt/TT + 0.5 means[tt, 2] &lt;- sin(seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 3] &lt;- -3+sin(seq(-1, 1, length.out = TT)[tt]*6.282) } } colnames(means) = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ## mutate(cluster = as.numeric(cluster)) if(return_model) return(model) ys &lt;- lapply(1:TT, FUN = function(tt){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, sd = 0.4)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 2){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, sd = .5)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 3){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, sd = .35)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } } Y &lt;- unlist(Y) mu &lt;- unlist(mu) cluster &lt;- rep(1:3, times = clusters_count) one_df = tibble(time = tt, Y = Y, mu = mu, cluster = cluster) return(one_df) }) %&gt;% bind_rows() return(ys) } dt2ylist() is a helper that takes the output generated from gendat_1d(), and splits it by the time column to create a ylist object, which is a \\(T\\)-length list of \\(n_t \\times d\\) matrices. #&#39; Converting to a list of matrices, \\code{ylist}, to input to \\code{flowtrend()}. #&#39; #&#39; @param dt Output from \\code{gendat_1d()}. #&#39; #&#39; @return List of matrices #&#39; @export dt2ylist &lt;- function(dt){ dt%&gt;% select(time, Y) %&gt;% arrange(time) %&gt;% group_by(time) %&gt;% group_split(.keep = FALSE) %&gt;% lapply(as.matrix) } Let’s generate some data using these functions. dt = gendat_1d(TT = 100, ntlist =rep(100,100)) print(dt) ## # A tibble: 10,000 × 4 ## time Y mu cluster ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.793 0.51 1 ## 2 1 0.365 0.51 1 ## 3 1 -0.395 0.51 1 ## 4 1 0.869 0.51 1 ## 5 1 0.452 0.51 1 ## 6 1 0.226 0.51 1 ## 7 1 0.341 0.51 1 ## 8 1 0.981 0.51 1 ## 9 1 1.24 0.51 1 ## 10 1 0.0614 0.51 1 ## # … with 9,990 more rows ylist = dt2ylist(dt) print(head(str(ylist[1:5]))) ## List of 5 ## $ : num [1:100, 1] 0.793 0.365 -0.395 0.869 0.452 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -0.0346 0.7322 0.6269 1.6358 0.3124 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -0.1544 0.687 0.6731 0.7829 0.0236 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] 0.646 0.459 -0.187 0.649 0.39 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] 0.772 0.7316 0.4177 0.7267 0.0709 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## NULL print(head(ylist[[1]])) ## Y ## [1,] 0.7928696 ## [2,] 0.3645849 ## [3,] -0.3953053 ## [4,] 0.8687907 ## [5,] 0.4523815 ## [6,] 0.2259644 Next, we’ll make some plotting functions 1d model and data. "],["plotting-1d-data.html", "4 Plotting 1d data", " 4 Plotting 1d data Given 1d data ylist and an estimated model object obj, we want to plot both in a single plot. plot_1d() lets you do this. #&#39; Makes 1d plot of data and model #&#39; #&#39; @param ylist Data. #&#39; @param obj flowtrend object. Defaults to NULL. #&#39; @param x time points. Defaults to NULL. #&#39; #&#39; @return ggplot object with data, and optionally, a flowtrend model overlaid. #&#39; @export plot_1d &lt;- function(ylist, obj=NULL, x = NULL, add_point = TRUE){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } ## make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() ## plot long matrix gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y), alpha = .1) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) ## theme(legend.position = &#39;none&#39;) if(is.null(obj)) return(gg) ## Add the model numclust = obj$numclust mnmat = obj$mn %&gt;% .[,1,] %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) est_long = full_join(mn_long, prob_long) gg = gg + geom_path(aes(x = time, y = mean, size = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;) if(add_point){ gg = gg + geom_line(aes(x = time, y = mean, size = prob, group = cluster), data = est_long, size = rel(1), shape = 17, col = &#39;black&#39;) } ## TODO: make it ignore the missing values at the gaps; currently this is not coded as NAs. ## Add the estimated 95% probability regions for data. stdev = obj$sigma %&gt;% .[,,1] %&gt;% sqrt() band_long = mn_long %&gt;% mutate(upper = case_when(cluster == &quot;1&quot; ~ mean + 1.96 * stdev[1], cluster == &quot;2&quot; ~ mean + 1.96 * stdev[2], cluster == &quot;3&quot; ~ mean + 1.96 * stdev[3]), lower = case_when(cluster == &quot;1&quot; ~ mean - 1.96 * stdev[1], cluster == &quot;2&quot; ~ mean - 1.96 * stdev[2], cluster == &quot;3&quot; ~ mean - 1.96 * stdev[3])) gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(1), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(1), alpha = .5) } The plotting function plot_1d() will be even more useful when we have a model, but can also simply plot the data ylist. Let’s try this out. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist, NULL, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model, linetype = &quot;dashed&quot;, size=2, alpha = .7) Voilà! Also, we will want to plot the estimated cluster probabilities of a model obj. #&#39; Makes cluster probability plot (lines over time). #&#39; #&#39; @param obj Estimated model (from e.g. \\code{flowtrend()}) #&#39; #&#39; @export plot_prob &lt;- function(obj, x = NULL){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } numclust = obj$numclust probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) prob_long %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster), size = rel(1)) + ggtitle(&quot;Estimated cluster probability&quot;) } We can’t test it out now, but we’ll use it later in 1d-example. "],["generating-2d-data.html", "5 Generating 2d data", " 5 Generating 2d data #&#39; Generates some synthetic 2-dimensional data with three clusters. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; @param die_off_time When the cluster probability dies off, in the #&#39; middle. For instance, 0.45 means it dies off at time \\code{.45*TT}, then #&#39; lives again at \\code{(1 - .45)*TT}. #&#39; #&#39; @return List containing (1) ylist, (2) mnlist, (3) clusterlist. #&#39; @export gendat_2d &lt;- function(TT, ntlist, die_off_time = 0.45){ ## ## Sample setup ## TT = 100 ## ntlist = rep(100, 100) ## die_off_time = .45 return_model = FALSE ## ## End of sample setup ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time probs &lt;- sapply(1:TT, FUN = function(tt){ cluster_prob1 = sin(tt/24 * 2 * pi) + 1 + (tt/TT)*5 cluster_prob2 = cos(tt/24 * 2 * pi) + 1 cluster_prob3 = 1 cluster_prob = c(cluster_prob1, cluster_prob2, cluster_prob3) cluster_prob = cluster_prob/sum(cluster_prob) return(cluster_prob) }) %&gt;% t() colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) ## Make cluster means, by time means &lt;- array(NA, dim = c(TT, 3, 2)) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1, 1] = means[tt, 1, 2] = tt/TT + 0.5 means[tt, 2, 1] = sin(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 2, 2] = 0 means[tt, 3, 1] = means[tt, 3, 2] = -3+cos(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*6.282) } } ## colnames(means) = c(1:3) dimnames(means)[[2]] = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ## mutate(cluster = as.numeric(cluster)) if(return_model) return(model) ## ys &lt;- lapply(1:TT, FUN = function(tt){ ylist = list() mulist = list() clusterlist = list() for(tt in 1:TT){ print(tt) Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii,,drop=TRUE] Sigma1 = matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2) ## MASS::mvrnorm(1000, mu=c(0,0), Sigma=Sigma1) %&gt;% plot() Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma= Sigma1)) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 2){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.5, 0.1)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 3){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.35, 0.35)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } } Y &lt;- Y %&gt;% purrr::compact() %&gt;% do.call(rbind, .) mu &lt;- mu %&gt;% purrr::compact() %&gt;% do.call(rbind, .) cluster &lt;- rep(1:3, times = clusters_count) ## one_df = tibble(time = tt, Y = Y, mu = mu, cluster = cluster) ylist[[tt]] = Y mulist[[tt]] = mu clusterlist[[tt]] = cluster } return(list(ylist = ylist, mulist = mulist, clusterlist = clusterlist, probs = probs, means = means)) } "],["trend-filtering.html", "6 Trend filtering", " 6 Trend filtering Trend filtering is a non-parametric regression technique for a sequence of output points \\(y = (y_1,..., y_T)\\) observed at locations \\(x = (x_1, ..., x_T)\\). It is usually assumed that \\(x_1, ..., x_T\\) are evenly spaced points, though this can be relaxed. The trend filtering estimate of order \\(l\\) of the time series \\(\\mu_t = \\mathbb{E}(y_t), t \\in x\\) is obtained by calculating \\[\\hat{\\mu} = \\mathop{\\mathrm{argmin}}_{\\mu \\in \\mathbb{R}^T} \\frac{1}{2}\\| \\mu - y\\|_2^2 + \\lambda \\| D^{(l+1)} \\mu\\|_1,\\] where \\(\\lambda\\) is a tuning parameter and \\(D^{(l+1)} \\in \\mathbb{R}^{T-l}\\) is the \\((l+1)^\\text{th}\\) order discrete differencing matrix. We first need to be able to construct the trend filtering “differencing matrix” used for smoothing the mixture parameters over time. The general idea of the trend filtering is explained masterfully in [Ryan’s paper, Section 6 and equation (41)]. The differencing matrix is formed by recursion, starting with \\(D^{(1)}\\). \\[\\begin{equation*} D^{(1)} = \\begin{bmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0\\\\ \\vdots &amp; &amp; &amp; &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{bmatrix}. \\end{equation*}\\] For \\(l&gt;1\\), the differencing matrox \\(D^{(l+1)}\\) is defined recursively as \\(D^{(l+1)} = D^{(1)} D^{(l)}\\), starting with \\(D^{(1)}\\). #&#39; Generating Difference Matrix of Specified Order #&#39; #&#39; @param n length of vector to be differenced #&#39; @param l order of differencing #&#39; @param x optional. Spacing of input points. #&#39; #&#39; @return A n by n-l-1 matrix #&#39; @export #&#39; #&#39; @examples gen_diff_mat &lt;- function(n, l, x = NULL){ ## Basic check if(!is.null(x)) stopifnot(length(x) == n) get_D1 &lt;- function(t) {do.call(rbind, lapply(1:(t-1), FUN = function(x){ v &lt;- rep(0, t) v[x] &lt;- -1 v[x+1] &lt;- 1 v }))} if(is.null(x)){ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% D } return(D) } } else{ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% diag(k/diff(x, lag = k)) %*% D } return(D) } } } For equally spaced inputs: TT = 10 l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) l = 2 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 -2 1 0 0 0 0 0 0 0 ## [2,] 0 1 -2 1 0 0 0 0 0 0 ## [3,] 0 0 1 -2 1 0 0 0 0 0 ## [4,] 0 0 0 1 -2 1 0 0 0 0 ## [5,] 0 0 0 0 1 -2 1 0 0 0 ## [6,] 0 0 0 0 0 1 -2 1 0 0 ## [7,] 0 0 0 0 0 0 1 -2 1 0 ## [8,] 0 0 0 0 0 0 0 1 -2 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 -2 1 0 0 0 0 0 0 0 ## [2,] 0 1 -2 1 0 0 0 0 0 0 ## [3,] 0 0 1 -2 1 0 0 0 0 0 ## [4,] 0 0 0 1 -2 1 0 0 0 0 ## [5,] 0 0 0 0 1 -2 1 0 0 0 ## [6,] 0 0 0 0 0 1 -2 1 0 0 ## [7,] 0 0 0 0 0 0 1 -2 1 0 ## [8,] 0 0 0 0 0 0 0 1 -2 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -1 3 -3 1 0 0 0 0 0 0 ## [2,] 0 -1 3 -3 1 0 0 0 0 0 ## [3,] 0 0 -1 3 -3 1 0 0 0 0 ## [4,] 0 0 0 -1 3 -3 1 0 0 0 ## [5,] 0 0 0 0 -1 3 -3 1 0 0 ## [6,] 0 0 0 0 0 -1 3 -3 1 0 ## [7,] 0 0 0 0 0 0 -1 3 -3 1 When the inputs have a gap in it: x = (1:10)[-(4:6)] l = 2 TT = length(x) Dl = gen_diff_mat(n = TT, l = l+1, x = x) print(Dl) Next, here’s a function to build a lasso regressor matrix \\(H\\) that can be used to solve an equivalent problem as the trend filtering of the ’th degree. (This is stated in Lemma 4, equation (25) from Tibshirani (2014)) #&#39; A lasso regressor matrix H that can be used to solve an equivalent problem as the trend filtering of the \\code{k}&#39;th degree. #&#39; #&#39; @param n Total number of time points. #&#39; @param k Degree of trend filtering for cluster probabilities. $k=0$ is fused lasso, $k=1$ is linear trend filtering, and so on. #&#39; @param x Time points #&#39; #&#39; @return $n$ by $n$ matrix. #&#39; #&#39; @export gen_tf_mat &lt;- function(n, k, x = NULL){ if(is.null(x) ){ x = (1:n)/n } if(!is.null(x)){ stopifnot(length(x) == n) } ## For every i,j&#39;th entry, use this helper function (from eq 25 of Tibshirani ## (2014)). gen_ij &lt;- function(i, j, k){ xi &lt;- x[i] if(j %in% 1:(k+1)){ return(xi^(j-1)) } if(j %in% (k+2):n){ ## Special handling for k==0, See lemma 4 eq 25 if(k == 0){ prd = 1 ind = j } if(k&gt;=1){ ind = j - (1:k) prd = prod(xi - x[ind]) } return(prd * ifelse(xi &gt;= x[max(ind)], 1, 0)) ## if(k &gt;= 1) prd = prod(xi - x[(j-k):(j-1)]) ## return(prd * ifelse(xi &gt;= x[(j-1)], 1, 0)) } } ## Generate the H matrix, entry-wise. H &lt;- matrix(nrow = n, ncol = n) for(i in 1:n){ for(j in 1:n){ H[i,j] &lt;- gen_ij(i,j, k) } } return(H) } Here’s a simple test, against an alternative function that only works for equally spaced data. #&#39; Creates trendfiltering regression matrix using Lemma 2 of Ryan Tibshirani&#39;s #&#39; trendfilter paper (2014); works on equally spaced data only. #&#39; #&#39; @param n Number of data points #&#39; @param k Order of trend filter. 0 is fused lasso, and so on. #&#39; @examples #&#39; ord = 1 #&#39; H_tf &lt;- gen_tf_mat_equalspace(n = 100, k = ord) #&#39; H_tf[,1] * 100 #&#39; H_tf[,2] * 100 #&#39; H_tf[,3] * 100 #&#39; H_tf[,4] * 100 #&#39; H_tf[,99] * 100 #&#39; H_tf[,100] * 100 #&#39; @return n by n matrix. gen_tf_mat_equalspace &lt;- function(n, k){ nn = n kk = k ##&#39; Connects kk to kk-1. sigm &lt;- function(ii, kk){ if(kk == 0) return(rep(1, ii)) cumsum(sigm(ii, kk-1)) } mat = matrix(NA, ncol = nn, nrow = nn) for(ii in 1:nn){ for(jj in 1:nn){ if(jj &lt;= kk+1) mat[ii,jj] = ii^(jj-1) / nn^(jj-1) if(ii &lt;= jj-1 &amp; jj &gt;= kk+2) mat[ii, jj] = 0 if(ii &gt; jj-1 &amp; jj &gt;= kk+2){ mat[ii, jj] = (sigm(ii-jj+1, kk) %&gt;% tail(1)) * factorial(kk) / nn^kk } } } return(mat) } testthat::test_that(&quot;Trend filtering regression matrix is created correctly.&quot;, { ## Check that equally spaced data creates same trendfilter regression matrix H1 &lt;- gen_tf_mat(10, 1) H1_other &lt;- gen_tf_mat(10, 1, x=(1:10)/10) testthat::expect_equal(H1, H1_other) ## Check the dimension testthat::expect_equal(dim(H1), c(10,10)) ## Check against an alternative function. for(ord in c(0,1,2,3,4)){ H &lt;- gen_tf_mat(10, ord) H_eq = gen_tf_mat_equalspace(10, ord) testthat::expect_true(max(abs(H_eq- H)) &lt; 1E-10) } }) "],["objective-data-log-likelihood.html", "7 Objective (data log-likelihood)", " 7 Objective (data log-likelihood) The function loglik_tt() calculates the log-likelihood of one cytogram, which is: \\[\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right). \\] #&#39; @param mu Cluster means. #&#39; @param prob Cluster probabilities. #&#39; @param prob Cluster variances. #&#39; @param ylist Data. #&#39; @param tt Time point. loglik_tt &lt;- function(ylist, tt, mu, sigma, prob, dimdat = NULL, countslist = NULL, numclust){ ## One particle&#39;s log likelihood (weighted density) weighted.densities = sapply(1:numclust, function(iclust){ if(dimdat == 1){ return(prob[tt,iclust] * dnorm(ylist[[tt]], mu[tt,,iclust], sqrt(sigma[iclust,,]))) } if(dimdat &gt; 1){ return(prob[tt,iclust] * dmvnorm_arma_fast(ylist[[tt]], mu[tt,,iclust], as.matrix(sigma[iclust,,]), FALSE)) } }) nt = nrow(ylist[[tt]]) counts = (if(!is.null(countslist)) countslist[[tt]] else rep(1, nt)) sum_wt_dens = rowSums(weighted.densities) sum_wt_dens = sum_wt_dens %&gt;% pmax(1E-100) return(sum(log(sum_wt_dens) * counts)) } Next, here is the function that calculates the entire objective from all cytograms, given model parameter mu, prob, and sigma. #&#39; Evaluating the penalized data log-likelihood on all data \\code{ylist} given parameters \\code{mu}, \\code{prob}, and \\code{sigma}. #&#39; #&#39; @param mu #&#39; @param prob #&#39; @param prob_link #&#39; @param sigma #&#39; @param ylist #&#39; @param Dl #&#39; @param l #&#39; @param lambda #&#39; @param l_prob #&#39; @param Dl_prob #&#39; @param lambda_prob #&#39; @param alpha #&#39; @param beta #&#39; @param denslist_by_clust #&#39; @param countslist #&#39; @param unpenalized if TRUE, return the unpenalized out-of-sample fit. #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples objective &lt;- function(mu, prob, prob_link = NULL, sigma, ## TT, N, dimdat, numclust, ylist, Dl, l = NULL, lambda = 0, l_prob = NULL, Dl_prob = NULL, lambda_prob = 0, alpha = NULL, beta = NULL, denslist_by_clust = NULL, countslist = NULL, unpenalized = FALSE){ ## Set some important variables TT = dim(mu)[1] numclust = dim(mu)[3] if(is.null(countslist)){ ntlist = sapply(ylist, nrow) } else { ntlist = sapply(countslist, sum) } N = sum(ntlist) dimdat = ncol(ylist[[1]]) ## Calculate the log likelihood loglik = sapply(1:TT, function(tt){ if(is.null(denslist_by_clust)){ return(loglik_tt(ylist, tt, mu, sigma, prob, countslist, numclust = numclust, dimdat = dimdat)) } else { return(loglik_tt_precalculate(ylist, tt, denslist_by_clust, prob, countslist, numclust)) ## TODO: This doesn&#39;t exist yet, but might need to, since.. speed! } }) if(unpenalized){ obj = -1/N * sum(unlist(loglik)) return(obj) } else { ## Return penalized likelihood mu.splt &lt;- asplit(mu, MARGIN = 3) diff_mu &lt;- sum(unlist(lapply(mu.splt, FUN = function(m) sum(abs(Dl %*% m))))) ## diff_prob &lt;- sum(abs(Dl_prob %*% log(prob * sapply(countslist, sum)))) diff_prob &lt;- sum(abs(Dl_prob %*% prob_link)) obj = -1/N * sum(unlist(loglik)) + lambda * diff_mu + lambda_prob * diff_prob return(obj) } } Here’s a helper to check numerical convergence of the EM algorithm. #&#39; Checks numerical improvement in objective value. Returns TRUE if |old-new|/|old| is smaller than tol. #&#39; #&#39; @param old Objective value from previous iteration. #&#39; @param new Objective value from current iteration. #&#39; @param tol Numerical tolerance. check_converge_rel &lt;- function(old, new, tol=1E-6){ return(abs((old-new)/old) &lt; tol ) } Here’s also a helper function to do the softmax-ing of \\(\\alpha_t \\in \\mathbb{R}^K\\). #&#39; Converts the Xbeta to softmax(Xbeta), so to speak. Xbeta is the linear functional of X from a multinomial regression; in our notation, it&#39;s alpha. #&#39; #&#39; @param prob_link alpha, which is a (T x K) matrix. #&#39; #&#39; @return exp(alpha)/rowSum(exp(alpha)). A (T x K) matrix. softmax &lt;- function(prob_link){ exp_prob_link = exp(prob_link) prob = exp_prob_link / rowSums(exp_prob_link) } testthat::test_that(&quot;Test for softmax&quot;,{ link = runif(100, min = -10, max = 10) %&gt;% matrix(nrow = 10, ncol = 10) testthat::expect_true(all(abs(rowSums(softmax(link)) - 1) &lt; 1E-13)) }) ## Test passed 🎉 "],["initial-parameters-for-em-algorithm.html", "8 Initial parameters for EM algorithm", " 8 Initial parameters for EM algorithm The EM algorithm requires some initial values for \\(\\mu\\), \\(\\pi\\) and \\(\\Sigma\\). Initializing \\(\\pi\\) is done in one line, prob = matrix(1/numclust, nrow = TT, ncol = numclust), which sets everything to \\(1/K\\). For \\(\\mu\\) and \\(\\Sigma\\), we write some functions. Essentially, initial means \\(\\mu\\) are jittered versions of a \\(K\\) means that are drawn from a downsampled version of \\(ylist\\) (downsampling is done because \\(ylist\\) can have a large number of particles). \\(\\Sigma\\) is \\(d\\times d\\) identity matrices, with fac=1 diagonal by default. TODO: better downsampling for binned data. TODO: think hard about best way to design sigma * TODO: this could be a good student project; best initialization techniques. How much does it take to get to the optimum? #&#39; Initialize the cluster centers. #&#39; #&#39; @param ylist A T-length list of (nt by 3) datasets. There should be T of #&#39; such datasets. 3 is actually \\code{mulen}. #&#39; @param numclust Number of clusters (M). #&#39; @param TT total number of (training) time points. #&#39; #&#39; @return An array of dimension (T x dimdat x M). #&#39; @export init_mn &lt;- function(ylist, numclust, TT, dimdat, countslist = NULL, seed=NULL){ if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) .Random.seed &lt;&lt;- seed } if(!is.null(countslist)){ ## Initialize the means by (1) collapsing to one cytogram (2) random ## sampling from this distribution, after truncation, TT = length(ylist) ylist_downsampled &lt;- lapply(1:TT, function(tt){ y = ylist[[tt]] counts = countslist[[tt]] ## Sample so that, in total, we get mean(nt)*30 sized sample. In the case ## of binned data, nt is the number of bins. if(nrow(y) &gt; 500) nsize = nrow(y) / TT * 30 else nsize = nrow(y) some_rows = sample(1:nrow(y), size = nsize, prob = counts/sum(counts)) y[some_rows,, drop=FALSE] }) yy = do.call(rbind, ylist_downsampled) new_means = yy[sample(1:nrow(yy), numclust),, drop=FALSE] jitter_sd = apply(yy, 2, sd) / 100 jitter_means = MASS::mvrnorm(n = nrow(new_means), mu = rep(0, dimdat), Sigma = diag(jitter_sd, ncol = dimdat)) new_means = new_means + jitter_means ## Repeat TT times. mulist = lapply(1:TT, function(tt){ new_means }) } else { TT = length(ylist) ylist_downsampled &lt;- lapply(1:TT, function(tt){ y = ylist[[tt]] counts = countslist[[tt]] nsize = pmin(nrow(y) / TT * 30, nrow(y)) y[sample(1:nrow(y), size = nsize),, drop=FALSE] }) ## Combine all the particles yy = do.call(rbind, ylist_downsampled) ## Get K new means from these inds = sample(1:nrow(yy), numclust) new_means = yy[inds,, drop=FALSE] mulist = lapply(1:TT, function(tt){ new_means }) } ## New (T x dimdat x numclust) array is created. muarray = array(NA, dim=c(TT, dimdat, numclust)) for(tt in 1:TT){ muarray[tt,,] = as.matrix(mulist[[tt]]) } return(muarray) } #&#39; Initialize the covariances. #&#39; #&#39; @param data The (nt by 3) datasets. There should be T of them. #&#39; @param numclust Number of clusters. #&#39; @param fac Value to use for the diagonal of the (dimdat x dimdat) covariance #&#39; matrix. #&#39; #&#39; @return An (K x dimdat x dimdat) array containing the (dimdat by dimdat) #&#39; covariances. #&#39; @export init_sigma &lt;- function(data, numclust, fac = 1){ ndat = nrow(data[[1]]) pdat = ncol(data[[1]]) sigmas = lapply(1:numclust, function(iclust){ onesigma = diag(fac * rep(1, pdat)) if(pdat==1) onesigma = as.matrix(fac) colnames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) rownames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) return(onesigma) }) sigmas = abind::abind(sigmas, along=0) return(sigmas) } Here’s a helper function for printing the progress. #&#39; A helper function to print the progress of a loop or simulation. #&#39; #&#39; @param isim Replicate number. #&#39; @param nsim Total number of replicates. #&#39; @param type Type of job you&#39;re running. Defaults to &quot;simulation&quot;. #&#39; @param lapsetime Lapsed time, in seconds (by default). #&#39; @param lapsetimeunit &quot;second&quot;. #&#39; @param start.time start time. #&#39; @param fill Whether or not to fill the line. #&#39; #&#39; @return No return print_progress &lt;- function(isim, nsim, type = &quot;simulation&quot;, lapsetime = NULL, lapsetimeunit = &quot;seconds&quot;, start.time = NULL, fill = FALSE){ ## If lapse time is present, then use it if(fill) cat(fill = TRUE) if(is.null(lapsetime) &amp; is.null(start.time)){ cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim) } else { if(!is.null(start.time)){ lapsetime = round(difftime(Sys.time(), start.time, units = &quot;secs&quot;), 0) remainingtime = round(lapsetime * (nsim-isim)/isim,0) endtime = Sys.time() + remainingtime } cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim, &quot;with lapsed time&quot;, lapsetime, lapsetimeunit, &quot;and remaining time&quot;, remainingtime, lapsetimeunit, &quot;and will finish at&quot;, strftime(endtime)) } if(fill) cat(fill = TRUE) } "],["e-step.html", "9 E step", " 9 E step #&#39; E step, which updates the &quot;responsibilities&quot;, which are posterior membership probabilities of each particle. #&#39; #&#39; @param mn #&#39; @param sigma #&#39; @param prob #&#39; @param ylist #&#39; @param numclust #&#39; @param denslist_by_clust #&#39; @param first_iter #&#39; @param countslist #&#39; #&#39; @return #&#39; @export #&#39; Estep &lt;- function(mn, sigma, prob, ylist = NULL, numclust, denslist_by_clust = NULL, first_iter = FALSE, countslist = NULL){ ## Basic setup TT = length(ylist) ntlist = sapply(ylist, nrow) resp = list() dimdat = dim(mn)[2] assertthat::assert_that(dim(mn)[1] == length(ylist)) ## Helper to calculate Gaussian density for each \\code{N(y_{t,k},mu_{t,k} and ## Sigma_k)}. calculate_dens &lt;- function(iclust, tt, y, mn, sigma, denslist_by_clust, first_iter) { mu &lt;- mn[tt, , iclust] if (dimdat == 1) { dens = dnorm(y, mu, sd = sqrt(sigma[iclust, , ])) } else { dens = dmvnorm_arma_fast(y, mu, sigma[iclust,,], FALSE) } return(dens) } ## Calculate posterior probability of membership of $y_{it}$. ncol.prob = ncol(prob) for (tt in 1:TT) { ylist_tt = ylist[[tt]] densmat &lt;- sapply(1:numclust, calculate_dens, tt, ylist_tt, mn, sigma, denslist_by_clust, first_iter) wt.densmat &lt;- matrix(prob[tt, ], nrow = ntlist[tt], ncol = ncol.prob, byrow = TRUE) * densmat wt.densmat = wt.densmat + 1e-10 wt.densmat &lt;- wt.densmat/rowSums(wt.densmat) resp[[tt]] &lt;- wt.densmat } ## Weight the responsibilities by $C_{it}$. if (!is.null(countslist)) { resp &lt;- Map(function(myresp, mycount) { myresp * mycount }, resp, countslist) } return(resp) } The E step should return a list of exactly the same size and format as ylist, which is a \\(T\\) -length list of matrices of size \\(n_t \\times d\\). testthat::test_that(&quot;E step returns appropriately sized responsibilities.&quot;,{ ## Generate some fake data TT = 100 ylist = lapply(1:TT, function(tt){ runif(90) %&gt;% matrix(ncol = 3, nrow = 30)}) numclust = 3 dimdat = 3 ## Initialize a few parameters, not carefully sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. ## Calculate responsibility ## TODO: the code fails here. why? resp = Estep(mn = mn, sigma = sigma, prob = prob, ylist = ylist, numclust = numclust) ## Check these things testthat::expect_equal(length(resp), length(ylist)) testthat::expect_equal(sapply(resp, dim), sapply(ylist, dim)) }) "],["m-step.html", "10 M step 10.1 M step for \\(\\pi\\) 10.2 M step for \\(\\Sigma\\) 10.3 M step for \\(\\mu\\)", " 10 M step The M step of the EM algorithm has three steps – one each for \\(\\mu\\), \\(\\pi\\), and \\(\\Sigma\\). 10.1 M step for \\(\\pi\\) #&#39; The M step for the cluster probabilities #&#39; #&#39; @param resp Responsibilities. #&#39; @param H_tf Trend filtering matrix. #&#39; @param countslist Particle multiplicities. #&#39; @param lambda_prob Regularization. #&#39; @param l_prob Trend filtering degree. #&#39; #&#39; @return (T x k) matrix containing the alphas, for \\code{prob = exp(alpha)/ #&#39; rowSums(exp(alpha))}. #&#39; @export #&#39; Mstep_prob &lt;- function(resp, H_tf, countslist = NULL, lambda_prob = NULL, l_prob = NULL, x = NULL){ ## Basic setup TT &lt;- length(resp) ## Basic checks stopifnot(is.null(l_prob) == is.null(lambda_prob)) ## If glmnet isn&#39;t actually needed, don&#39;t use it. if(is.null(l_prob) &amp; is.null(lambda_prob)){ ## Calculate the average responsibilities, per time point. if(is.null(countslist)){ resp.avg &lt;- lapply(resp, colMeans) %&gt;% do.call(rbind, .) } else { resp.avg &lt;- lapply(1:TT, FUN = function(ii){ colSums(resp[[ii]])/sum(countslist[[ii]]) }) %&gt;% do.call(rbind, .) } return(resp.avg) ## If glmnet is actually needed, use it. } else { lambda_range &lt;- function(lam, nlam = 50, lam.max = 5*lam){ return(exp(seq(log(lam.max), log(lam), length.out = nlam))) } penalty.facs &lt;- c(rep(0, l_prob+1), rep(1, nrow(H_tf) - l_prob - 1)) resp.predict &lt;- do.call(rbind, lapply(resp, colSums)) glmnet_obj &lt;- glmnet(x = H_tf, y = resp.predict, family = &quot;multinomial&quot;, penalty.factor = penalty.facs, maxit = 1e7, lambda = mean(penalty.facs)*lambda_range(lambda_prob), standardize = F, intercept = FALSE) ## todo: replicate the parameters. pred_link &lt;- predict(glmnet_obj, newx = H_tf, type = &quot;link&quot;, s = mean(penalty.facs) * lambda_prob)[,,1] return(pred_link) } } ## Things TODO: (1) check whether the resp.avg and result from penalized regression match, and (2) check if the scaling has entered the lambdas. This should return a \\(T\\) by \\(K\\) matrix, which we’ll test here: testthat::test_that(&quot;Mstep of pi returns a (T x K) matrix.&quot;, { ## Generate some fake responsibilities and trend filtering matrix TT = 100 numclust = 3 nt = 10 resp = lapply(1:TT, function(tt){ oneresp = runif(nt*numclust) %&gt;% matrix(ncol=numclust) oneresp = oneresp/rowSums(oneresp) }) H_tf &lt;- gen_tf_mat(n = TT, k = 0) ## Check the size pred_link = Mstep_prob(resp, H_tf, l_prob = 0, lambda_prob = 1E-3) testthat::expect_equal(dim(pred_link), c(TT, numclust)) pred_link = Mstep_prob(resp, H_tf) testthat::expect_equal(dim(pred_link), c(TT, numclust)) ## Check the correctness pred_link = Mstep_prob(resp, H_tf) }) Each row of this matrix should contain the fitted values \\(\\alpha_k \\in \\mathbb{R}^3\\) where \\(\\alpha_{kt} = h_t^T w_{k}\\), for.. \\(h_t\\) that are rows of the trend filtering matrix \\(H \\in \\mathbb{R}^{T \\times T}\\). \\(w_k \\in \\mathbb{R}^{n}\\) that are the regression coefficients estimated by glmnet(). Here is a test for the correctness of the M step for \\(\\pi\\). testthat::test_that(&quot;Test the M step of \\pi against CVXR&quot;, {}) 10.2 M step for \\(\\Sigma\\) #&#39; M step for cluster covariance (sigma). #&#39; #&#39; @param resp Responsibility. #&#39; @param ylist Data. #&#39; @param mn Means #&#39; @param numclust Number of clusters. #&#39; #&#39; @return (K x d x d) array containing K (d x d) covariance matrices. #&#39; @export #&#39; #&#39; @examples Mstep_sigma &lt;- function(resp, ylist, mn, numclust){ ## Find some sizes TT = length(ylist) ntlist = sapply(ylist, nrow) dimdat = ncol(ylist[[1]]) cs = c(0, cumsum(ntlist)) ## Set up empty residual matrix (to be reused) cs = c(0, cumsum(ntlist)) vars &lt;- vector(mode = &quot;list&quot;, numclust) ylong = do.call(rbind, ylist) ntlist = sapply(ylist, nrow) irows = rep(1:nrow(mn), times = ntlist) for(iclust in 1:numclust){ resp.thisclust = lapply(resp, function(myresp) myresp[,iclust, drop = TRUE]) resp.long = do.call(c, resp.thisclust) mnlong = mn[irows,,iclust] if(is.vector(mnlong)) mnlong = mnlong %&gt;% cbind() ## browser() ## vars[[iclust]] = estepC(ylong, mnlong, sqrt(resp.long), sum(resp.long)) ## TODO: see if this could be sped up. resid &lt;- ylong - mnlong resid_weighted &lt;- resp.long * resid sig_temp &lt;- t(resid_weighted) %*% resid/sum(resp.long) vars[[iclust]] &lt;- sig_temp } ## Make into an array sigma_array = array(NA, dim=c(numclust, dimdat, dimdat)) for(iclust in 1:numclust){ sigma_array[iclust,,] = vars[[iclust]] } ## Basic check stopifnot(all(dim(sigma_array) == c(numclust, dimdat, dimdat))) return(sigma_array) } 10.3 M step for \\(\\mu\\) This is a big one. It uses the ADMM written in section OO, reproduced briefly here. We need a convergence checker for the outer layer of LA-ADMM: #&#39; LA-ADMM requires an convergence check for the outer layer. #&#39; #&#39; @param objectives Objectives of the outer layer. #&#39; #&#39; @return TRUE if relative improvement is smaller than 1E-5 in the last four #&#39; outer iterations&#39; objective. #&#39; @export outer_converge &lt;- function(objectives){ consec = 4 if(length(objectives) &lt; consec){ return(FALSE) } else { mytail = utils::tail(objectives, consec) rel_diffs = mytail[1:(consec-1)]/mytail[2:consec] return(all(abs(rel_diffs) - 1 &lt; 1E-5)) } } Next, we define the main function Mstep_mu(). #&#39; Computes the M step for mu. TODO: use templates for the argument. As shown #&#39; here: #&#39; https://stackoverflow.com/questions/15100129/using-roxygen2-template-tags #&#39; #&#39; @param resp Responsbilities of each particle. #&#39; @param ylist #&#39; @param lambda #&#39; @param l #&#39; @param sigma #&#39; @param sigma_eig_by_clust #&#39; @param Dlm1 #&#39; @param Dl #&#39; @param TT #&#39; @param N #&#39; @param dimdat #&#39; @param first_iter #&#39; @param mus #&#39; @param Zs #&#39; @param Ws #&#39; @param uws #&#39; @param uzs #&#39; @param maxdev #&#39; @param x #&#39; @param niter #&#39; @param err_rel #&#39; @param err_abs #&#39; @param zerothresh #&#39; @param local_adapt #&#39; @param local_adapt_niter #&#39; @param space #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples Mstep_mu &lt;- function(resp, ylist, lambda = 0.5, l = 3, sigma, sigma_eig_by_clust = NULL, Dlm1sqrd, Dlm1, Dl, TT, N, dimdat, first_iter = TRUE, e_mat, ## Warm startable variables mus = NULL, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, ## End of warm startable variables maxdev = NULL, x = NULL, niter = (if(local_adapt) 1e3 else 1e4), err_rel = 1E-3, err_abs = 0, zerothresh = 1E-6, local_adapt = FALSE, local_adapt_niter = 10, space = 50){ #################### ## Preliminaries ### #################### TT = length(ylist) numclust = ncol(resp[[1]]) dimdat = ncol(ylist[[1]]) ntlist = sapply(ylist, nrow) resp.sum = lapply(resp, colSums) %&gt;% do.call(rbind, .) N = sum(unlist(resp.sum)) ## NEW (make more efficient, later) # starting rho for LA-ADMM if(local_adapt){ rho.init = 1e-3 } else{ if(!is.null(x)){ rho.init = lambda*((max(x) - min(x))/length(x))^l #print(rho.init) rho.init = lambda }else{ rho.init = lambda } } ## Other preliminaries schur_syl_A_by_clust = schur_syl_B_by_clust = term3list = list() ybarlist = list() ycentered_list = Xcentered_list = yXcentered_list = list() Qlist = list() sigmainv_list = list() convergences = list() for(iclust in 1:numclust){ ## Retrieve sigma inverse from pre-computed SVD, if necessary if(is.null(sigma_eig_by_clust)){ sigmainv = solve(sigma[iclust,,]) } else { sigmainv = sigma_eig_by_clust[[iclust]]$sigma_inv } resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) ## Center y and X # obj &lt;- weight_ylist(iclust, resp, resp.sum, ylist) #ycentered &lt;- obj$ycentered ## Form the Sylvester equation coefficients in AX + XB + C = 0 # syl_A = rho * sigma[iclust,,] # Q = 1/N * t(Xcentered) %*% D %*% Xcentered # syl_B = Q %*% Xinv AB &lt;- get_AB_mats(y = y, resp = resp.iclust, Sigma_inv = sigmainv, e_mat = e_mat, N = N, Dl = Dl, Dlm1 = Dlm1, Dlm1sqrd = Dlm1sqrd, rho = rho.init, z = NULL, w = NULL, uz = NULL, uw = NULL) ## Store the Schur decomposition schur_syl_A_by_clust[[iclust]] = myschur(AB$A) schur_syl_B_by_clust[[iclust]] = myschur(AB$B) ## Calculate coefficients for objective value calculation # Qlist[[iclust]] = Q ## ## Also calculate some things for the objective value ## ylong = sweep(do.call(rbind, ylist), 2, obj$ybar) ## longwt = do.call(c, lapply(1:TT, function(tt){ resp[[tt]][,iclust]})) %&gt;% sqrt() ## wt.long = longwt * ylong ## wt.ylong = longwt * ylong ## crossprod(wt.ylong, wt.ylong) ## Store the third term # term3list[[iclust]] = 1 / N * sigmainv %*% yXcentered # ybarlist[[iclust]] = obj$ybar # ycentered &lt;- NULL ycentered_list[[iclust]] = ycentered #print(ycentered) # Xcentered_list[[iclust]] = Xcentered # yXcentered_list[[iclust]] = yXcentered sigmainv_list[[iclust]] = sigmainv } ########################################## ## Run ADMM separately on each cluster ## ######################################### yhats = admm_niters = admm_inner_iters = vector(length = numclust, mode = &quot;list&quot;) if(first_iter) mus = vector(length = numclust, mode = &quot;list&quot;) # if(first_iter){ Zs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat) ) Ws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = dimdat, ncol = TT - l)) uzs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat) ) uws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = dimdat, ncol = TT - l)) # Zs = Ws = Us = vector(length = numclust, mode = &quot;list&quot;) # } fits = matrix(NA, ncol = numclust, nrow = ceiling(niter / space)) #browser() ## For every cluster, run LA-ADMM resid_mat_list = list() start.time = Sys.time() for(iclust in 1:numclust){ resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) resp.sum.iclust &lt;- lapply(resp.sum, FUN = function(r) matrix(r[iclust])) ## Possibly locally adaptive ADMM, for now just running with rho == lambda res = la_admm_oneclust(K = (if(local_adapt) local_adapt_niter else 1), local_adapt = local_adapt, iclust = iclust, niter = niter, TT = TT, N = N, dimdat = dimdat, maxdev = maxdev, schurA = schur_syl_A_by_clust[[iclust]], schurB = schur_syl_B_by_clust[[iclust]], #term3 = term3list[[iclust]], sigmainv = sigmainv_list[[iclust]], # Xinv = Xinv, # Xaug = Xaug, # Xa = Xa, rho = rho.init, rhoinit = rho.init, sigma = sigma, lambda = lambda, resp = resp.iclust, l = l, Dl = Dl, Dlm1 = Dlm1, #resp.sum = resp.sum.iclust, y = ylist, err_rel = err_rel, err_abs = err_abs, zerothresh = zerothresh, sigma_eig_by_clust = sigma_eig_by_clust, space = space, objective = F, ## Warm starts from previous *EM* iteration first_iter = first_iter, # beta = betas[[iclust]], #mu = mus[[iclust]], uw = uws[[iclust]], uz = uzs[[iclust]], z = Zs[[iclust]], w = Ws[[iclust]] ) ## Store the results mus[[iclust]] = res$mu yhats[[iclust]] = t(res$yhat) ## fits[,iclust] = res$fits ## TODO: revive this, for testing? We&#39;ll see. admm_niters[[iclust]] = res$kk admm_inner_iters[[iclust]] = res$inner.iter ## Store other things for for warmstart Zs[[iclust]] = res$Z uzs[[iclust]] = res$uz uws[[iclust]] = res$uw Ws[[iclust]] = res$W ## The upper triangular matrix remains the same. ## The upper triangular matrix remains the same. resid_mat_list[[iclust]] = res$resid_mat ## temporary convergences[[iclust]] = res$converge # print(res$converge) } ## Aggregate the yhats into one array yhats_array = array(NA, dim = c(TT, dimdat, numclust)) for(iclust in 1:numclust){ yhats_array[,,iclust] = yhats[[iclust]] } ## Each are lists of length |numclust|. return(list(mns = yhats_array, ## fits = fits, resid_mat_list = resid_mat_list, convergences = convergences, admm_niters = admm_niters, ## Temporary: Seeing the number of ## outer iterations it took to ## converge. admm_inner_iters = admm_inner_iters, ## For warmstarts Zs = Zs, Ws = Ws, uws = uws, uzs = uzs, N = N, ## For using in the Sigma M step ycentered_list = ycentered_list, Xcentered_list = Xcentered_list, yXcentered_list = yXcentered_list, Qlist = Qlist )) } TODO: Right now, sigma_eig_by_clust is not used. When speeding up the code, do this first. Likewise, ycentered_list isn’t used, while it is clearly useful in Sigma M step #&#39; Testing against \\code{Mstep_mu()}, for ONE cluster. #&#39; @param ylist #&#39; @param resp #&#39; @param lambda #&#39; @param l #&#39; @param Sigma_inv inverse of Sigma Mstep_mu_cvxr &lt;- function(ylist, resp, lambda, l, Sigma_inv, thresh = 1E-8, maxdev = NULL, dimdat, N, ecos_thresh = 1E-8, scs_eps = 1E-5){ ## Define dimensions TT = length(ylist) ## Responsibility Weighted Data ytildes &lt;- lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- resp[[tt]] yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) colSums(yy) }) %&gt;% bind_rows() %&gt;% as.matrix() ## Auxiliary term, needed to make the objective interpretable aux.y &lt;- Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- sqrt(resp[[tt]]) yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) sum(diag(yy %*% Sigma_inv %*% t(yy))) })) ## Mu, d x T matrix mumat &lt;- CVXR::Variable(cols=dimdat, rows=TT) ## Summed sqrt responsibilities - needed in the objective. resp.sum.sqrt &lt;- lapply(resp, FUN = function(x) sqrt(sum(x))) ## Differencing Matrix, TT-l + 1 x TT Dl &lt;- gen_diff_mat(n = TT, l = l) ## Forming the objective obj = 1/(2*N) *( Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) CVXR::quad_form(resp.sum.sqrt[[tt]]*mumat[tt,], Sigma_inv))) -2 * Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% mumat[tt,])) + aux.y) + lambda * sum(CVXR::sum_entries(abs(Dl %*% mumat), axis = 1)) ## Putting together the ball constraint rowmns &lt;- matrix(rep(1, TT^2), nrow = TT)/TT mu_dotdot &lt;- rowmns %*% mumat constraints = list() if(!is.null(maxdev)){ constraints = list(CVXR::sum_entries(CVXR::square(mumat - mu_dotdot), axis = 2) &lt;= rep(maxdev^2, TT) ) } ## Try all two CVXR solvers. prob &lt;- CVXR::Problem(CVXR::Minimize(obj), constraints) result = NULL result &lt;- tryCatch({ CVXR::solve(prob, solver=&quot;ECOS&quot;, FEASTOL = ecos_thresh, RELTOL = ecos_thresh, ABSTOL = ecos_thresh) }, error=function(err){ err$message = paste(err$message, &quot;\\n&quot;, &quot;Lasso solver using ECOS has failed.&quot; ,sep=&quot;&quot;) cat(err$message, fill=TRUE) return(NULL) }) ## If anything is wrong, flag to use SCS solver scs = FALSE if(is.null(result)){ scs = TRUE } else { if(result$status != &quot;optimal&quot;) scs = TRUE } ## Use the SCS solver if(scs){ result = CVXR::solve(prob, solver=&quot;SCS&quot;, eps = scs_eps) if(any(is.na(result$getValue(mumat)))){ ## A clumsy way to check. stop(&quot;Lasso solver using both ECOS and SCS has failed.&quot;, sep=&quot;&quot;) } } ## Record Interesting Parameters num_iters &lt;- result$num_iters status &lt;- result$status mumat &lt;- result$getValue(mumat) val &lt;- result$value return(list(mu = mumat, value = val, status = status, num_iters = num_iters)) } This function solves the following problem: \\[ \\begin{align*} &amp;\\text{minimize}_{\\mu} {\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^\\top \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l)}\\mu_{j\\cdot }\\|_1}\\\\ &amp;\\text{subject to}\\;\\; {\\| \\mu_{\\cdot t} - \\bar{\\mu}_{\\cdot \\cdot}\\|_2 \\le r \\;\\;\\forall t=1,\\cdots, T, } \\end{align*} \\] and is directly equivalent to Mstep_mu(). The resulting solution and the objective value should be the same. Let’s check that. First, set up some objects to run Mstep_mu() and Mstep_mu_cvxr(). numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .01 l = 1 x = 1:TT Dl = gen_diff_mat(n = TT, l = l+1, x = x) Dlm1 = gen_diff_mat(n = TT, l = l, x = x) Dlm1sqrd &lt;- t(Dlm1) %*% Dlm1 maxdev = NULL Then, compare the result of the two implementations. They should look identical. ## overall ADMM res1 = Mstep_mu(resp, ylist, lambda, l=l, sigma=sigma, Dlm1sqrd = Dlm1sqrd, Dlm1=Dlm1, Dl=Dl, TT=TT, N=N, dimdat=dimdat, e_mat=etilde_mat(TT = TT), maxdev = maxdev) mn1 = res1$mns ## CVXR just ONE cluster res2list = lapply(1:numclust, function(iclust){ Sigma_inv_oneclust = solve(sigma[iclust,,]) resp_oneclist = lapply(resp, function(resp_onetime){resp_onetime[,iclust, drop=FALSE]}) N = sum(unlist(resp)) res2 = Mstep_mu_cvxr(ylist, resp_oneclist, lambda, l+1, Sigma_inv_oneclust, thresh = 1E-8, maxdev = maxdev, dimdat, N) res2$mu }) mn2 = array(NA, dim=c(100, dimdat, 3)) for(iclust in 1:numclust){ mn2[,,iclust] = res2list[[iclust]] %&gt;% as.matrix() } ## Plot! plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm vs cvxr&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn1[,1,] %&gt;% matlines(lwd = 1, lty = 1) mn2[,1,] %&gt;% matlines(lwd = 3, lty = 3) TODO: We just need to bundle this into testthat style tests, and make sure to test several lambda values. TODO: maybe do this for unevenly spaced inputs. CVXR needs to take a different D matrix. testthat::test_that(&quot;Test the M step of \\mu against CVXR&quot;, {}) "],["flowtrend.html", "11 flowtrend", " 11 flowtrend Now we’ve assembled all ingredients we need, we’ll build the main function flowtrend_once() to estimate a flowtrend model. Here goes: #&#39; Estimate flowtrend model once. #&#39; #&#39; @param ylist Data. #&#39; @param countslist Counts corresponding to multiplicities. #&#39; @param x Times, if points are not evenly spaced. Defaults to NULL, in which #&#39; case the value becomes \\code{1:T}, for the $T==length(ylist)$. #&#39; @param numclust Number of clusters. #&#39; @param niter Maximum number of EM iterations. #&#39; @param l Degree of differencing for the mean trend filtering #&#39; @param l_prob Degree of differencing for the probability trend filtering #&#39; @param mn Initial value for cluster means. Defaults to NULL, in which case #&#39; initial values are randomly chosen from the data. #&#39; @param lambda Smoothing parameter for means #&#39; @param lambda_prob Smoothing parameter for probabilities #&#39; @param verbose Loud or not? EM iteration progress is printed. #&#39; @param tol_em Relative numerical improvement of the objective value at which #&#39; to stop the EM algorithm #&#39; @param maxdev Maximum deviation of cluster means across time.. #&#39; @param countslist_overwrite #&#39; @param admm_err_rel #&#39; @param admm_err_abs #&#39; @param admm_local_adapt #&#39; @param admm_local_adapt_niter #&#39; #&#39; @return List object with flowtrend model estimates. #&#39; @export #&#39; #&#39; @examples flowtrend_once &lt;- function(ylist, countslist = NULL, x = NULL, numclust, niter = 1000, l, l_prob = NULL, mn = NULL, lambda = 0, lambda_prob = NULL, verbose = FALSE, tol_em = 1E-4, maxdev = NULL, countslist_overwrite = NULL, ## beta Mstep (ADMM) settings admm = TRUE, admm_err_rel = 1E-3, admm_err_abs = 1E-4, ## Mean M step (Locally Adaptive ADMM) settings admm_local_adapt = FALSE, admm_local_adapt_niter = if(admm_local_adapt) 10 else 1){ ## Basic checks if(!is.null(maxdev)){ assertthat::assert_that(maxdev!=0) } else { maxdev = 1E10 } assertthat::assert_that(numclust &gt; 1) assertthat::assert_that(niter &gt; 1) if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } ## Setup for EM algorithm TT = length(ylist) dimdat = ncol(ylist[[1]]) if(is.null(x)) x &lt;- 1:TT Dl = gen_diff_mat(n = TT, l = l+1, x = x) Dlm1 = gen_diff_mat(n = TT, l = l, x = x) Dlm1sqrd &lt;- t(Dlm1) %*% Dlm1 e_mat &lt;- etilde_mat(TT = TT) # needed to generate B Dl_prob = gen_diff_mat(n = TT, l = l_prob+1, x = x) H_tf &lt;- gen_tf_mat(n = length(countslist), k = l_prob, x = x) if(is.null(mn)) mn = init_mn(ylist, numclust, TT, dimdat, countslist = countslist) ntlist = sapply(ylist, nrow) N = sum(ntlist) ## Initialize some objects prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. denslist_by_clust &lt;- NULL objectives = c(+1E20, rep(NA, niter-1)) sigma_fac &lt;- diff(range(do.call(rbind, ylist)))/8 sigma = init_sigma(ylist, numclust, sigma_fac) ## (T x numclust x (dimdat x dimdat)) sigma_eig_by_clust = NULL zero.betas = zero.alphas = list() ## The least elegant solution I can think of.. used only for blocked cv if(!is.null(countslist_overwrite)) countslist = countslist_overwrite #if(!is.null(countslist)) check_trim(ylist, countslist) vals &lt;- vector(length = niter) start.time = Sys.time() for(iter in 2:niter){ if(verbose){ print_progress(iter-1, niter-1, &quot;EM iterations.&quot;, start.time = start.time) } resp &lt;- Estep(mn, sigma, prob, ylist = ylist, numclust = numclust, denslist_by_clust = denslist_by_clust, first_iter = (iter == 2), countslist = countslist) ## M step (three parts) ## 1. Means res.mu = Mstep_mu(resp, ylist, lambda = lambda, first_iter = (iter == 2), l = l, Dl = Dl, Dlm1 = Dlm1, Dlm1sqrd = Dlm1sqrd, sigma_eig_by_clust = sigma_eig_by_clust, sigma = sigma, maxdev = maxdev, e_mat = e_mat, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, x = x, err_rel = admm_err_rel, err_abs = admm_err_abs, local_adapt = admm_local_adapt, local_adapt_niter = admm_local_adapt_niter) mn = res.mu$mns ## 2. Sigma sigma = Mstep_sigma(resp, ylist, mn, numclust) # sigma_eig_by_clust &lt;- eigendecomp_sigma_array(sigma) # denslist_by_clust &lt;- make_denslist_eigen(ylist, mn, TT, dimdat, numclust, # sigma_eig_by_clust, # countslist) ## 3. Probabilities prob_link = Mstep_prob(resp, countslist = countslist, H_tf = H_tf, lambda_prob = lambda_prob, l_prob = l_prob, x = x) prob = softmax(prob_link) objectives[iter] = objective(ylist = ylist, mu = mn, sigma = sigma, prob = prob, prob_link = prob_link, lambda = lambda, Dl = Dl, l = l, countslist = countslist, Dl_prob = Dl_prob, l_prob = l_prob, lambda_prob = lambda_prob) ## Check convergence if(check_converge_rel(objectives[iter-1], objectives[iter], tol = tol_em)) break } return(structure(list(mn = mn, prob = prob, prob_link = prob_link, sigma = sigma, objectives = objectives[2:iter], final.iter = iter, resp = resp, ## Above is output, below are data/algorithm settings. dimdat = dimdat, TT = TT, N = N, l = l, x = x, numclust = numclust, lambda = lambda, lambda_prob = lambda_prob, maxdev = maxdev, niter = niter ), class = &quot;flowtrend&quot;)) } Next, flowtrend() is the main user-facing function. #&#39; Main function. Repeats the EM algorithm (\\code{flowtrend_once()}) with |nrep| restarts (5 by default). #&#39; #&#39; @param nrestart : number of random restarts #&#39; @param ... : arguments for \\code{flowtrend_once()} #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples flowtrend &lt;- function(nrestart = 10, ...){ out.models &lt;- lapply(1:nrestart, FUN = function(x){ model.temp &lt;- flowtrend_once(...) model.obj &lt;- tail(model.temp$objectives, n = 1) return(list(model = model.temp, objective = model.obj)) }) objectives &lt;- sapply(out.models, FUN = function(x) x$objective) best.model &lt;- which.min(objectives) return(out.models[[best.model]][[&quot;model&quot;]]) } "],["reordering-clusters.html", "12 Reordering clusters", " 12 Reordering clusters It’s useful to be able to reorder, or permute, one model’s cluster labels (cluster 1,2,.. of newres which are arbitrary) to that of another model origres. The function reorder_kl() does this by (1) taking the posterior probabilities of the particles in ylist_particle (row-binded to be a \\(\\sum_t n_t \\times K\\) matrix), and then (2) using a Hungarian algorithm [@kuhn-hungarian] to best match the two elongated matrices \\(A\\) and \\(B\\) by measuring the (symmetric? TODO check) KL divergence between all permutations of the \\(K\\) columns (TODO: This is currently just copy-pasted from flowmix. But actually, this is an example of a function that can be directly borrowed; all the work is done in reorder_clust(). So we’ll deal with that!) #&#39; Reorder the cluster numbers for a new flowtrend object \\code{newres}; the best #&#39; permutation (reordering) is to match the original flowmix object #&#39; \\code{origres}. #&#39; #&#39; @param newres New flowtrend object to reorder. #&#39; @param origres Original flowtrend object. #&#39; @param ylist_particle The particle-level data. #&#39; @param fac Defaults to 100, to take 1/100&#39;th of the particles from each time point. #&#39; @param verbose Loud or not? #&#39; #&#39; @return Reordered res #&#39; #&#39; @export reorder_kl &lt;- function(newres, origres, ylist_particle, fac = 100, verbose = FALSE){ ## Randomly sample 1/100 of the original particles (mainly for memory reasons) TT = length(ylist_particle) N = sapply(ylist_particle, nrow) %&gt;% sum() ntlist = sapply(ylist_particle, nrow) indlist = lapply(1:TT, function(tt){ nt = ntlist[[tt]] ind = sample(1:nt, round(nt / fac), replace=FALSE) }) ## Sample responsibilities ylist_particle_small = Map(function(ind, y){ y[ind,,drop = FALSE] }, indlist, ylist_particle) ## Calculate new responsibilities resp_orig_small &lt;- Estep(origres$mn, origres$sigma, origres$prob, ylist = ylist_particle_small, numclust = origres$numclust, first_iter = TRUE) resp_new_small &lt;- Estep(newres$mn, newres$sigma, newres$prob, ylist = ylist_particle_small, numclust = newres$numclust, first_iter = TRUE) assertthat::assert_that(all(sapply(resp_orig_small, dim) == sapply(resp_new_small, dim))) ## Get best ordering (using symm. KL divergence and Hungarian algorithm for ## matching) best_ord &lt;- get_best_match_from_kl(resp_new_small, resp_orig_small) if(verbose) cat(&quot;New order is&quot;, best_ord, fill=TRUE) newres_reordered_kl = newres %&gt;% reorder_clust(ord = best_ord) ## Return the reordered object return(newres_reordered_kl) } This function uses get_best_match_from_kl(), which takes two lists containing responsibilities (posterior probabilities of particles) – one from each model – and returns the cluster ordering to apply to the model that produced resp_new. We define this function and a couple of helper functions next. #&#39; Compute KL divergence from responsibilities between two models&#39; #&#39; responsibilities \\code{resp_new} and \\code{resp_old}. #&#39; #&#39; @param resp_new New responsibilities #&#39; @param resp_orig Original responsiblities. #&#39; #&#39; @return Calculate reordering \\code{o} of the clusters in model represented #&#39; by \\code{resp_new}. To be clear, \\code{o[i]} of new model is the best #&#39; match with the i&#39;th cluster of the original model. #&#39; #&#39; @export #&#39; @importFrom clue solve_LSAP get_best_match_from_kl &lt;- function(resp_new, resp_orig){ ## Basic checks . = NULL ## Fixing check() assertthat::assert_that(all(sapply(resp_new, dim) == sapply(resp_orig , dim))) ## Row-bind all the responsibilities to make a long matrix distmat = form_symmetric_kl_distmat(resp_orig %&gt;% do.call(rbind,.), resp_new %&gt;% do.call(rbind,.)) ## Use Hungarian algorithm to solve. fit &lt;- clue::solve_LSAP(distmat) o &lt;- as.numeric(fit) ## Return the ordering return(o) } ##&#39; From two probability matrices, form a (K x K) distance matrix of the ##&#39; (n)-vectors. The distance between the vectors is the symmetric KL ##&#39; divergence. ##&#39; ##&#39; @param mat1 Matrix 1 of size (n x K). ##&#39; @param mat2 Matrix 2 of size (n x K). ##&#39; ##&#39; @return K x K matrix containing symmetric KL divergence of each column of ##&#39; \\code{mat1} and \\code{mat2}. form_symmetric_kl_distmat &lt;- function(mat1, mat2){ ## Manually add some small, in case some columns are all zero mat1 = (mat1 + 1E-10) %&gt;% pmin(1) mat2 = (mat2 + 1E-10) %&gt;% pmin(1) ## Calculate and return distance matrix. KK1 = ncol(mat1) KK2 = ncol(mat2) distmat = matrix(NA, ncol=KK2, nrow=KK1) for(kk1 in 1:KK1){ for(kk2 in 1:KK2){ mydist = symmetric_kl(mat1[,kk1, drop=TRUE], mat2[,kk2, drop=TRUE]) distmat[kk1, kk2] = mydist } } stopifnot(all(!is.na(distmat))) return(distmat) } ##&#39; Symmetric KL divergence, of two probability vectors. ##&#39; ##&#39; @param vec1 First probability vector. ##&#39; @param vec2 Second prbability vector. ##&#39; ##&#39; @return Symmetric KL divergence (scalar). symmetric_kl &lt;- function(vec1, vec2){ stopifnot(all(vec1 &lt;= 1) &amp; all(vec1 &gt;= 0)) stopifnot(all(vec2 &lt;= 1) &amp; all(vec2 &gt;= 0)) kl &lt;- function(vec1, vec2){ sum(vec1 * log(vec1 / vec2)) } return((kl(vec1, vec2) + kl(vec2, vec1))/2) } Finally, the function that actually performs the manual reordering the clusters of an estimated model obj is reorder_clust(). #&#39; Reorder the results of one object so that cluster 1 through #&#39; \\code{numclust} is in a particular order. The default is decreasing order of #&#39; the averages (over time) of the cluster means. #&#39; #&#39; @param res Model object. #&#39; @param ord Defaults to NULL. Use if you have an ordering in mind. #&#39; #&#39; @return Same object, but with clusters reordered. #&#39; #&#39; @export reorder_clust &lt;- function(res, ord = NULL){ ## Find an order by sums (averages) if(is.null(ord)) ord = res$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing = TRUE) if(!is.null(ord)) all(sort(ord) == 1:res$numclust) ## Reorder mean res$mn = res$mn[,,ord, drop=FALSE] ## Reorder sigma res$sigma = res$sigma[ord,,,drop=FALSE] ## Reorder prob res$prob = res$prob[,ord, drop=FALSE] ## Reorder the responsibilities if(&#39;resp&#39; %in% res){ resp_temp = list() for(tt in 1:TT){ rep_temp[[tt]] = res$resp[[tt]][,ord] } } return(res) } Here’s an example of how to use this. devtools::load_all(&quot;~/repos/FlowTF&quot;) set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model twice. set.seed(2) objlist &lt;- lapply(1:2, function(isim){ flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 1, verbose = TRUE)}) ## Perform the reordering, make three plots newres = objlist[[1]] origres = objlist[[2]] newres_reordered = reorder_kl(newres, origres, ylist, fac = 100, verbose = FALSE) plot_1d(ylist, newres, x = x, add_point = FALSE) + ggtitle(&quot;before reordering, model 1&quot;) plot_1d(ylist, origres, x = x, add_point = FALSE) + ggtitle(&quot;model 2&quot;) plot_1d(ylist, newres_reordered, x = x, add_point = FALSE) + ggtitle(&quot;reordered model 1&quot;) "],["tuning-the-regularization-parameters-for-flowtrend.html", "13 Tuning the regularization parameters for flowtrend 13.1 Predicting and evaluating on new time points 13.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test 13.3 Define CV data folds 13.4 CV = many single jobs 13.5 Running cross-validation 13.6 Summarizing the output 13.7 CV on your own computer 13.8 Next up", " 13 Tuning the regularization parameters for flowtrend We’re going to take a (huge) leap, and assume the flowtrend() function has been built. We need to build up quite a few functions before we’re able to do cross-validation. These include: Predicting out-of-sample, using predict_flowtrend(). Evaluating data fit (by likelihood) in an out-of-sample measurement, using objective(..., unpenalized = TRUE). Numerically estimating the maximum regularization values to test, using get_max_lambda(). Making data splits, using make_cv_folds(). 13.1 Predicting and evaluating on new time points First, let’s write a couple of functions interpolate_mn() and interpolate_prob() which linearly interpolate the means and probabilities at new time points. #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param mn length(x) by dimdat by numclust matrix. #&#39; #&#39; @return A dimdat-length vector. interpolate_mn &lt;- function(x, tt, iclust, mn){ ## Basic checks stopifnot(length(x) == dim(mn)[1]) stopifnot(iclust &lt;= dim(mn)[3]) if(tt %in% x) return(mn[which(x==tt),,iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation mn_t &lt;- mn[ceiling_t_ind,,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + mn[floor_t_ind,,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) ## Basic checks stopifnot(length(mn_t) == dim(mn)[2]) return(mn_t) } #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param prob length(x) by numclust array or matrix. #&#39; #&#39; @return One probability. interpolate_prob &lt;- function(x, tt, iclust, prob){ ## Basic checks numdat = dim(prob)[1] numclust = dim(prob)[2] stopifnot(length(x) == numdat) stopifnot(iclust &lt;= numclust) if(tt %in% x) return(prob[which(x == tt),iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation prob_t &lt;- prob[ceiling_t_ind,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + prob[floor_t_ind,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t ## Basic checks stopifnot(length(prob_t) == 1) stopifnot(0 &lt;= prob_t &amp; prob_t &lt;= 1) return(prob_t) } Next, let’s build a prediction function predict_flowtrend() which takes the model object obj, and the new time points newtimes, and produces. #&#39; Prediction: Given new timepoints in the original time interval,generate a set #&#39; of means and probs (and return the same Sigma). #&#39; #&#39; @param obj Object returned from covariate EM flowtrend(). #&#39; @param newtimes New times at which to make predictions. #&#39; #&#39; @return List containing mean, prob, and sigma, and x. #&#39; #&#39; @export #&#39; predict_flowtrend &lt;- function(obj, newtimes = NULL){ ## Check the dimensions newx &lt;- newtimes if(is.null(newtimes)){ newx = obj$x } ## Check if the new times are within the time range of the original data stopifnot(all(sapply(newx, FUN = function(t) t &gt;= min(obj$x) &amp; t &lt;= max(obj$x)))) ## Setup some things x &lt;- obj$x TT_new = length(newx) numclust = obj$numclust dimdat = obj$dimdat ## Predict the means (manually). newmn_array = array(NA, dim = c(TT_new, dimdat, numclust)) for(iclust in 1:numclust){ newmn_oneclust &lt;- lapply(newx, function(tt){ interpolate_mn(x, tt, iclust, obj$mn) }) %&gt;% do.call(rbind, . ) newmn_array[,,iclust] = newmn_oneclust } ## Predict the probs. newprob = array(NA, dim = c(TT_new, numclust)) for(iclust in 1:numclust){ newprob_oneclust &lt;- lapply(newx, function(tt){ interpolate_prob(x, tt, iclust, obj$prob) }) %&gt;% do.call(c, .) newprob[,iclust] = newprob_oneclust } ## Basic checks stopifnot(all(dim(newprob) == c(TT_new,numclust))) stopifnot(all(newprob &gt;= 0)) stopifnot(all(newprob &lt;= 1)) ## Return the predictions return(list(mn = newmn_array, prob = newprob, sigma = obj$sigma, x = newx)) } Here’s a quick test (no new data) to make sure this function returns a list containing: the mean, probability, covariance, and new times. devtools::load_all(&quot;~/repos/FlowTF&quot;) ## Temporary! flowtrend hasn&#39;t fully ported over the m-step yet. ## ℹ Loading FlowTF ## Registered S3 methods overwritten by &#39;RcppEigen&#39;: ## method from ## predict.fastLm RcppArmadillo ## print.fastLm RcppArmadillo ## summary.fastLm RcppArmadillo ## print.summary.fastLm RcppArmadillo testthat::test_that(&quot;The prediction function returns the right things&quot;, { ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1, niter = 3) predobj = predict_flowtrend(obj) testthat::expect_named(predobj, c(&quot;mn&quot;, &quot;prob&quot;, &quot;sigma&quot;, &quot;x&quot;)) }) Now, we try to make predictions at new held-out time points held_out=25:35, from a model that is estimated without those time points. ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster labels of the fitted model. obj = reorder_clust(obj) testthat::test_that(&quot;prediction function returns the right things&quot;, { predobj = predict_flowtrend(obj, newtimes = held_out) ## Check a few things testthat::expect_equal(predobj$x, held_out) testthat::expect_equal(rowSums(predobj$prob), rep(1, length(held_out))) testthat::expect_equal(dim(predobj$mn), c(length(held_out), 1, 3)) }) Plot the predicted means \\(\\mu\\) and probabilities \\(\\pi\\), with purple points at the interpolated means. We can see that it works as expected. predobj = predict_flowtrend(obj, newtimes = held_out) g = plot_1d(ylist, obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = held_out, cluster = iclust) }) %&gt;% bind_rows() g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = 2)##, alpha = .8) The estimated probabilities are shown here, with purple points showing the interpolation. It works as expected. plot_prob(obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = 3) Let’s now try to space inputs unevenly, by x. set.seed(100) dt &lt;- gendat_1d(1000, rep(100, 1000), die_off_time = 0.45) dt_model &lt;- gendat_1d(1000, rep(100, 1000), die_off_time = 0.45, return_model = TRUE) ## held_out = 25:35 ## dt_subset = dt %&gt;% subset(time %ni% held_out) ylist_orig = dt %&gt;% dt2ylist() ## x = dt %&gt;% pull(time) %&gt;% unique() ## x = runif(min = 1, max = 100, n = 100) %&gt;% sort() x = sample(1:1000, 100) %&gt;% sort() ylist = ylist_orig[x] obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) ## Make mean predictions newtimes = seq(from=min(x),to=max(x),length=10000) predobj = predict_flowtrend(obj, newtimes = newtimes) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = newtimes, cluster = iclust) }) %&gt;% bind_rows() The estimated means \\(\\mu\\) in the training data are shown as solid triangle points. The out-of-sample \\(\\mu\\) predictions made on a fine grid of time points (shown by the yellow lines) look fine. g = plot_1d(ylist, obj, x = x, add_point = FALSE) g + ggtitle(&quot;Fitted model&quot;) g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = rel(1), alpha = .7) + ggtitle(&quot;Predictions on fine grid of times&quot;) The out-of-sample \\(\\pi\\) predictions are the lines that connect the points. They look great as well. plot_prob(obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) ## geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = rel(1), alpha = .7) Next, we’ll try evaluating an estimated model’s prediction in an out-of-sample measurement. This will be measured by the model prediction’s out-of-sample objective (negative log-likelihood). ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 5) ## Make prediction predobj = predict_flowtrend(obj, newtimes = held_out) ## Use the predicted (interpolated) model parameters obj_pred = objective(mu = predobj$mn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) truemn = array(NA, dim = dim(predobj$mn)) truemn[,1,] = dt_model %&gt;% select(time, cluster, mean) %&gt;% pivot_wider(names_from = cluster, values_from = mean) %&gt;% subset(time %in% held_out) %&gt;% select(-time) %&gt;% as.matrix() ## Use the true mean obj_better = objective(mu = truemn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) ## Here is the estimated model plot_1d(ylist, obj, x= (1:100)[-held_out], add_point = FALSE) The out-of-sample prediction is similar for the predicted model and the estimated model. Below, we’re showing just the predicted means at the held-out points, overlaid with data. (This is measured by the objective (= negative log likelihood), so lower is better! Red is worse than black, naturally.) {r fit, fig.width = 7, fig.height = 5}) g = plot_1d(ylist = dt %&gt;% subset(time %in% held_out) %&gt;% dt2ylist(), x= held_out) + xlim(c(0,100)) g + geom_line(aes(x=time, y = value, group = name), data = data.frame(truemn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time)) + geom_line(aes(x=time, y = value, group = name), data = data.frame(predobj$mn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time), col = 'red') + ggtitle(paste0(round(obj_pred,3), \" (red, predicted) vs. \", round(obj_better, 3), \"(black, truth)\")) 13.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test What should the maximum value of regularization parameters to use? It’s useful to be able to calculate the smallest value of regularization parameters that result in fully flat \\(\\mu\\) and \\(\\pi\\) over time, in all clusters. Call these \\(\\lambda_\\mu^{\\text{max}}\\) and \\(\\lambda_{\\pi}^{\\text{max}}\\). We use these to form a 2d grid of candidate \\(\\lambda\\) values – logarithmically-spaced pairs of values between starting at \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\) decreasing to some small pair of values. The function get_max_lambda() numerically estimates this maximum pair \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\). It proceeds by first running flowtrend() on a very large pair \\((\\lambda_\\mu, \\lambda_\\pi)\\), then sequentially halving both values while checking if the resulting estimated \\(\\mu\\) and \\(\\pi\\) are all flat over time. As soon as they cease to be flat, we stop and take the immediately previous pair of values of \\((\\lambda_\\mu, \\lambda_\\pi)\\). get_max_lambda() is a wrapper around the workhorse calc_max_lambda(). It obtains the value and saves it to a maxres_file (which defaults to maxres.Rdata) in the destin directory. #&#39; A wrapper for \\code{calc_max_lambda}. Saves the two maximum lambda values in #&#39; a file. #&#39; #&#39; @param destin Where to save the output (A two-lengthed list called #&#39; &quot;maxres&quot;). #&#39; @param maxres_file Filename for output. Defaults to maxres.Rdata. #&#39; @param ... Additional arguments to \\code{flowtrend()}. #&#39; @inheritParams calc_max_lambda #&#39; #&#39; @return No return #&#39; #&#39; @export get_max_lambda &lt;- function(destin, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist, numclust, maxdev, max_lambda_mean, max_lambda_prob, ...){ if(file.exists(file.path(destin, maxres_file))){ load(file.path(destin, maxres_file)) cat(&quot;Maximum regularization values are loaded.&quot;, fill=TRUE) return(maxres) } else { print(Sys.time()) cat(&quot;Maximum regularization values being calculated.&quot;, fill = TRUE) cat(&quot;with initial lambda values (prob and mu):&quot;, fill = TRUE) print(c(max_lambda_prob, max_lambda_mean)); maxres = calc_max_lambda(ylist = ylist, countslist = countslist, numclust = numclust, maxdev = maxdev, ## This function&#39;s settings max_lambda_prob = max_lambda_prob, max_lambda_mean = max_lambda_mean, ...) save(maxres, file = file.path(destin, maxres_file)) cat(&quot;file was written to &quot;, file.path(destin, maxres_file), fill=TRUE) cat(&quot;maximum regularization value calculation done.&quot;, fill = TRUE) print(Sys.time()) return(maxres) } } The aforementioned workhorse calc_max_lambda() is here. #&#39; Estimate maximum lambda values numerically. First starts with a large #&#39; initial value \\code{max_lambda_mean} and \\code{max_lambda_prob}, and runs #&#39; the EM algorithm on decreasing set of values (sequentially halved). This #&#39; stops once you see non-flat probabilities or means, and returns the *smallest* #&#39; regularization (lambda) value pair that gives full sparsity. #&#39; #&#39; Note that the \\code{zero_stabilize=TRUE} option is used in #&#39; \\code{flowtrend()}, which basically means the EM algorithm runs only until #&#39; the zero pattern stabilizes. #&#39; #&#39; @param ylist List of responses. #&#39; @param numclust Number of clusters. #&#39; @param max_lambda_mean Defaults to 4000. #&#39; @param max_lambda_prob Defaults to 1000. #&#39; @param iimax Maximum value of x for 2^{-x} factors to try. #&#39; @param ... Other arguments to \\code{flowtrend_once()}. #&#39; #&#39; @return list containing the two maximum values to use. #&#39; #&#39; @export calc_max_lambda &lt;- function(ylist, countslist = NULL, numclust, max_lambda_mean = 4000, max_lambda_prob = 1000, verbose = FALSE, iimax = 16, ...){ ## Get range of regularization parameters. facs = sapply(1:iimax, function(ii) 2^(-ii+1)) ## DECREASING order print(&quot;running the models once&quot;) for(ii in 1:iimax){ ## print_progress(ii, iimax, &quot;regularization values&quot;, fill = TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) cat(&quot;#### lambda_prob = &quot;, max_lambda_prob * facs[ii], &quot; and lambda = &quot;, max_lambda_mean * facs[ii], &quot;being tested. &quot;, fill=TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], verbose = verbose, ...) ## TODO: CHECK FLATNESS INSTEAD OF ZERONESS ## In each dimension, the data should only vary by a relatively small amount (say 1/100) browser() idim = 1 toler_by_dim = sapply(1:res$dimdat, function(idim){ datrange = ylist %&gt;% sapply(FUN = function(y) y %&gt;% .[,idim] %&gt;% range()) %&gt;% range() toler = (datrange[2] - datrange[1]) / (100 * length(ylist)) }) mean_is_flat = sapply(1:res$dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,])) &lt; toler_by_dim) }) toler_prob = 0.01 / length(ylist) prob_is_flat = all(abs(diff(res$prob)) &lt; toler_prob) if(all(mean_is_flat) &amp; prob_is_flat) ## I think this is it? abs_range / 100 plot_1d(ylist, res) plot_prob(res) + ylim(c(0,1)) names(res) res$lambda res$lmbda_prob res$prob[,1] %&gt;% plot() res$objectives %&gt;% plot(type = &#39;o&#39;, cex=.5) ## End of the flatness ## Check zero-ness toler = 0 sum_nonzero_prob = sum(res$alpha[,-1] &gt; toler) sum_nonzero_beta = sum(unlist(lapply(res$beta, function(cf){ sum(cf[-1,] &gt; toler) }))) ## If there are *any* nonzero values, do one of the following if(sum_nonzero_alpha + sum_nonzero_beta != 0){ ## If there are *any* nonzero values at the first iter, prompt a restart ## with higher initial lambda values. if(ii==1){ stop(paste0(&quot;Max lambdas: &quot;, max_lambda_mean, &quot; and &quot;, max_lambda_prob, &quot; were too small as maximum reg. values. Go up and try again!!&quot;)) ## If there are *any* nonzero values, return the immediately preceding ## lambda values -- these were the smallest values we had found that gives ## full sparsity. } else { ## Check one more time whether the model was actually zero, by fully running it; res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], ...) ## Check if both curves are basically flat if(FALSE){ toler = 0 sum_nonzero_alpha = sum(res$alpha[,-1] &gt; toler) sum_nonzero_beta = sum(unlist(lapply(res$beta, function(cf){ sum(cf[-1,] &gt; toler) }))) } res$mean res$prob ## If there are *any* nonzero values, do one of the following if(sum_nonzero_alpha + sum_nonzero_beta != 0){ return(list(mean = max_lambda_mean * facs[ii-1], prob = max_lambda_prob *facs[ii-1])) } ## Otherwise, just proceed to the next iteration. } } cat(fill=TRUE) } } 13.3 Define CV data folds make_cv_folds() makes the cross-validation “folds”, which are the \\(K\\) (nfold) list of data indices. These are not times! They simply split of 1:length(ylist). #&#39; Define the time folds cross-validation. #&#39; #&#39; @param nfold Number of folds. #&#39; @return List of fold indices. #&#39; @export #&#39; make_cv_folds &lt;- function(ylist=NULL, nfold, TT=NULL){ ## Make hour-long index list if(is.null(TT)) TT = length(ylist) folds &lt;- rep(1:nfold, ceiling( (TT-2)/nfold))[1:(TT-2)] inds &lt;- lapply(1:nfold, FUN = function(k) (2:(TT-1))[folds == k]) plot(NA, ylim=c(0,1), xlim=c(1,100)) abline(v=inds[[1]]) names(inds) = paste0(&quot;Fold&quot;, 1:nfold) return(inds) } We can visualize how the data is to be split. In the following plot, vertical lines mark data indices in each fold, using different colors . For nfold = 5, the first fold is every 5th point starting at 2, \\(\\{2,7,\\dots\\}\\), and the second fold is \\(\\{3,8,\\dots\\}\\), and so forth. Note: the first index \\(1\\) and the last \\(TT\\) are left out at this stage, and instead made available to all folds at training time (in cv_flowtrend()). This is because, otherwise, it would be impossible to make predictions at either ends of the data. nfold = 5 TT = 100 inds = make_cv_folds(nfold = nfold, TT = TT) print(inds) plot(NA, xlim = c(0,TT), ylim=1:2, ylab = &quot;&quot;, xlab = &quot;Data index of ylist&quot;, yaxt = &quot;n&quot;, xaxt=&quot;n&quot;) axis(1, at = c(1, seq(10, 100,10))) for(ifold in 1:nfold){ abline(v = inds[[ifold]], col = ifold, lwd = 2) } ## $Fold1 ## [1] 2 7 12 17 22 27 32 37 42 47 52 57 62 67 72 77 82 87 92 97 ## ## $Fold2 ## [1] 3 8 13 18 23 28 33 38 43 48 53 58 63 68 73 78 83 88 93 98 ## ## $Fold3 ## [1] 4 9 14 19 24 29 34 39 44 49 54 59 64 69 74 79 84 89 94 99 ## ## $Fold4 ## [1] 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 ## ## $Fold5 ## [1] 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 13.4 CV = many single jobs Next, we build the immediate elements needed for cross-validation. There are two applications of flowtrend() on data for cross-validation; one is when estimating models from held-in data folds, and the other is when re-estimating models on the full data. Estimating models on the held-in data is done by one_job(). Re-estimating models on the entire dataset is done by one_job_refit(). Here is one_job(). #&#39; Helper function to run ONE job for CV, in iprob, imu, ifold, irestart. #&#39; #&#39; @param iprob Index for prob. #&#39; @param imu Index for beta. #&#39; @param ifold Index for CV folds. #&#39; @param irestart Index for 1 through nrestart. #&#39; @param folds CV folds (from \\code{make_cv_folds()}). #&#39; @param destin Destination directory. #&#39; @param lambda_means List of regularization parameters for mean model. #&#39; @param lambda_probs List of regularization parameters for prob model. #&#39; @param ylist Data. #&#39; @param countslist Counts or biomass. #&#39; @param ... Rest of arguments for \\code{flowtrend_once()}. #&#39; #&#39; @return Nothing is returned. Instead, a file named &quot;1-1-1-1-cvscore.Rdata&quot; #&#39; is saved in \\code{destin}. (The indices here are iprob-imu-ifold-irestart). #&#39; #&#39; @export one_job &lt;- function(iprob, imu, ifold, irestart, folds, destin, lambda_means, lambda_probs, seedtab = NULL, ## The rest that is needed explicitly for flowtrend() ylist, countslist, l, l_prob, ...){ ## Get the train/test data TT &lt;- length(ylist) test.inds = unlist(folds[ifold]) test.dat = ylist[test.inds] test.count = countslist[test.inds] train.inds = c(1, unlist(folds[-ifold]), TT) train.dat = ylist[train.inds] train.count = countslist[train.inds] ## Check whether this job has been done already. filename = make_cvscore_filename(iprob, imu, ifold, irestart) if(file.exists(file.path(destin, filename))){ cat(filename, &quot;already done&quot;, fill=TRUE) return(NULL) } ## Get the seed ready if(!is.null(seedtab)){ seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob, imu == !!imu, ifold == !!ifold, irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } lambda_prob = lambda_probs[iprob] lambda_mean = lambda_means[imu] ## Run the algorithm (all this trouble because of |nrestart|) args = list(...) args$ylist = train.dat args$countslist = train.count args$x = train.inds args$lambda = lambda_mean args$lambda_prob = lambda_prob args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)){ args = args[-which(names(args) %in% &quot;nrestart&quot;)] ## remove |nrestart| prior to feeding to flowtrend_once(). } tryCatch({ ## Estimate model argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend&quot;)), argn)) res.train = eval(call, args) ## Assign mn and prob pred = predict_flowtrend(res.train, newtimes = test.inds) stopifnot(all(pred$prob &gt;= 0)) ## Build Dl ## ## Evaluate on test data, by calculating objective (penalized likelihood with penalty parameters set to 0) cvscore = objective(mu = pred$mn, prob = pred$prob, sigma = pred$sigma, ylist = test.dat, countslist = test.count, unpenalized = TRUE) ## Dl = diag(rep(1, length(test.count))), ## TODO: what is wrong here? ## lambda_prob = 0, ## lambda = 0) ## prob = res.train$prob, ## beta = res.train$beta) ## Store (temporarily) the run times time_per_iter = res.train$time_per_iter final_iter = res.train$final.iter total_time = res.train$total_time ## Store the results. mn = res.train$mn prob = res.train$prob objectives = res.train$objectives ## Save the CV results save(cvscore, ## Time time_per_iter, final_iter, total_time, ## Results lambda_mean, lambda_prob, lambda_means, lambda_probs, mn, prob, objectives, ## Save the file file = file.path(destin, filename)) return(NULL) }, error = function(err) { err$message = paste(err$message, &quot;\\n(No file will be saved for lambdas (&quot;, signif(lambda_probs[iprob],3), &quot;, &quot;, signif(lambda_means[imu],3), &quot;) whose indices are: &quot;, iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot; .)&quot;,sep=&quot;&quot;) cat(err$message, fill=TRUE) warning(err)}) } Here is one_job_refit(). #&#39; Refit model for one pair of regularization parameter values. Saves to #&#39; \\code{nrestart} files named like &quot;1-4-3-fit.Rdata&quot;, for #&#39; &quot;(iprob)-(imu)-(irestart)-fit.Rdata&quot;. #&#39; #&#39; (Note, \\code{nrestart} is not an input to this function.) #&#39; #&#39; @inheritParams one_job #&#39; #&#39; @export one_job_refit &lt;- function(iprob, imu, destin, lambda_means, lambda_probs, l, l_prob, seedtab = NULL, ## The rest that is needed explicitly for flowtrend_once() ylist, countslist, ...){ args = list(...) nrestart = args$nrestart assertthat::assert_that(!is.null(nrestart)) for(irestart in 1:nrestart){ ## Writing file filename = make_refit_filename(iprob = iprob, imu = imu, irestart = irestart) if(file.exists(file.path(destin, filename))){ cat(filename, &quot;already done&quot;, fill=TRUE) next } else { ## Get the seed ready if(!is.null(seedtab)){ ifold = 0 seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob, imu == !!imu, ifold == !!ifold, irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } ## Get the fitted results on the entire data args = list(...) args$ylist = ylist args$countslist = countslist args$lambda_prob = lambda_probs[iprob] args$lambda = lambda_means[imu] args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)) args = args[-which(names(args) %in% &quot;nrestart&quot;)] ## remove |nrestart| prior to feeding ## Call the function. argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res = eval(call, args) ## Save the results cat(&quot;Saving file here:&quot;, file.path(destin, filename), fill=TRUE) save(res, file=file.path(destin, filename)) } } } Since cross-validation entails running many jobs, we need to index individual “jobs” carefully. Here are some more helpers for indexing: make_iimat(): Make a table whose rows index each “job” (iprob, imu, ifold, irestart), to be used by one_job(). make_iimat_small(): Make a table whose rows index each (iprob, imu, irestart) for re-estimating models, to be used by one_job_refit(). #&#39; Indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; #&#39; ind iprob imu ifold irestart #&#39; 55 6 1 2 1 #&#39; 56 7 1 2 1 #&#39; 57 1 2 2 1 #&#39; 58 2 2 2 1 #&#39; 59 3 2 2 1 #&#39; 60 4 2 2 1 #&#39; @param cv_gridsize CV grid size. #&#39; @param nfold Number of CV folds. #&#39; @param nrestart Number of random restarts of EM algorithm. #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat &lt;- function(cv_gridsize, nfold, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, ifold = 1:nfold, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } #&#39; 2d indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; (#, iprob, imu, irestart) #&#39; 1, 1, 1, 1 #&#39; 2, 1, 2, 1 #&#39; 3, 1, 3, 1 #&#39; #&#39; @inheritParams make_iimat #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat_small &lt;- function(cv_gridsize, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } Let’s see the integer matrices that these functions make. make_iimat(cv_gridsize = 5, nfold = 5, nrestart = 10) %&gt;% head() make_iimat_small(cv_gridsize = 5, nrestart = 10) %&gt;% head() ## ind iprob imu ifold irestart ## 1 1 1 1 1 1 ## 2 2 2 1 1 1 ## 3 3 3 1 1 1 ## 4 4 4 1 1 1 ## 5 5 5 1 1 1 ## 6 6 1 2 1 1 ## ind iprob imu irestart ## 1 1 1 1 1 ## 2 2 2 1 1 ## 3 3 3 1 1 ## 4 4 4 1 1 ## 5 5 5 1 1 ## 6 6 1 2 1 Next, the functions make_cvscore_filename() and make_refit_filename() are used to form the names of the numerous output files. #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_cvscore_filename &lt;- function(iprob, imu, ifold, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot;-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; @param irestart #&#39; #&#39; @export make_refit_filename &lt;- function(iprob, imu, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, irestart, &quot;-fit.Rdata&quot;) return(filename) } Here’s a useful helper logspace(max, min) to make logarithmically spaced set of numbers, given min and max. We can use this to make a grid of lambda pairs to be used for cross-validation. #&#39; Helper function to logarithmically space out R. \\code{length} values linear #&#39; on the log scale from \\code{max} down to \\code{min}. #&#39; #&#39; @param max Maximum value. #&#39; @param min Minimum value. #&#39; @param length Length of the output string. #&#39; @param min.ratio Factor to multiply to \\code{max}. #&#39; #&#39; @return Log spaced #&#39; #&#39; @export logspace &lt;- function(max, min=NULL, length, min.ratio = 1E-4){ if(is.null(min)) min = max * min.ratio vec = 10^seq(log10(min), log10(max), length = length) stopifnot(abs(vec[length(vec)] - max) &lt; 1E10) return(vec) } 13.5 Running cross-validation Putting the helpers all together, you get the main user-facing function cv_flowtrend(). #&#39; Cross-validation for flowtrend(). Saves results to separate files in #&#39; \\code{destin}. #&#39; #&#39; @param destin Directory where output files are saved. #&#39; @param nfold Number of cross-validation folds. Defaults to 5. #&#39; @param nrestart Number of repetitions. #&#39; @param save_meta If TRUE, save meta data. #&#39; @param lambda_means Regularization parameters for means. #&#39; @param lambda_probs Regularization parameters for probs. #&#39; @param folds Manually provide CV folds (list of time points of data to use #&#39; as CV folds). Defaults to NULL. #&#39; @param mc.cores Use this many CPU cores. #&#39; @param blocksize Contiguous time blocks from which to form CV time folds. #&#39; @param refit If TRUE, estimate the model on the full data, for each pair of #&#39; regularization parameters. #&#39; @param ... Additional arguments to flowtrend(). #&#39; @inheritParams flowtrend_once #&#39; #&#39; @return No return. #&#39; #&#39; @export cv_flowtrend &lt;- function(## Data ylist, countslist, ## Define the locations to save the CV. destin = &quot;.&quot;, ## Regularization parameter values lambda_means, lambda_probs, l, l_prob, iimat = NULL, ## Other settings maxdev, numclust, nfold, nrestart, verbose = FALSE, refit = FALSE, save_meta = FALSE, mc.cores = 1, folds = NULL, seedtab = NULL, ...){ ## Basic checks stopifnot(length(lambda_probs) == length(lambda_means)) cv_gridsize = length(lambda_means) ## There&#39;s an option to input one&#39;s own iimat matrix. if(is.null(iimat)){ ## Make an index of all jobs if(!refit) iimat = make_iimat(cv_gridsize, nfold, nrestart) if(refit) iimat = make_iimat_small(cv_gridsize, nrestart) } ## Define the CV folds ## folds = make_cv_folds(ylist = ylist, nfold = nfold, blocksize = 1) if(is.null(folds)){ folds = make_cv_folds(ylist = ylist, nfold = nfold) } else { stopifnot(length(folds) == nfold) } ## Save meta information, once. if(save_meta){ if(!refit){ save(folds, nfold, nrestart, ## Added recently cv_gridsize, lambda_means, lambda_probs, ylist, countslist, ## Save the file file = file.path(destin, &#39;meta.Rdata&#39;)) print(paste0(&quot;wrote meta data to &quot;, file.path(destin, &#39;meta.Rdata&#39;))) } } ## Run the EM algorithm many times, for each value of (iprob, imu, ifold, irestart) start.time = Sys.time() parallel::mclapply(1:nrow(iimat), function(ii){ print_progress(ii, nrow(iimat), &quot;Jobs (EM replicates) assigned on this computer&quot;, start.time = start.time) if(!refit){ iprob = iimat[ii,&quot;iprob&quot;] imu = iimat[ii,&quot;imu&quot;] ifold = iimat[ii,&quot;ifold&quot;] irestart = iimat[ii,&quot;irestart&quot;] ## if(verbose) cat(&#39;(iprob, imu, ifold, irestart)=&#39;, c(iprob, imu, ifold, irestart), fill=TRUE) } else { iprob = iimat[ii, &quot;iprob&quot;] imu = iimat[ii, &quot;imu&quot;] ifold = 0 } if(!refit){ one_job(iprob = iprob, imu = imu, l = l, l_prob = l_prob, ifold = ifold, irestart = irestart, folds = folds, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments for flowtrend() ylist = ylist, countslist = countslist, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, verbose = FALSE, seedtab = seedtab) } else { one_job_refit(iprob = iprob, imu = imu, l = l, l_prob = l_prob, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments to flowtrend() ylist = ylist, countslist = countslist, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, nrestart = nrestart, verbose = FALSE, seedtab = seedtab) } return(NULL) }, mc.cores = mc.cores) } 13.6 Summarizing the output Once the cross-validation is finished (and saved into many files called e.g. 1-1-1-1-cvscore.Rdata or 1-1-1-fit.Rdata), we can use cv_summary() to summarize the results. If you look closely, you’ll notice that cv_aggregate() is the workhorse. #&#39; Main function for summarizing the cross-validation results. #&#39; #&#39; @inheritParams cv_flowtrend #&#39; @param save If TRUE, save to \\code{file.path(destin, filename)}. #&#39; @param filename File name to save to. #&#39; #&#39; @return List containing summarized results from cross-validation. Here are #&#39; some objects in this list: \\code{bestres} is the the overall best model #&#39; chosen from the cross-validation; \\code{cvscoremat} is a 2d matrix of CV #&#39; scores from all pairs of regularization parameters; \\code{bestreslist} is a #&#39; list of all the best models (out of \\code{nrestart} EM replications) from the #&#39; each pair of lambda values. If \\code{isTRUE(save)}, nothing is returned. #&#39; #&#39; @export cv_summary &lt;- function(destin = &quot;.&quot;, save = FALSE, filename = &quot;summary.RDS&quot; ){ #################### ## Load data ####### #################### load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things: nrestart, nfold, cv_gridsize stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) ########################## ## Get the CV results. ### ########################## a = cv_aggregate(destin) cvscore.mat = a$cvscore.mat min.inds = a$min.inds ## Get results from refitting bestreslist = cv_aggregate_res(destin = destin) bestres = bestreslist[[paste0(min.inds[1] , &quot;-&quot;, min.inds[2])]] if(is.null(bestres)){ stop(paste0(&quot;The model with lambda indices (&quot;, min.inds[1], &quot;,&quot;, min.inds[2], &quot;) is not available.&quot;)) } ## ######################## ## ## Get coefficients #### ## ######################## ## betalist = lapply(1:bestres$numclust, function(iclust){ ## ## Get all betas ## rownames(bestres$beta[[iclust]])[-1] = colnames(bestres$X) ## cf = bestres$beta[[iclust]][-1,, drop=FALSE] ## ## Remove the rows that are all zero ## all.zero.rows = which(apply(cf, 1, function(myrow)all(myrow == 0))) ## if(length(all.zero.rows) &gt; 0){ ## cf = cf[-all.zero.rows,, drop=FALSE] ## } ## round(Matrix::Matrix(cf, sparse=TRUE),3) ## }) ## names(betalist) = paste0(&quot;Beta matrix, cluster &quot;, 1:bestres$numclust) ## pretty.betas = betalist ## colnames(bestres$alpha)[-1 ] = colnames(bestres$X) ## alpha = t(bestres$alpha) ## alpha[which(abs(alpha) &lt; 1E-5)] = 0 ## pretty.alphas = round(Matrix::Matrix(alpha, sparse=TRUE),3) ###################### ## Get the output #### ###################### ## pretty.mns = . ## pretty.probs = . ## pretty.sigmas = . ###################### ## Get the sigmas #### ###################### if(bestres$dimdat == 1){ pretty.sigmas = sqrt(bestres$sigma[,1,]) names(pretty.sigmas) = paste0(&quot;Cluster &quot;, 1:bestres$numclust) } else { sigmas = lapply(1:bestres$numclust, function(iclust){ diag(bestres$sigma[iclust,,]) }) names(sigmas) = paste0(&quot;Cluster &quot;, 1:bestres$numclust) pretty.sigmas = lapply(sigmas, sqrt) } out = list(bestres = bestres, cvscore.mat = cvscore.mat, min.inds = min.inds, ## ## Pretty formatted data ## Todo: get this done. ## pretty.mns = pretty.mns, ## pretty.probs = pretty.probs, ## pretty.sigmas = pretty.sigmas, ## List of all best models for all lambda pairs. bestreslist = bestreslist, destin = destin) if(save){ saveRDS(out, file=file.path(destin, filename))} return(out) } #&#39; Aggregate CV scores from the results, saved in \\code{destin}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_aggregate &lt;- function(destin){ ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, ## ## |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ obj = matrix(NA, nrow=nfold, ncol=nrestart) for(ifold in 1:nfold){ for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, irestart] = cvscore obj[ifold, irestart] = objectives[length(objectives)] }, error = function(e){}) } } ## Pick out the CV scores with the *best* (lowest) objective value cvscores = cvscore.array[iprob, imu , ,] best.models = apply(obj, 1, function(myrow){ ind = which(myrow == min(myrow, na.rm=TRUE)) if(length(ind)&gt;1) ind = ind[1] ## Just choose one, if there is a tie. return(ind) }) final.cvscores = sapply(1:nfold, function(ifold){ #cvscores[ifold, best.models[ifold]] cvscores[ifold] }) cvscore.mat[iprob, imu] = mean(final.cvscores) } } ## Clean a bit cvscore.mat[which(is.nan(cvscore.mat), arr.ind=TRUE)] = NA ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|) rownames(cvscore.mat) = signif(lambda_probs,3) colnames(cvscore.mat) = signif(lambda_means,3) ## Find the minimum mat = cvscore.mat min.inds = which(mat == min(mat, na.rm = TRUE), arr.ind = TRUE) ## Return the results out = list(cvscore.array = cvscore.array, cvscore.mat = cvscore.mat, lambda_means = lambda_means, lambda_probs = lambda_probs, min.inds = min.inds) return(out) } #&#39; Helper to aggregate parallelized CV results and obtain the |res| object, all #&#39; saved in |destin|. #&#39; #&#39; @inheritParams cv_aggregate #&#39; #&#39; @return List containing, for every (iprob, imu), the &quot;best&quot; estimated #&#39; model out of the |nrestart| replicates (best in the sense that it had the best #&#39; likelihood value out of the |nrestart| replicates.) cv_aggregate_res &lt;- function(destin){ load(file.path(destin, &quot;meta.Rdata&quot;)) ## df.mat = matrix(NA, ncol=cv_gridsize, nrow=cv_gridsize) res.list = list() for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## Objective values, over nrestart obj = rep(NA, nrestart) ## df = rep(NA, nrestart) # res.list.inner = list() for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) tryCatch({ ## Load fitted result load(file.path(destin, filename)) res.list.inner[[irestart]] = res ## Also store objective obj[irestart] = res$objectives[length(res$objectives)] }, error = function(e){}) } ## Calculate the df of the best model if(!all(is.na(obj))){ res.list[[paste0(iprob, &quot;-&quot;, imu)]] = res.list.inner[[which.min(obj)]] ## which.min? } } } return(res.list) } 13.7 CV on your own computer Using all this functionality, we’d like to be able to cross-validate on our own laptop, using cv_flowtrend(). Let’s try it out. cv_gridsize = 3 l = 1 l_prob = 1 set.seed(332) ylist = gendat_1d(10, rep(100,10)) %&gt;% dt2ylist() plot_1d(ylist) folds = make_cv_folds(lapply(1:100, function(ii)cbind(runif(10))), nfold = 5, TT = length(ylist)) lambda_means = lambda_probs = logspace(min = 1E-5, max = 1, length = cv_gridsize) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = &quot;~/repos/flowtrend/tempoutput&quot;, lambda_means = lambda_means, lambda_probs = lambda_probs, l = l, l_prob = l_prob, maxdev = NULL, numclust = 3, nfold = 5, nrestart = 5, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 6) } ## Test out the summary obj = cv_summary(destin = &quot;~/repos/flowtrend/tempoutput&quot;) plot_1d(ylist, obj$bestres) Okay, now we know it’s possible to run on (say) a laptop. A more realistic application would go something like this: litr::load_all(&quot;~/repos/flowtrend/index.Rmd&quot;) get_max_lambda(&quot;~/repos/flowtrend/tempoutput&quot;, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist = NULL, numclust = 3, maxdev = 1, max_lambda_prob = 3000000, max_lambda_mean = 3000000, verbose = TRUE, l = 1, l_prob = 1) load(&quot;maxres.Rdata&quot;, verbose = TRUE) ## Loads &quot;maxres&quot; object lambda_means = logspace(max = maxres$means, length = 5) lambda_probs = logspace(max = maxres$prob, length = 5) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = &quot;~/repos/flowtrend/tempoutput&quot;, lambda_means = lambda_means, lambda_probs = lambda_probs, l = 2, l_prob = 1, maxdev = 2, numclust = 3, nfold = 5, nrestart = 5, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 6) } 13.8 Next up TODO: Things to check out - ADMM convergence &lt;– take a detailed look - spaced CV correctness check. "],["testing-the-flowtrend-method.html", "14 Testing the flowtrend method 14.1 1d example 14.2 1d example with gap", " 14 Testing the flowtrend method We’re going to assume the flowtrend() function has been built. We’re going to test it now. library(tidyverse) devtools::load_all(&quot;~/repos/FlowTF&quot;) litr::load_all(&quot;index.Rmd&quot;) 14.1 1d example ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(18) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 3) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) The objective value (that is, the penalized log likelihood) should be monotone across EM algorithm iterations. testthat::test_that(&quot;Objective value decreases over EM iterations.&quot;,{ devtools::load_all(&quot;~/repos/FlowTF&quot;) ## for(iseed in 1:5){ for(iseed in 1){ ## Generate increasingly noisy data set.seed(iseed*100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.2) dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, ## lambda_prob = .5, lambda_prob = 0.05, nrestart = 1) ## Test objective monotonicity testthat::expect_true(all(diff(obj$objective) &lt; 0)) } }) ## i Loading FlowTF ## Test passed 🎊 (TODO: resolve the issue of the objective value sometimes rising!) While the slight rise in objective value is not egregious, I would like to get to the bottom of this. The next code block shows a self-contained example of the objective value rising. I didn’t stop the algorithm (allowing the full niter=200 iterations) to see if it increases. Now I wonder – is it due to glmnet? If we use CVXR, will it go away? litr::load_all(&quot;~/repos/flowtrend/index.Rmd&quot;) devtools::load_all(&quot;~/repos/FlowTF&quot;) iseed = 1 set.seed(iseed * 100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.2) dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = 0.05, nrestart = 1, niter = 200, verbose = TRUE) ## TODO: remember to comment out the convergence check!! objectives = obj$objective for(iter in 2:200){ if(check_converge_rel(objectives[iter-1], objectives[iter], tol = 1E-4)) break } stopping_iter = iter print(range(diff(obj$objective))) plot(obj$objective, type =&#39;l&#39;, xlim = c(0,50)) abline(v=iter, lty = &#39;dotted&#39;) abline(h=min(obj$objective), lty = &#39;dotted&#39;, col = &#39;blue&#39;) plot(diff(obj$objective), type =&#39;l&#39;, ylim = c(-0.001, 0.0005)) abline(h=0, col = &#39;blue&#39;, lty = &#39;dotted&#39;) The data and estimated model are shown here. plot_1d(ylist, obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if ## `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## ℹ The deprecated feature was likely used in the litr package. ## Please report the issue to the authors. ## Joining, by = c(&quot;time&quot;, &quot;cluster&quot;) ## Warning in geom_line(aes(x = time, y = mean, size = prob, group = cluster), : Ignoring unknown parameters: `shape` The estimated probabilities are shown here. plot_prob(obj) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) 14.2 1d example with gap Repeating this exercise with data that has a gap (between times 25 and 35). held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() ## Fit model set.seed(0) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 2) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) No model estimates are being made at the gap (between 25 and 35)! The model estimates (the solid colored lines) are automatically generated by ggplot::geom_line(). But actually, linear interpolation is what we’ll want to do when making predictions at new time points. There’s more about this shortly. TODO: This blob is a bit outdated. We’ve already built the predict_flowtrend() function. plot_1d(ylist, obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Joining, by = c(&quot;time&quot;, &quot;cluster&quot;) ## Warning in geom_line(aes(x = time, y = mean, size = prob, group = cluster), : Ignoring unknown parameters: `shape` "],["d-to-1d.html", "15 2d to 1d", " 15 2d to 1d Collapse 2d to 1d data, and plot it. litr::load_all(&quot;~/repos/flowtrend/index.Rmd&quot;) set.seed(100) dt &lt;- gendat_2d(100, rep(100, 100)) ylist = dt$ylist ## collapse to 1d ylist_1d = lapply(ylist, function(a) a[,1, drop = FALSE]) x = 1:length(ylist_1d) ## Data plot_1d(ylist_1d) ## Fit model set.seed(100) obj = flowtrend(ylist = ylist, numclust = 3, l = 2, l_prob = 2, lambda = .001, lambda_prob = .0001, verbose = TRUE) # Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) ## Show the estimated model overlaid with data mns = dt$means %&gt;% .[,,1] %&gt;% reshape2::melt() colnames(mns) = c(&quot;time&quot;, &quot;cluster&quot;, &quot;mean&quot;) mns = mns %&gt;% as_tibble() plot_1d(ylist_1d, obj, add_point = FALSE) + geom_line(aes(x = time, y = mean, group = cluster), data = mns, size=3) + facet_wrap(~cluster) ## Plot the probabilities trueprobs = dt$probs colnames(trueprobs) = 1:3##paste0(&quot;clust&quot;, 1:3) trueprobs = trueprobs %&gt;% as_tibble() %&gt;% add_column(time = 1:nrow(trueprobs)) trueprobs = trueprobs %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) ## Compare the true probabilities plot_prob(obj) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = trueprobs)##, alpha = .5, linetype = &quot;dashed&quot;) + Notes: look into literature for how to do diagnostics of misidentified mixture model. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
