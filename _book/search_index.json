[["index.html", "Creating the flowtrend R package 1 Introduction", " Creating the flowtrend R package Sangwon Hyun 2024-02-08 1 Introduction This package implements flowtrend, a model used for smooth estimation of mixture models across time. The documentation and package are both created using one simple command: litr::render(&quot;index.Rmd&quot;, output_format = litr::litr_gitbook()) This line is purely for testing (to be deleted later!). litr::load_all(&quot;index.Rmd&quot;)##, output_format = litr::litr_gitbook()) "],["package-setup.html", "2 Package setup 2.1 Plotting 1d data", " 2 Package setup The DESCRIPTION file is created using this code. usethis::create_package( path = &quot;.&quot;, fields = list( Package = params$package_name, Version = &quot;0.0.0.9000&quot;, Title = &quot;flowtrend&quot;, Description = &quot;Time-smooth mixture modeling for flow cytometry data.&quot;, `Authors@R` = person( given = &quot;Sangwon&quot;, family = &quot;Hyun&quot;, email = &quot;sangwonh@ucsc.edu&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;) ) ) ) usethis::use_mit_license(copyright_holder = &quot;Sangwon Hyun&quot;) The following is what will show up when someone types package?flowtrend in the console. #&#39; flowtrend #&#39; #&#39; This package implements the `flowtrend` method for automatic gating of flow cytometry data using trend filtering. #&#39; #&#39; @docType package This package will have some dependancies: library(tidyverse) library(ggplot2) usethis::use_package(&quot;tidyverse&quot;, type = &quot;depends&quot;) usethis::use_package(&quot;ggplot2&quot;) usethis::use_package(&quot;clue&quot;) usethis::use_package(&quot;glmnet&quot;) &lt;!--chapter:end:1description.Rmd--&gt; # 1d data ## Generating 1d data This function generates synthetic 1-dimensional data, and returns it in a &quot;long&quot; format matrix, with columns `time`, `y`, `mu`, and `cluster`. The latter two are the true underlying parameters. ```r #&#39; Generates some synthetic 1-dimensional data with three clusters. Returns a #&#39; data frame with (1) time, (2) Y (3) mu (4) cluster assignments. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; @param offset Defaults to 0. How much to push up cluster 1. #&#39; @param return_model If true, return true cluster means and probabilities #&#39; instead of data. #&#39; #&#39; @return long matrix with data or model. #&#39; @export gendat_1d &lt;- function(TT, ntlist, offset = 0, return_model = FALSE, sd3=NULL){ ## Basic checks stopifnot(length(ntlist) == TT) ## ## Temporary ## prob_link = cbind(1:10, (11:2)/2) ## matplot(prob_link) ## probs = exp(prob_link[,1])/(exp(prob_link[,2]) + exp(prob_link[,1])) ## plot(probs) ## diff(probs) %&gt;% plot() ## ## End of temporary ## Make cluster probabilities, by time ## cluster_prob1 = sapply(1:TT, function(tt) 1 + (tt/TT) * 5) ## cluster_prob2 = sapply(1:TT, function(tt) 8 - (tt/TT) * 5) ## cluster_prob3 = rep(3, TT) ## TT = 100 ## prob_link1_1 = sapply(1:TT, function(tt){ 3 + (tt/TT) }) ## prob_link1_2 = sapply(1:TT, function(tt){ 4 - (tt/TT)*2 }) ## prob_link1 = pmin(prob_link1_1, prob_link1_2) ## plot(prob_link1) prob_link1 = rep(NA, TT) prob_link1[1] = 2 for(tt in 2:floor(TT/2)){ prob_link1[tt] = prob_link1[tt-1] + (1/TT) } for(tt in (floor(TT/2)+1):TT){ prob_link1[tt] = prob_link1[tt-1] - .5*(1/TT) } prob_link2 = sapply(1:TT, function(tt) 3 - 0.25*(tt/TT)) prob_link3 = sapply(1:TT, function(tt) 2.5) linkmat = cbind(prob_link1, prob_link2, prob_link3) ##%&gt;% matplot() mysum = exp(linkmat) %&gt;% rowSums() cluster_prob1 = exp(prob_link1) / mysum cluster_prob2 = exp(prob_link2) / mysum cluster_prob3 = exp(prob_link3) / mysum probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) colnames(probs) = 1:3 sd1 = 0.4 sd2 = 0.5 if(is.null(sd3)) sd3 = 0.35 ## if(!is.null(sd3)) sd3 = 1/1.5 ## The ratio of amplitude:data-standard-deviation is about ## ## $1.50695$. probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time means &lt;- matrix(NA, TT, 3) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1] &lt;- offset + tt/TT - 1.5 means[tt, 2] &lt;- sin(seq(-1, 1, length.out = TT)[tt] * 3.1415) means[tt, 3] &lt;- -3+sin(seq(-1, 1, length.out = TT)[tt] * 6.282) } } colnames(means) = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) model = model %&gt;% left_join(tibble(cluster = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), sd = c(sd1, sd2, sd3)), by = &quot;cluster&quot;) ## Does this work? if(return_model) return(model) ys &lt;- lapply(1:TT, FUN = function(tt){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd1)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 2){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd2)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } if(ii == 3){ mn = means[tt,ii, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + rnorm(1, mean=0, sd = sd3)) mu[[ii]] &lt;- rep(mn, clusters_count[ii,1]) } } Y &lt;- unlist(Y) mu &lt;- unlist(mu) cluster &lt;- rep(1:3, times = clusters_count) one_df = tibble(time = tt, Y = Y, mu = mu, cluster = cluster) return(one_df) }) %&gt;% bind_rows() return(ys) } dt2ylist() is a helper that takes the output generated from gendat_1d(), and splits it by the time column to create a ylist object, which is a \\(T\\)-length list of \\(n_t \\times d\\) matrices. #&#39; Converting to a list of matrices, \\code{ylist}, to input to \\code{flowtrend()}. #&#39; #&#39; @param dt Output from \\code{gendat_1d()}. #&#39; #&#39; @return List of matrices #&#39; @export dt2ylist &lt;- function(dt){ dt%&gt;% dplyr::select(time, Y) %&gt;% arrange(time) %&gt;% group_by(time) %&gt;% group_split(.keep = FALSE) %&gt;% lapply(as.matrix) } Let’s generate some data using these functions. dt = gendat_1d(TT = 100, ntlist =rep(100,100)) print(dt) ## # A tibble: 10,000 × 4 ## time Y mu cluster ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 -1.25 -1.49 1 ## 2 1 -1.11 -1.49 1 ## 3 1 -0.690 -1.49 1 ## 4 1 -0.901 -1.49 1 ## 5 1 -1.18 -1.49 1 ## 6 1 -2.04 -1.49 1 ## 7 1 -1.77 -1.49 1 ## 8 1 -1.13 -1.49 1 ## 9 1 -1.91 -1.49 1 ## 10 1 -1.76 -1.49 1 ## # ℹ 9,990 more rows ylist = dt2ylist(dt) print(head(str(ylist[1:5]))) ## List of 5 ## $ : num [1:100, 1] -1.25 -1.109 -0.69 -0.901 -1.183 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -1.749 -1.473 -1.519 -0.927 -2.134 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -2.41 -1.34 -1.75 -1.59 -1.6 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -1.77 -2.02 -1.49 -1.67 -1.25 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## $ : num [1:100, 1] -2.158 -0.802 -1.742 -1.228 -2.053 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr &quot;Y&quot; ## NULL print(head(ylist[[1]])) ## Y ## [1,] -1.2504698 ## [2,] -1.1088363 ## [3,] -0.6899823 ## [4,] -0.9008499 ## [5,] -1.1831319 ## [6,] -2.0389235 Next, we’ll make some plotting functions 1d model and data. 2.1 Plotting 1d data Given 1d data ylist and an estimated model object obj, we want to plot both in a single plot. plot_1d() lets you do this. #&#39; Makes 1d plot of data and model #&#39; #&#39; @param ylist Data. #&#39; @param obj flowtrend object. Defaults to NULL. #&#39; @param x time points. Defaults to NULL. #&#39; @param add_point if TRUE, add the means as points. #&#39; @param idim if provided, take the idim. #&#39; #&#39; @return ggplot object with data, and optionally, a flowtrend model overlaid. #&#39; @export plot_1d &lt;- function(ylist, countslist=NULL, obj=NULL, x = NULL, add_point = FALSE, idim = NULL, alpha = .1){ ## Basic checks if(!is.null(x)){ stopifnot(length(x) == length(ylist)) times = x } else { times = 1:length(ylist) } ## If more than 2d data is provided, take the idim&#39;th info only. dimdat = ncol(ylist[[1]]) if(dimdat &gt;= 2){ assertthat::assert_that(!is.null(idim)) assertthat::assert_that(idim %in% 1:dimdat) if(!is.null(obj)){ obj$mn = obj$mn %&gt;% .[,idim,,drop=FALSE] obj$sigma = obj$sigma %&gt;% .[,idim,idim,drop=FALSE] } ylist = ylist %&gt;% lapply(function(a) a[,idim, drop=FALSE]) } if(is.null(countslist)){ ## make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() colnames(ymat) = c(&quot;time&quot;, &quot;Y&quot;, &quot;counts&quot;) ## when ylist[[tt]] already has a column name, this is needed. ## plot long matrix gg = ymat %&gt;% ggplot() + geom_point(aes(x = time, y = Y), alpha = alpha) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) ## theme(legend.position = &#39;none&#39;) } else { ## make data into long matrix ymat &lt;- lapply(1:length(ylist), FUN = function(tt){ data.frame(time = times[tt], Y = ylist[[tt]], counts = countslist[[tt]]) }) %&gt;% bind_rows() %&gt;% as_tibble() colnames(ymat) = c(&quot;time&quot;, &quot;Y&quot;, &quot;counts&quot;) ## when ylist[[tt]] already has a column name, this is needed. ## plot long matrix gg = ymat %&gt;% ggplot() + geom_raster(aes(x = time, y = Y, fill = counts)) + theme_bw() + ylab(&quot;Data&quot;) + xlab(&quot;Time&quot;) + scale_fill_gradientn(colours = c(&quot;white&quot;, &quot;black&quot;)) ## theme(legend.position = &#39;none&#39;) } if(is.null(obj)){ return(gg) } else { ## Add the model numclust = obj$numclust mnmat = obj$mn %&gt;% .[,1,] %&gt;% `colnames&lt;-`(1:numclust) %&gt;% as_tibble() %&gt;% add_column(time = times) probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) mn_long = mnmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) est_long = full_join(mn_long, prob_long, by = c(&quot;time&quot;,&quot;cluster&quot;)) gg = gg + geom_path(aes(x = time, y = mean, linewidth = prob, group = cluster, color = cluster), data = est_long, lineend = &quot;round&quot;, linejoin=&quot;mitre&quot;) if(add_point){ gg = gg + geom_line(aes(x = time, y = mean, linewidth = prob, group = cluster), data = est_long, size = rel(1), col = &#39;black&#39;) } ## TODO: make it ignore the missing values at the gaps; currently this is not coded as NAs. ## Add the estimated 95% probability regions for data. stdev = obj$sigma %&gt;% .[,,1] %&gt;% sqrt() ## band_long = ## mn_long %&gt;% mutate(upper = case_when(cluster == &quot;1&quot; ~ mean + 1.96 * stdev[1], ## cluster == &quot;2&quot; ~ mean + 1.96 * stdev[2], ## cluster == &quot;3&quot; ~ mean + 1.96 * stdev[3]), ## lower = case_when(cluster == &quot;1&quot; ~ mean - 1.96 * stdev[1], ## cluster == &quot;2&quot; ~ mean - 1.96 * stdev[2], ## cluster == &quot;3&quot; ~ mean - 1.96 * stdev[3])) mn_long_by_clust = mn_long %&gt;% group_by(cluster) %&gt;% group_split() band_long_by_clust = lapply(1:numclust, function(iclust){ mn_long_by_clust[[iclust]] %&gt;% mutate(upper = mean + 1.96 * stdev[iclust]) %&gt;% mutate(lower = mean - 1.96 * stdev[iclust]) }) band_long = band_long_by_clust %&gt;% bind_rows() gg + geom_line(aes(x = time, y = upper, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + geom_line(aes(x = time, y = lower, group = cluster, color = cluster), data = band_long, size = rel(.7), alpha = .5) + guides(size = &quot;none&quot;) # To turn off line size from legend } } The plotting function plot_1d() will be even more useful when we have a model, but can also simply plot the data ylist. Let’s try this out. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist, NULL, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model, linetype = &quot;dashed&quot;, size=2, alpha = .7) Voilà! Also, we will want to plot the estimated cluster probabilities of a model obj. #&#39; Makes cluster probability plot (lines over time). #&#39; #&#39; @param obj Estimated model (from e.g. \\code{flowtrend()}) #&#39; #&#39; @export plot_prob &lt;- function(obj, x = NULL){ ## Basic checks if(!is.null(x)){ times = x } else { stop(&quot;must provide x&quot;) times = 1:length(ylist) } numclust = obj$numclust probmat = obj$prob %&gt;% as_tibble() %&gt;% setNames(1:numclust) %&gt;% add_column(time = times) prob_long = probmat %&gt;% pivot_longer(-time, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) prob_long %&gt;% ggplot() + geom_line(aes(x=time, y = prob, group = cluster, col = cluster), size = rel(1)) + ggtitle(&quot;Estimated cluster probability&quot;) } We can’t test it out now, but we’ll use it later in 1d-example. "],["d-data.html", "3 2d data 3.1 Generating 2d data 3.2 Plotting 2d data", " 3 2d data 3.1 Generating 2d data #&#39; Generates some synthetic 2-dimensional data with three clusters. #&#39; #&#39; @param TT Number of time points. #&#39; @param nt Number of particles at each time. #&#39; #&#39; @return List containing (1) ylist, (2) mnlist, (3) clusterlist. #&#39; @export gendat_2d &lt;- function(TT, ntlist){ ## Basic checks stopifnot(length(ntlist) == TT) ## Make cluster probabilities, by time cluster_prob1 = sapply(1:TT, function(tt) sin(tt/24 * 2 * pi)/3 + 1 + (tt/TT)*5) cluster_prob2 = sapply(1:TT, function(tt) cos(tt/24 * 2 * pi)/3 + 8 - (tt/TT)*5) cluster_prob3 = rep(3, TT) probs = cbind(cluster_prob1, cluster_prob2, cluster_prob3) probs = probs/rowSums(probs) colnames(probs) = 1:3 probs_long = as_tibble(probs) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;prob&quot;) probs_long %&gt;% ggplot() + geom_line(aes(x=time,y=prob, group=cluster, col=cluster)) + ylim(c(0,1)) ## Make cluster means, by time means &lt;- array(NA, dim = c(TT, 3, 2)) for(ii in 1:3){ for(tt in 1:TT){ means[tt, 1, 1] = means[tt, 1, 2] = tt/TT + 0.5 means[tt, 2, 1] = sin(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*3.1415) means[tt, 2, 2] = 0 means[tt, 3, 1] = means[tt, 3, 2] = -3+cos(tt/24 * 2 * pi)##seq(-1, 1, length.out = TT)[tt]*6.282) } } dimnames(means)[[2]] = c(1:3) means_long = as_tibble(means) %&gt;% add_column(time = 1:TT) %&gt;% pivot_longer(-&quot;time&quot;, names_to = &quot;cluster&quot;, values_to = &quot;mean&quot;) model = full_join(means_long, probs_long, by = c(&quot;time&quot;, &quot;cluster&quot;)) ylist = list() mulist = list() clusterlist = list() for(tt in 1:TT){ Y &lt;- vector(mode = &quot;list&quot;, length = 2) mu &lt;- vector(mode = &quot;list&quot;, length = 2) clusters_count &lt;- rmultinom(n = 1, size = ntlist[tt], prob = probs[tt,]) for(ii in 1:3){ if(ii == 1){ mn = means[tt,ii,,drop=TRUE] Sigma1 = matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2) Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma= Sigma1)) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 2){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.5, 0.1)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } if(ii == 3){ mn = means[tt,ii,, drop=TRUE] Y[[ii]] &lt;- replicate(clusters_count[ii,1], mn + MASS::mvrnorm(1, mu=c(0,0), Sigma = diag(c(0.35, 0.35)))) %&gt;% t() mu[[ii]] &lt;- replicate(clusters_count[ii,1], mn) %&gt;% t() } } Y &lt;- Y %&gt;% purrr::compact() %&gt;% do.call(rbind, .) mu &lt;- mu %&gt;% purrr::compact() %&gt;% do.call(rbind, .) cluster &lt;- rep(1:3, times = clusters_count) ylist[[tt]] = Y mulist[[tt]] = mu clusterlist[[tt]] = cluster } return(list(ylist = ylist, mulist = mulist, clusterlist = clusterlist, probs = probs, means = means)) } 3.2 Plotting 2d data Here’s a simpler plotting function for 2d data at the particle level. #&#39; Simple plotter for 2d particle data. plot_2d &lt;- function(ylist, obj = NULL, tt){ ## Basic checks stopifnot(ncol(ylist[[1]]) == 2) if(!is.null(obj)) stopifnot(obj$dimdat == 2) ## Get data from one timepoint y = ylist %&gt;% .[[tt]] y = y %&gt;% as_tibble() colnames(y) = paste0(&quot;dim&quot;, c(1,2)) ## Make a simple scatterplot p = y %&gt;% ggplot() + geom_point(aes(x=dim1, y=dim2), alpha = .2) + theme_minimal() + coord_fixed() p = p + ggtitle(paste0(&quot;Time=&quot;, tt)) + coord_cartesian(xlim = c(-6.5, 2.5), ylim = c(-6.5, 2.5)) + theme_minimal() ## Adding visualizations of the model |obj| if(is.null(obj)){ return(p) } else { mnlist = lapply(1:obj$numclust, function(iclust){ one_mnmat = obj$mn[,,iclust] colnames(one_mnmat) = paste0(&quot;dim&quot;, 1:2) one_mnmat %&gt;% as_tibble() %&gt;% add_column(cluster = iclust) }) mnmat = do.call(rbind, mnlist) mn_colours = rep(&quot;red&quot;, 3) for(iclust in 1:obj$numclust){ ## Add ellipse el = ellipse::ellipse(x = obj$sigma[iclust,,], centre = obj$mn[tt,,iclust]) %&gt;% as_tibble() p = p + geom_path(aes(x = x, y = y), data = el, colour = mn_colours[iclust], lty = 2, lwd = pmin(obj$prob[tt,iclust] * 8, 0.8)) ## Add mean p = p + geom_point(aes(x = dim1, y = dim2), data = mnmat %&gt;% subset(cluster == iclust) %&gt;% .[tt,], colour = mn_colours[iclust], ## size = rel(3)) size = obj$prob[tt,iclust] * 10) } } return(p) } Let’s try it out. datobj = gendat_2d(2, c(1000,1000)) plot_2d(datobj$ylist, tt=1) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## ℹ The deprecated feature was likely used in the litr package. ## Please report the issue to the authors. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. "],["trend-filtering.html", "4 Trend filtering", " 4 Trend filtering Trend filtering is a non-parametric regression technique for a sequence of output points \\(y = (y_1,..., y_T)\\) observed at locations \\(x = (x_1, ..., x_T)\\). It is usually assumed that \\(x_1, ..., x_T\\) are evenly spaced points, though this can be relaxed. The trend filtering estimate of order \\(l\\) of the time series \\(\\mu_t = \\mathbb{E}(y_t), t \\in x\\) is obtained by calculating \\[\\hat{\\mu} = \\mathop{\\mathrm{argmin}}_{\\mu \\in \\mathbb{R}^T} \\frac{1}{2}\\| \\mu - y\\|_2^2 + \\lambda \\| D^{(l+1)} \\mu\\|_1,\\] where \\(\\lambda\\) is a tuning parameter and \\(D^{(l+1)} \\in \\mathbb{R}^{T-l}\\) is the \\((l+1)^\\text{th}\\) order discrete differencing matrix. We first need to be able to construct the trend filtering “differencing matrix” used for smoothing the mixture parameters over time. The general idea of the trend filtering is explained masterfully in [Ryan’s paper, Section 6 and equation (41)]. The differencing matrix is formed by recursion, starting with \\(D^{(1)}\\). \\[\\begin{equation*} D^{(1)} = \\begin{bmatrix} -1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 1 &amp; \\cdots &amp; 0 &amp; 0\\\\ \\vdots &amp; &amp; &amp; &amp; \\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; -1 &amp; 1 \\end{bmatrix}. \\end{equation*}\\] For \\(l&gt;1\\), the differencing matrox \\(D^{(l+1)}\\) is defined recursively as \\(D^{(l+1)} = D^{(1)} D^{(l)}\\), starting with \\(D^{(1)}\\). #&#39; Generating Difference Matrix of Specified Order #&#39; #&#39; @param n length of vector to be differenced #&#39; @param l order of differencing #&#39; @param x optional. Spacing of input points. #&#39; #&#39; @return A n by n-l-1 matrix #&#39; @export #&#39; #&#39; @examples gen_diff_mat &lt;- function(n, l, x = NULL){ ## Basic check if(!is.null(x)) stopifnot(length(x) == n) if(is.unsorted(x)) stop(&quot;x must be in increasing order!&quot;) get_D1 &lt;- function(t) {do.call(rbind, lapply(1:(t-1), FUN = function(x){ v &lt;- rep(0, t) v[x] &lt;- -1 v[x+1] &lt;- 1 v }))} if(is.null(x)){ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D &lt;- get_D1(n-k) %*% D } return(D) } } else{ if(l == 0){ return(diag(rep(1,n))) } if(l == 1){ return(get_D1(n)) } if(l &gt; 1){ D &lt;- get_D1(n) for(k in 1:(l-1)){ D1 = get_D1(n-k) facmat = diag(k / diff(x, lag = k)) D &lt;- D1 %*% facmat %*% D } return(D) } } } For equally spaced inputs with \\(l=1\\) and \\(l=2\\): l = 1 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) l = 2 Dl = gen_diff_mat(n = 10, l = l+1, x = NULL) print(Dl) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 -2 1 0 0 0 0 0 0 0 ## [2,] 0 1 -2 1 0 0 0 0 0 0 ## [3,] 0 0 1 -2 1 0 0 0 0 0 ## [4,] 0 0 0 1 -2 1 0 0 0 0 ## [5,] 0 0 0 0 1 -2 1 0 0 0 ## [6,] 0 0 0 0 0 1 -2 1 0 0 ## [7,] 0 0 0 0 0 0 1 -2 1 0 ## [8,] 0 0 0 0 0 0 0 1 -2 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -1 3 -3 1 0 0 0 0 0 0 ## [2,] 0 -1 3 -3 1 0 0 0 0 0 ## [3,] 0 0 -1 3 -3 1 0 0 0 0 ## [4,] 0 0 0 -1 3 -3 1 0 0 0 ## [5,] 0 0 0 0 -1 3 -3 1 0 0 ## [6,] 0 0 0 0 0 -1 3 -3 1 0 ## [7,] 0 0 0 0 0 0 -1 3 -3 1 When the inputs have a gap in it: ## See what a l=2 difference matrix looks like: x = (1:10)[-(3)] l = 2 TT = length(x) Dl = gen_diff_mat(n = TT, l = l+1, x = x) print(Dl) ## Formally test it d1 = gen_diff_mat(n = 10, l = l+1, x = 1:10) d2 = gen_diff_mat(n = 10, l = l+1, x = (1:10)*2) print(d1) print(d2) d1_d2_ratio = as.numeric(d1/d2) %&gt;% na.omit() %&gt;% as.numeric() testthat::expect_true(all(d1_d2_ratio==d1_d2_ratio[1])) ## correct ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] -0.6666667 1.3333333 -1.333333 0.6666667 0 0 0 0 0 ## [2,] 0.0000000 -0.3333333 2.000000 -2.6666667 1 0 0 0 0 ## [3,] 0.0000000 0.0000000 -1.000000 3.0000000 -3 1 0 0 0 ## [4,] 0.0000000 0.0000000 0.000000 -1.0000000 3 -3 1 0 0 ## [5,] 0.0000000 0.0000000 0.000000 0.0000000 -1 3 -3 1 0 ## [6,] 0.0000000 0.0000000 0.000000 0.0000000 0 -1 3 -3 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -1 3 -3 1 0 0 0 0 0 0 ## [2,] 0 -1 3 -3 1 0 0 0 0 0 ## [3,] 0 0 -1 3 -3 1 0 0 0 0 ## [4,] 0 0 0 -1 3 -3 1 0 0 0 ## [5,] 0 0 0 0 -1 3 -3 1 0 0 ## [6,] 0 0 0 0 0 -1 3 -3 1 0 ## [7,] 0 0 0 0 0 0 -1 3 -3 1 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 0.00 0.00 0.00 ## [2,] 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 0.00 0.00 ## [3,] 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 0.00 ## [4,] 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 0.00 ## [5,] 0.00 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 0.00 ## [6,] 0.00 0.00 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 0.00 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.00 -0.25 0.75 -0.75 0.25 A formal test for unevenly spaced inputs will come soon, once we’ve defined gen_tf_mat, next. We now build a function to build a lasso regressor matrix \\(H\\) that can be used to solve an equivalent problem as the trend filtering of the ’th degree. (This is stated in Lemma 4, equation (25) from Tibshirani (2014)) #&#39; A lasso regressor matrix H that can be used to solve an equivalent problem as the trend filtering of the \\code{k}&#39;th degree. #&#39; #&#39; @param n Total number of time points. #&#39; @param k Degree of trend filtering for cluster probabilities. $k=0$ is fused lasso, $k=1$ is linear trend filtering, and so on. #&#39; @param x Time points #&#39; #&#39; @return $n$ by $n$ matrix. #&#39; #&#39; @export gen_tf_mat &lt;- function(n, k, x = NULL){ if(is.null(x) ){ x = (1:n)/n } if(!is.null(x)){ stopifnot(length(x) == n) } ## For every i,j&#39;th entry, use this helper function (from eq 25 of Tibshirani ## (2014)). gen_ij &lt;- function(x, i, j, k){ xi &lt;- x[i] if(j %in% 1:(k+1)){ return(xi^(j-1)) } if(j %in% (k+2):n){ ## Special handling for k==0, See lemma 4 eq 25 if(k == 0){ prd = 1 ind = j } if(k &gt;= 1){ ind = j - (1:k) prd = prod(xi - x[ind]) } return(prd * ifelse(xi &gt;= x[max(ind)], 1, 0)) ## if(k &gt;= 1) prd = prod(xi - x[(j-k):(j-1)]) ## return(prd * ifelse(xi &gt;= x[(j-1)], 1, 0)) } } ## Generate the H matrix, entry-wise. H &lt;- matrix(nrow = n, ncol = n) for(i in 1:n){ for(j in 1:n){ H[i,j] &lt;- gen_ij(x, i,j, k) } } return(H) } Here’s a simple test of gen_tf_mat(), against an alternative function built for equally spaced data. #&#39; Creates trendfiltering regression matrix using Lemma 2 of Ryan Tibshirani&#39;s #&#39; trendfilter paper (2014); works on equally spaced data only. #&#39; #&#39; @param n Number of data points #&#39; @param k Order of trend filter. 0 is fused lasso, and so on. #&#39; @examples #&#39; ord = 1 #&#39; H_tf &lt;- gen_tf_mat_equalspace(n = 100, k = ord) #&#39; H_tf[,1] * 100 #&#39; H_tf[,2] * 100 #&#39; H_tf[,3] * 100 #&#39; H_tf[,4] * 100 #&#39; H_tf[,99] * 100 #&#39; H_tf[,100] * 100 #&#39; @return n by n matrix. gen_tf_mat_equalspace &lt;- function(n, k){ nn = n kk = k ##&#39; Connects kk to kk-1. sigm &lt;- function(ii, kk){ if(kk == 0) return(rep(1, ii)) cumsum(sigm(ii, kk-1)) } mat = matrix(NA, ncol = nn, nrow = nn) for(ii in 1:nn){ for(jj in 1:nn){ if(jj &lt;= kk+1) mat[ii,jj] = ii^(jj-1) / nn^(jj-1) if(ii &lt;= jj-1 &amp; jj &gt;= kk+2) mat[ii, jj] = 0 if(ii &gt; jj-1 &amp; jj &gt;= kk+2){ mat[ii, jj] = (sigm(ii-jj+1, kk) %&gt;% tail(1)) * factorial(kk) / nn^kk } } } return(mat) } testthat::test_that(&quot;Trend filtering regression matrix is created correctly on equally spaced data.&quot;, { ## Check that equally spaced data creates same trendfilter regression matrix ## Degree 1 H1 &lt;- gen_tf_mat(10, 1) H1_other &lt;- gen_tf_mat(10, 1, x=(1:10)/10) testthat::expect_equal(H1, H1_other) ## Degree 1 H2 &lt;- gen_tf_mat(10, 2) H2_other &lt;- gen_tf_mat(10, 2, x=(1:10)/10) testthat::expect_equal(H2, H2_other) ## Check the dimension testthat::expect_equal(dim(H1), c(10,10)) ## Check against an alternative function. for(ord in c(0,1,2,3,4)){ H &lt;- gen_tf_mat(10, ord) H_eq = gen_tf_mat_equalspace(10, ord) testthat::expect_true(max(abs(H_eq- H)) &lt; 1E-10) } }) ## Test passed Finally, let’s test the difference matrix \\(D\\) formed using unevenly spaced \\(x\\)’s. testthat::test_that(&quot;Uneven spaced D matrix is formed correctly&quot;, { x = (1:6)[-(2)] ## k = 0 is piecewise constant, k=1 is piecewise linear, etc. for(k in 0:3){ l = k+1 ## Form Dl using our function Dl &lt;- flowtrend::gen_diff_mat(n=5, l = l, x=x) ## Compare it to rows of H matrix, per section 6 of Tibshirani et al. 2014 gen_tf_mat(n = length(x), k = k, x = x) %&gt;% solve() %&gt;% `*`(factorial(k)) %&gt;% ## This part is missing in Tibshirani et al. 2014 tail(length(x)-(k+1)) -&gt; Hx ratiomat = Hx/Dl ## Process the ratios of each entry, and check that they&#39;re equal to 1 ratios = ratiomat[!is.nan(ratiomat)] ratios = ratios[is.finite(ratios)] testthat::expect_true(all.equal(ratios, rep(1, length(ratios)))) } }) ## Test passed "],["objective-data-log-likelihood.html", "5 Objective (data log-likelihood)", " 5 Objective (data log-likelihood) The function loglik_tt() calculates the log-likelihood of one cytogram, which is: \\[\\sum_{i=1}^{n_t} C_i^{(t)} \\log\\left( \\sum_{k=1}^K \\pi_{itk} \\phi(y_i^{(t)}; \\mu_{kt}, \\Sigma_k) \\right). \\] #&#39; Log likelihood for a single time point&#39;s data. #&#39; #&#39; @param mu Cluster means. #&#39; @param prob Cluster probabilities. #&#39; @param prob Cluster variances. #&#39; @param ylist Data. #&#39; @param tt Time point. loglik_tt &lt;- function(ylist, tt, mu, sigma, prob, dimdat = NULL, countslist = NULL, numclust){ ## One particle&#39;s log likelihood (weighted density) weighted.densities = sapply(1:numclust, function(iclust){ if(dimdat == 1){ return(prob[tt,iclust] * dnorm(ylist[[tt]], mu[tt,,iclust], sqrt(sigma[iclust,,]))) } if(dimdat &gt; 1){ return(prob[tt,iclust] * dmvnorm_arma_fast(ylist[[tt]], mu[tt,,iclust], as.matrix(sigma[iclust,,]), FALSE)) } }) nt = nrow(ylist[[tt]]) counts = (if(!is.null(countslist)) countslist[[tt]] else rep(1, nt)) sum_wt_dens = rowSums(weighted.densities) sum_wt_dens = sum_wt_dens %&gt;% pmax(1E-100) return(sum(log(sum_wt_dens) * counts)) } Next, here is the function that calculates the entire objective from all cytograms, given model parameter mu, prob, and sigma. #&#39; Evaluating the penalized data log-likelihood on all data \\code{ylist} given parameters \\code{mu}, \\code{prob}, and \\code{sigma}. #&#39; #&#39; @param mu #&#39; @param prob #&#39; @param prob_link #&#39; @param sigma #&#39; @param ylist #&#39; @param Dl #&#39; @param l #&#39; @param lambda #&#39; @param l_prob #&#39; @param Dl_prob #&#39; @param lambda_prob #&#39; @param alpha #&#39; @param beta #&#39; @param denslist_by_clust #&#39; @param countslist #&#39; @param unpenalized if TRUE, return the unpenalized out-of-sample fit. #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples objective &lt;- function(mu, prob, prob_link = NULL, sigma, ## TT, N, dimdat, numclust, ylist, Dlp1, l = NULL, lambda = 0, l_prob = NULL, Dlp1_prob = NULL, lambda_prob = 0, alpha = NULL, beta = NULL, denslist_by_clust = NULL, countslist = NULL, unpenalized = FALSE){ ## Set some important variables TT = dim(mu)[1] numclust = dim(mu)[3] if(is.null(countslist)){ ntlist = sapply(ylist, nrow) } else { ntlist = sapply(countslist, sum) } N = sum(ntlist) dimdat = ncol(ylist[[1]]) ## Calculate the log likelihood loglik = sapply(1:TT, function(tt){ if(is.null(denslist_by_clust)){ return(loglik_tt(ylist, tt, mu, sigma, prob, countslist, numclust = numclust, dimdat = dimdat)) } else { ## TODO: This function doesn&#39;t exist yet, but might need to, since.. speed! return(loglik_tt_precalculate(ylist, tt, denslist_by_clust, prob, countslist, numclust)) } }) if(unpenalized){ obj = -1/N * sum(unlist(loglik)) return(obj) } else { ## Return penalized likelihood mu.splt &lt;- asplit(mu, MARGIN = 3) diff_mu &lt;- sum(unlist(lapply(mu.splt, FUN = function(m) sum(abs(Dlp1 %*% m))))) diff_prob &lt;- sum(abs(Dlp1_prob %*% prob_link)) obj = -1/N * sum(unlist(loglik)) + lambda * diff_mu + lambda_prob * diff_prob return(obj) } } Here’s a helper to check numerical convergence of the EM algorithm. #&#39; Checks numerical improvement in objective value. Returns TRUE if |old-new|/|old| is smaller than tol. #&#39; #&#39; @param old Objective value from previous iteration. #&#39; @param new Objective value from current iteration. #&#39; @param tol Numerical tolerance. check_converge_rel &lt;- function(old, new, tol=1E-6){ return(abs((old-new)/old) &lt; tol ) } Here’s also a helper function to do the softmax-ing of \\(\\alpha_t \\in \\mathbb{R}^K\\). #&#39; Softmax function. #&#39; #&#39; @param prob_link alpha, which is a (T x K) matrix. #&#39; #&#39; @return exp(alpha)/rowSum(exp(alpha)). A (T x K) matrix. softmax &lt;- function(prob_link){ exp_prob_link = exp(prob_link) prob = exp_prob_link / rowSums(exp_prob_link) } testthat::test_that(&quot;Test for softmax&quot;,{ link = runif(100, min = -10, max = 10) %&gt;% matrix(nrow = 10, ncol = 10) testthat::expect_true(all(abs(rowSums(softmax(link)) - 1) &lt; 1E-13)) }) ## Test passed "],["initial-parameters-for-em-algorithm.html", "6 Initial parameters for EM algorithm", " 6 Initial parameters for EM algorithm The EM algorithm requires some initial values for \\(\\mu\\), \\(\\pi\\) and \\(\\Sigma\\). Initializing \\(\\pi\\) is done in one line, prob = matrix(1/numclust, nrow = TT, ncol = numclust), which sets everything to \\(1/K\\). For \\(\\mu\\) and \\(\\Sigma\\), we write some functions. Essentially, initial means \\(\\mu\\) are jittered versions of a \\(K\\) means that are drawn from a downsampled version of \\(ylist\\) (downsampling is done because \\(ylist\\) can have a large number of particles). \\(\\Sigma\\) is \\(d\\times d\\) identity matrices, with fac=1 diagonal by default. #&#39; Initialize the cluster centers. #&#39; #&#39; @param ylist A T-length list of (nt by 3) datasets. There should be T of #&#39; such datasets. 3 is actually \\code{mulen}. #&#39; @param numclust Number of clusters (M). #&#39; @param TT total number of (training) time points. #&#39; #&#39; @return An array of dimension (T x dimdat x M). #&#39; @export init_mn &lt;- function(ylist, numclust, TT, dimdat, countslist = NULL, seed=NULL){ if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) .Random.seed &lt;&lt;- seed } if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } ## Initialize the means by (1) collapsing to one cytogram (2) random ## sampling from this distribution, after truncation, TT = length(ylist) ylist_downsampled &lt;- lapply(1:TT, function(tt){ y = ylist[[tt]] counts = countslist[[tt]] ## Sample so that, in total, we get mean(nt) * 30 sized sample. In the case ## of binned data, nt is the number of bins. if(nrow(y) &gt; 500) nsize = pmin(nrow(y) / TT * 30, nrow(y)) else nsize = nrow(y) some_rows = sample(1:nrow(y), size = nsize, prob = counts/sum(counts)) y[some_rows,, drop=FALSE] }) ## Jitter the means a bit yy = do.call(rbind, ylist_downsampled) new_means = yy[sample(1:nrow(yy), numclust),, drop=FALSE] jitter_sd = apply(yy, 2, sd) / 100 jitter_means = MASS::mvrnorm(n = nrow(new_means), mu = rep(0, dimdat), Sigma = diag(jitter_sd, ncol = dimdat)) new_means = new_means + jitter_means ## Repeat TT times == flat/constant initial means across time. mulist = lapply(1:TT, function(tt){ new_means }) ## } else { ## TT = length(ylist) ## ylist_downsampled &lt;- lapply(1:TT, function(tt){ ## y = ylist[[tt]] ## counts = countslist[[tt]] ## nsize = pmin(nrow(y) / TT * 30, nrow(y)) ## y[sample(1:nrow(y), size = nsize),, drop=FALSE] ## }) ## ## Combine all the particles ## yy = do.call(rbind, ylist_downsampled) ## ## Get K new means from these ## inds = sample(1:nrow(yy), numclust) ## new_means = yy[inds,, drop=FALSE] ## mulist = lapply(1:TT, function(tt){ new_means }) ## } ## New (T x dimdat x numclust) array is created. muarray = array(NA, dim=c(TT, dimdat, numclust)) for(tt in 1:TT){ muarray[tt,,] = as.matrix(mulist[[tt]]) } return(muarray) } #&#39; Initialize the covariances. #&#39; #&#39; @param data The (nt by 3) datasets. There should be T of them. #&#39; @param numclust Number of clusters. #&#39; @param fac Value to use for the diagonal of the (dimdat x dimdat) covariance #&#39; matrix. #&#39; #&#39; @return An (K x dimdat x dimdat) array containing the (dimdat by dimdat) #&#39; covariances. #&#39; @export init_sigma &lt;- function(data, numclust, fac = 1){ ndat = nrow(data[[1]]) pdat = ncol(data[[1]]) sigmas = lapply(1:numclust, function(iclust){ onesigma = diag(fac * rep(1, pdat)) if(pdat==1) onesigma = as.matrix(fac) colnames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) rownames(onesigma) = paste0(&quot;datcol&quot;, 1:pdat) return(onesigma) }) sigmas = abind::abind(sigmas, along=0) return(sigmas) } Here’s a helper function for printing the progress. #&#39; A helper function to print the progress of a loop or simulation. #&#39; #&#39; @param isim Replicate number. #&#39; @param nsim Total number of replicates. #&#39; @param type Type of job you&#39;re running. Defaults to &quot;simulation&quot;. #&#39; @param lapsetime Lapsed time, in seconds (by default). #&#39; @param lapsetimeunit &quot;second&quot;. #&#39; @param start.time start time. #&#39; @param fill Whether or not to fill the line. #&#39; #&#39; @return No return print_progress &lt;- function(isim, nsim, type = &quot;simulation&quot;, lapsetime = NULL, lapsetimeunit = &quot;seconds&quot;, start.time = NULL, fill = FALSE){ ## If lapse time is present, then use it if(fill) cat(fill = TRUE) if(is.null(lapsetime) &amp; is.null(start.time)){ cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim) } else { if(!is.null(start.time)){ lapsetime = round(difftime(Sys.time(), start.time, units = &quot;secs&quot;), 0) remainingtime = round(lapsetime * (nsim-isim)/isim,0) endtime = Sys.time() + remainingtime } cat(&quot;\\r&quot;, type, &quot; &quot;, isim, &quot;out of&quot;, nsim, &quot;with lapsed time&quot;, lapsetime, lapsetimeunit, &quot;and remaining time&quot;, remainingtime, lapsetimeunit, &quot;and will finish at&quot;, strftime(endtime)) } if(fill) cat(fill = TRUE) } "],["e-step.html", "7 E step", " 7 E step #&#39; E step, which updates the &quot;responsibilities&quot;, which are posterior membership probabilities of each particle. #&#39; #&#39; @param mn #&#39; @param sigma #&#39; @param prob #&#39; @param ylist #&#39; @param numclust #&#39; @param denslist_by_clust #&#39; @param first_iter #&#39; @param countslist #&#39; #&#39; @return #&#39; @export #&#39; Estep &lt;- function(mn, sigma, prob, ylist = NULL, numclust, denslist_by_clust = NULL, first_iter = FALSE, countslist = NULL){ ## Basic setup TT = length(ylist) ntlist = sapply(ylist, nrow) resp = list() dimdat = dim(mn)[2] assertthat::assert_that(dim(mn)[1] == length(ylist)) ## Helper to calculate Gaussian density for each \\code{N(y_{t,k},mu_{t,k} and ## Sigma_k)}. calculate_dens &lt;- function(iclust, tt, y, mn, sigma, denslist_by_clust, first_iter) { mu &lt;- mn[tt, , iclust] if (dimdat == 1) { dens = dnorm(y, mu, sd = sqrt(sigma[iclust, , ])) } else { dens = dmvnorm_arma_fast(y, mu, sigma[iclust,,], FALSE) } return(dens) } ## Calculate posterior probability of membership of $y_{it}$. ncol.prob = ncol(prob) for (tt in 1:TT) { ylist_tt = ylist[[tt]] densmat &lt;- sapply(1:numclust, calculate_dens, tt, ylist_tt, mn, sigma, denslist_by_clust, first_iter) wt.densmat &lt;- matrix(prob[tt, ], nrow = ntlist[tt], ncol = ncol.prob, byrow = TRUE) * densmat wt.densmat = wt.densmat + 1e-10 wt.densmat &lt;- wt.densmat/rowSums(wt.densmat) resp[[tt]] &lt;- wt.densmat } ## Weight the responsibilities by $C_{it}$. if (!is.null(countslist)) { resp &lt;- Map(function(myresp, mycount) { myresp * mycount }, resp, countslist) } return(resp) } The E step should return a list of exactly the same size and format as ylist, which is a \\(T\\) -length list of matrices of size \\(n_t \\times d\\). (This test fails for some reason; will come back to this.) testthat::test_that(&quot;E step returns appropriately sized responsibilities.&quot;,{ ## Generate some fake data TT = 10 ylist = lapply(1:TT, function(tt){ runif(90) %&gt;% matrix(ncol = 3, nrow = 30)}) numclust = 3 dimdat = 3 ## Initialize a few parameters, not carefully sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. ## ## Calculate responsibility ## resp = Estep(mn = mn, sigma = sigma, prob = prob, ylist = ylist, numclust = numclust) ## Check these things ## testthat::expect_equal(length(resp), length(ylist)) ## ?testthat::expect_equal ## testthat::expect_equal(sapply(resp, dim), sapply(ylist, dim), reporter = &quot;stop&quot;) }) ## -- Skip (???): E step returns appropriately sized responsibilities. ------------ ## Reason: empty test "],["m-step.html", "8 M step 8.1 M step for \\(\\pi\\) 8.2 M step for \\(\\Sigma\\) 8.3 M step for \\(\\mu\\) 8.4 Helpers for M step (\\(\\mu\\)) 8.5 All Rcpp functions", " 8 M step The M step of the EM algorithm has three steps – one each for \\(\\mu\\), \\(\\pi\\), and \\(\\Sigma\\). 8.1 M step for \\(\\pi\\) #&#39; The M step for the cluster probabilities #&#39; #&#39; @param resp Responsibilities. #&#39; @param H_tf Trend filtering matrix. #&#39; @param countslist Particle multiplicities. #&#39; @param lambda_prob Regularization. #&#39; @param l_prob Trend filtering degree. #&#39; #&#39; @return (T x k) matrix containing the alphas, for \\code{prob = exp(alpha)/ #&#39; rowSums(exp(alpha))}. #&#39; @export #&#39; Mstep_prob &lt;- function(resp, H_tf, countslist = NULL, lambda_prob = NULL, l_prob = NULL, x = NULL){ ## Basic setup TT &lt;- length(resp) ## Basic checks stopifnot(is.null(l_prob) == is.null(lambda_prob)) ## If glmnet isn&#39;t actually needed, don&#39;t use it. if(is.null(l_prob) &amp; is.null(lambda_prob)){ ## Calculate the average responsibilities, per time point. if(is.null(countslist)){ resp.avg &lt;- lapply(resp, colMeans) %&gt;% do.call(rbind, .) } else { resp.avg &lt;- lapply(1:TT, FUN = function(ii){ colSums(resp[[ii]])/sum(countslist[[ii]]) }) %&gt;% do.call(rbind, .) } return(resp.avg) ## If glmnet is needed, use it. } else { lambda_range &lt;- function(lam, nlam = 50, lam.max = 5*lam){ return(exp(seq(log(lam.max), log(lam), length.out = nlam))) } penalty.facs &lt;- c(rep(0, l_prob+1), rep(1, nrow(H_tf) - l_prob - 1)) resp.predict &lt;- do.call(rbind, lapply(resp, colSums)) glmnet_obj &lt;- glmnet::glmnet(x = H_tf, y = resp.predict, family = &quot;multinomial&quot;, penalty.factor = penalty.facs, maxit = 1e7, lambda = mean(penalty.facs)*lambda_range(lambda_prob), standardize = F, intercept = FALSE) pred_link &lt;- predict(glmnet_obj, newx = H_tf, type = &quot;link&quot;, s = mean(penalty.facs) * lambda_prob)[,,1] return(pred_link) } } This should return a \\(T\\) by \\(K\\) matrix, which we’ll test here: testthat::test_that(&quot;Mstep of pi returns a (T x K) matrix.&quot;, { ## Generate some fake responsibilities and trend filtering matrix TT = 100 numclust = 3 nt = 10 resp = lapply(1:TT, function(tt){ oneresp = runif(nt*numclust) %&gt;% matrix(ncol=numclust) oneresp = oneresp/rowSums(oneresp) }) H_tf &lt;- gen_tf_mat(n = TT, k = 0) ## Check the size pred_link = Mstep_prob(resp, H_tf, l_prob = 0, lambda_prob = 1E-3) testthat::expect_equal(dim(pred_link), c(TT, numclust)) pred_link = Mstep_prob(resp, H_tf) testthat::expect_equal(dim(pred_link), c(TT, numclust)) ## Check the correctness pred_link = Mstep_prob(resp, H_tf) }) Each row of this matrix should contain the fitted values \\(\\alpha_k \\in \\mathbb{R}^3\\) where \\(\\alpha_{kt} = h_t^T w_{k}\\), for.. \\(h_t\\) that are rows of the trend filtering matrix \\(H \\in \\mathbb{R}^{T \\times T}\\). \\(w_k \\in \\mathbb{R}^{n}\\) that are the regression coefficients estimated by glmnet(). Here is a test for the correctness of the M step for \\(\\pi\\). testthat::test_that(&quot;Test the M step of \\pi against CVXR&quot;, {}) 8.2 M step for \\(\\Sigma\\) #&#39; M step for cluster covariance (sigma). #&#39; #&#39; @param resp Responsibility. #&#39; @param ylist Data. #&#39; @param mn Means #&#39; @param numclust Number of clusters. #&#39; #&#39; @return (K x d x d) array containing K (d x d) covariance matrices. #&#39; @export #&#39; #&#39; @examples Mstep_sigma &lt;- function(resp, ylist, mn, numclust){ ## Find some sizes TT = length(ylist) ntlist = sapply(ylist, nrow) dimdat = ncol(ylist[[1]]) cs = c(0, cumsum(ntlist)) ## Set up empty residual matrix (to be reused) cs = c(0, cumsum(ntlist)) vars &lt;- vector(mode = &quot;list&quot;, numclust) ylong = do.call(rbind, ylist) ntlist = sapply(ylist, nrow) irows = rep(1:nrow(mn), times = ntlist) for(iclust in 1:numclust){ resp.thisclust = lapply(resp, function(myresp) myresp[,iclust, drop = TRUE]) resp.long = do.call(c, resp.thisclust) mnlong = mn[irows,,iclust] if(is.vector(mnlong)) mnlong = mnlong %&gt;% cbind() resid &lt;- ylong - mnlong resid_weighted &lt;- resp.long * resid sig_temp &lt;- t(resid_weighted) %*% resid/sum(resp.long) vars[[iclust]] &lt;- sig_temp } ## Make into an array sigma_array = array(NA, dim=c(numclust, dimdat, dimdat)) for(iclust in 1:numclust){ sigma_array[iclust,,] = vars[[iclust]] } ## Basic check stopifnot(all(dim(sigma_array) == c(numclust, dimdat, dimdat))) return(sigma_array) } 8.3 M step for \\(\\mu\\) This is a big one. It uses the ADMM algorithm as written in section OO of the paper, reproduced briefly here. We need a convergence checker for the outer layer of LA-ADMM, which checks whether the objective values have plateaued. Next, we define the main function Mstep_mu(). This uses a “locally-adaptive” ADMM paper. M-step_mu() calls la_admm_oneclust() on each cluster \\(k=1,\\cdots, K\\); this function will be introduced shortly. #&#39; Computes the M step for mu. TODO: use templates for the argument. As shown #&#39; here: #&#39; https://stackoverflow.com/questions/15100129/using-roxygen2-template-tags #&#39; #&#39; @param resp Responsbilities of each particle. #&#39; @param ylist #&#39; @param lambda #&#39; @param l #&#39; @param sigma #&#39; @param sigma_eig_by_clust #&#39; @param Dl #&#39; @param Dlp1 #&#39; @param TT #&#39; @param N #&#39; @param dimdat #&#39; @param first_iter #&#39; @param mus #&#39; @param Zs #&#39; @param Ws #&#39; @param uws #&#39; @param uzs #&#39; @param maxdev #&#39; @param x #&#39; @param niter #&#39; @param err_rel #&#39; @param err_abs #&#39; @param zerothresh #&#39; @param local_adapt #&#39; @param local_adapt_niter #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples Mstep_mu &lt;- function(resp, ylist, lambda = 0.5, l = 3, sigma, sigma_eig_by_clust = NULL, Dlsqrd, Dl, tDl, Dlp1, TT, N, dimdat, first_iter = TRUE, e_mat, ## Warm startable variables mus = NULL, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, ## End of warm startable variables maxdev = NULL, x = NULL, niter = (if(local_adapt) 1e2 else 1e3), err_rel = 1E-3, err_abs = 0, zerothresh = 1E-6, local_adapt = FALSE, local_adapt_niter = 10, rho_init = 0.01, iter = NULL){ #################### ## Preliminaries ### #################### TT = length(ylist) numclust = ncol(resp[[1]]) dimdat = ncol(ylist[[1]]) ntlist = sapply(ylist, nrow) resp.sum = lapply(resp, colSums) %&gt;% do.call(rbind, .) N = sum(unlist(resp.sum)) ## Other preliminaries schur_syl_A_by_clust = schur_syl_B_by_clust = term3list = list() ybarlist = list() ycentered_list = Xcentered_list = yXcentered_list = list() Qlist = list() sigmainv_list = list() for(iclust in 1:numclust){ ## Retrieve sigma inverse from pre-computed SVD, if necessary if(is.null(sigma_eig_by_clust)){ sigmainv = solve(sigma[iclust,,]) } else { sigmainv = sigma_eig_by_clust[[iclust]]$sigma_inv } resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) AB &lt;- get_AB_mats(y = y, resp = resp.iclust, Sigma_inv = sigmainv, e_mat = e_mat, N = N, Dlp1 = Dlp1, Dl = Dl, Dlsqrd = Dlsqrd, rho = rho_init, z = NULL, w = NULL, uz = NULL, uw = NULL) ## Store the Schur decomposition schur_syl_A_by_clust[[iclust]] = myschur(AB$A) schur_syl_B_by_clust[[iclust]] = myschur(AB$B) ycentered &lt;- NULL ycentered_list[[iclust]] = ycentered sigmainv_list[[iclust]] = sigmainv } ########################################## ## Run ADMM separately on each cluster ## ######################################### admm_niters = admm_inner_iters = vector(length = numclust, mode = &quot;list&quot;) if(first_iter) mus = vector(length = numclust, mode = &quot;list&quot;) # if(first_iter){ Zs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat)) Ws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT - l, ncol = dimdat)) uzs &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT, ncol = dimdat)) uws &lt;- lapply(1:numclust, function(x) matrix(0, nrow = TT - l, ncol = dimdat)) # Zs = Ws = Us = vector(length = numclust, mode = &quot;list&quot;) # } ## For every cluster, run LA-ADMM start.time = Sys.time() for(iclust in 1:numclust){ resp.iclust &lt;- lapply(resp, FUN = function(r) matrix(r[,iclust])) ## Possibly locally adaptive ADMM, for now just running with rho == lambda res = la_admm_oneclust(K = (if(local_adapt) local_adapt_niter else 1), local_adapt = local_adapt, iclust = iclust, niter = niter, TT = TT, N = N, dimdat = dimdat, maxdev = maxdev, schurA = schur_syl_A_by_clust[[iclust]], schurB = schur_syl_B_by_clust[[iclust]], sigmainv = sigmainv_list[[iclust]], rho = rho_init, rhoinit = rho_init, ## rho = (if(iclust == 1) rho_init else res$rho/2), ## rhoinit = (if(iclust == 1) rho_init else res$rho/2), sigma = sigma, lambda = lambda, resp = resp.iclust, resp_sum = resp.sum[,iclust], l = l, Dlp1 = Dlp1, Dl = Dl, tDl = tDl, y = ylist, err_rel = err_rel, err_abs = err_abs, zerothresh = zerothresh, sigma_eig_by_clust = sigma_eig_by_clust, iter = iter, ## Warm starts from previous *EM* iteration first_iter = first_iter, ## mu = mus[[iclust]], ## I think we want this. uw = uws[[iclust]], uz = uzs[[iclust]], z = Zs[[iclust]], w = Ws[[iclust]]) ## print(res$rho) ## Store the results mus[[iclust]] = res$mu admm_niters[[iclust]] = res$kk admm_inner_iters[[iclust]] = res$inner.iter ## Store other things for for warmstart ## mus[[iclust]] = res$mu Zs[[iclust]] = res$Z uzs[[iclust]] = res$uz uws[[iclust]] = res$uw Ws[[iclust]] = res$W ## The upper triangular matrix remains the same. (code missing?) } ## Aggregate the yhats into one array mu_array = array(NA, dim = c(TT, dimdat, numclust)) for(iclust in 1:numclust){ mu_array[,,iclust] = mus[[iclust]] } ## Each are lists of length |numclust|. return(list(mns = mu_array, admm_niters = admm_niters, admm_inner_iters = admm_inner_iters, ## For warmstarts Zs = Zs, Ws = Ws, uws = uws, uzs = uzs, N = N, ## Return the column rho = res$rho, ## For using in the Sigma M step ycentered_list = ycentered_list, Xcentered_list = Xcentered_list, yXcentered_list = yXcentered_list, Qlist = Qlist )) } The locally adaptive admm for a single cluster involves an inner and outer loop. The inner loop is an ADMM for a fixed \\(\\rho\\). The outer loop is written here in la_admm_oneclust, and runs the inner ADMM with a fixed step-size \\(\\rho\\) while sequentially doubling \\(\\rho\\). #&#39; Locally adaptive ADMM. #&#39; #&#39; @param K #&#39; @param ... #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples la_admm_oneclust &lt;- function(K, ...){ ## Initialize arguments for ADMM. args &lt;- list(...) p = args$p l = args$l TT = args$TT dimdat = args$dimdat rhoinit = args$rhoinit ## This initialization can come from the previous *EM* iteration. if(args$first_iter){ mu = matrix(0, nrow = TT, ncol = dimdat) Z &lt;- matrix(0, nrow = TT, ncol = dimdat) W &lt;- matrix(0, nrow = TT-l, ncol = dimdat) uz &lt;- matrix(0, nrow = TT, ncol = dimdat) uw &lt;- matrix(0, nrow = TT - l, ncol = dimdat) args[[&#39;mu&#39;]] &lt;- mu args[[&#39;z&#39;]] &lt;- Z args[[&#39;w&#39;]] &lt;- W args[[&#39;uz&#39;]] &lt;- uz args[[&#39;uw&#39;]] &lt;- uw } cols = c() some_admm_objectives = c() ## Run ADMM repeatedly with (1) double rho, and (2) previous b for(kk in 1:K){ if(kk &gt; 1){ ## Z = matrix(0, nrow = TT, ncol = dimdat) ## W = matrix(0, nrow = TT - l , ncol = dimdat) ## uz = matrix(0, nrow = TT, ncol = dimdat) ## uw = matrix(0, nrow = TT - l , ncol = dimdat) ## These ensure warm starts are true args[[&#39;mu&#39;]] &lt;- mu args[[&#39;z&#39;]] &lt;- Z args[[&#39;w&#39;]] &lt;- W args[[&#39;uz&#39;]] &lt;- uz args[[&#39;uw&#39;]] &lt;- uw args[[&#39;rho&#39;]] &lt;- rho } ## Run ADMM args[[&#39;outer_iter&#39;]] &lt;- kk ## Call main function argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;admm_oneclust&quot;)), argn)) res = eval(call, args) if(any(abs(res$mu)&gt;1E2)){ stop(&quot;mu is blowing up! Probably because the initial ADMM step size (rho) is too large (and possibly the ball constraint on the means is large.&quot;) } some_admm_objectives = c(some_admm_objectives, res$single_admm_objective) ## Handling the scenario where the objectives are all zero padding = 1E-12 some_admm_objectives = some_admm_objectives + padding ## See if outer iterations should terminate if(res$converge){ res$converge &lt;- T break } ## Update some parameters; double the rho value, and update the B matrix rho = 2 * args$rho ## tQ = 2 * args$schurB$tQ ## This seems wrong. Delete now. mu = res$mu Z = res$Z W = res$W uz = res$uz uw = res$uw ## print(&quot;args$rho&quot;) ## print(args$rho) } if(!res$converge) warning(&quot;ADMM didn&#39;t converge for one cluster.&quot;) ## Record how long the admm took; in terms of # iterations. res$kk = kk ## Record the final rho res$rho = args$rho return(res) } Next, the inner loop. The workhorse admm_oneclust() actually performs the ADMM update for one cluster for a fixed step-size \\(\\rho\\) across optimization iterations iter=1,...,n. This admm_oneclust() uses the following helpers: W_update_fused(): this uses the prox function, which uses a RCpp function. prox_dp. Z_update() U_update_Z() U_update_W() A prox_dp function, written in C, will be used. #&#39; Solve a fused lasso problem for the W update. Internally, a fused lasso dynamic #&#39; programming solver \\code{prox()} (which calls \\code{prox_dp()} written in C) #&#39; is used. #&#39; W_update_fused &lt;- function(l, TT, mu, uw, rho, lambda, Dl){ # modified lambda for fused lasso routine mod_lam &lt;- lambda/rho # generating pseudo response xi if( l &lt; 0 ){ stop(&quot;l should never be /below/ zero!&quot;) } else if( l == 0 ){ xi &lt;- mu + 1/rho * uw ## This is faster } else { xi &lt;- Dl %*% mu + 1/rho * uw if(any(is.nan(xi))) browser() ## l = 2 is quadratic trend filtering ## l = 1 is linear trend filtering ## l = 0 is fused lasso ## D^{(1)} is first differences, so it correponds to l=0 ## Dl = gen_diff_mat(n = TT, l = l, x = x) &lt;--- (T-l) x T matrix } ## Running the fused LASSO ## which solves min_zhat 1/2 |z-zhat|_2^2 + lambda |D^{(1)}zhat| ## fit &lt;- prox(z = xi, lam = mod_lam) ## fit &lt;- prox_dp(z = xi, lam = mod_lam) ## instead of FlowTF::prox() ## fit &lt;- flowtrendprox::prox_dp(z = xi, lam = mod_lam) fit &lt;- FlowTF::prox(z = xi, lam = mod_lam) ## TODO: eventually change to fit &lt;- flowtrendprox::prox(z = xi, lam = mod_lam) return(fit) } ## This function is in FlowTF now. It&#39;s the last function there! ## #&#39; Fused LASSO for scalar inputs. ## #&#39; ## #&#39; @param z scalar input to be smoothed via the fused LASSO ## #&#39; @param lam Fused LASSO smoothing parameter ## #&#39; ## #&#39; @return Estimates of the fused LASSO solution ## #&#39; @export prox ## #&#39; ## #&#39; @references All credit for writing this function goes to Ryan Tibshirani. See ## #&#39; the original code for calling this function at ## #&#39; ## #&#39; @useDynLib FlowTF prox_dp ## prox &lt;- function(z, lam) { ## o &lt;- .C(&quot;prox_dp&quot;, ## as.integer(length(z)), ## as.double(z), ## as.double(lam), ## as.double(numeric(length(z))), ## # dup=FALSE, ## PACKAGE=&quot;FlowTF&quot;) ## return(o[[4]]) ## } Z_update &lt;- function(m, Uz, C, rho){ mat = m + Uz/rho Z = projCmat(mat, C) return(Z) } projCmat &lt;- function(mat, C){ if(!is.null(C)){ vlens = sqrt(rowSums(mat * mat)) inds = which(vlens &gt; C) if(length(inds) &gt; 0){ mat[inds,] = mat[inds,] * C / vlens[inds] } } return(mat) } #&#39; @param U (T x dimdat) matrix. U_update_Z &lt;- function(U, rho, mu, Z, TT){ # return(U + rho * (scale(mu, scale = F) - Z)) stopifnot(nrow(U) == TT) Unew = U + rho * (mu - colMeans(mu) - Z) ## Expect a (T-l) x dimdat matrix. stopifnot(all(dim(U) == dim(Unew))) stopifnot(nrow(U) == TT) return(Unew) } #&#39; @param U ((T-l) x dimdat) matrix. U_update_W &lt;- function(U, rho, mu, W, l, Dl, TT){ # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso # D^{(1)} is first differences, so it correponds to l=0 # D^{(l+1)} is used for order-l trend filtering. stopifnot(nrow(W) == TT - l) ## if(l == 0){ ## Unew = U + rho * (mu - W) ## } else { ## Unew = U + rho * ( diff(mu, differences = l) - W) ## } Unew &lt;- U + rho * (Dl %*% mu - W) ## Expect a (T-l) x dimdat matrix. stopifnot(all(dim(U) == dim(Unew))) stopifnot(nrow(U) == TT-l) return(Unew) } Here is that main workhorse admm_oneclust(). #&#39; One cluster&#39;s admm for a fixed step size (rho). admm_oneclust &lt;- function(iclust = 1, niter, y, Dl, tDl, Dlp1, l = NULL, TT, N, dimdat, maxdev, rho, rhoinit = rho, Xinv, schurA, schurB, sigmainv, lambda, resp, resp_sum, ylist, err_rel = 1e-3, err_abs = 0, zerothresh, mu, z, w, uw, uz, ## warmstart = FALSE, ## mu.warm = if(!warmstart) NULL, first_iter,## Not used iter, outer_iter, local_adapt, sigma, sigma_eig_by_clust){ ## Initialize the variables ### ## resid_mat = matrix(NA, nrow = ceiling(niter/5), ncol = 4) ## colnames(resid_mat) = c(&quot;primresid&quot;, &quot;primerr&quot;, &quot;dualresid&quot;, &quot;dualerr&quot;) resid_mat = matrix(NA, nrow = ceiling(niter/5), ncol = 6) colnames(resid_mat) = c(&quot;prim1&quot;, &quot;prim2&quot;, &quot;primresid&quot;, &quot;primerr&quot;, &quot;dualresid&quot;, &quot;dualerr&quot;) rhofac = rho / rhoinit ## This doesn&#39;t change over iterations schurB = myschur(schurB$orig * rhofac) ## In flowmix, this is done on A. Here, it&#39;s done on B (in AX + XB + C = 0). TA = schurA$T ##* rhofac TB = schurB$T UA = schurA$Q UB = schurB$Q tUA = schurA$tQ tUB = schurB$tQ ## This also doesn&#39;t change over iterations C1 &lt;- do.call(cbind, lapply(1:TT, FUN = function(tt){ multmat &lt;- apply(y[[tt]], FUN = function(yy) yy * resp[[tt]], MARGIN = 2) sigmainv %*% colSums(multmat) })) for(iter in 1:niter){ syl_C &lt;- get_C_mat(C1 = C1, resp_sum = resp_sum, TT = TT, dimdat = dimdat, Sigma_inv = sigmainv, N = N, Dl = Dl, rho = rho, z = z, w = w, uz = uz, uw = uw, l=l) FF = (-1) * tUA %*% syl_C %*% UB mu = UA %*% matrix_function_solve_triangular_sylvester_barebonesC2(TA, TB, FF) %*% tUB mu = t(mu) stopifnot(nrow(mu) == TT) stopifnot(ncol(mu) == dimdat) ## if(warmstart &amp; iter == 1){ ## print(&quot;warmed up mu!&quot;) ## mu = mu.warm ## } z &lt;- Z_update(mu - colMeans(mu), Uz = uz, C = maxdev, rho = rho) if(any(abs(mu)&gt;1E2)){ stop(&quot;mu is blowing up!&quot;) ## break } wlist = lapply(1:dimdat, function(j){ W_update_fused(l = l, TT = TT, mu = mu[, j, drop = TRUE], rho = rho, lambda = lambda, uw = uw[,j,drop=TRUE], Dl = Dl)}) w &lt;- do.call(cbind, wlist) stopifnot(nrow(w) == TT-l) stopifnot(ncol(w) == dimdat) uz = U_update_Z(uz, rho, mu, z, TT) ## uw = U_update_W(uw, rho, mu, w, l, TT) uw = U_update_W(uw, rho, mu, w, l, Dl, TT) ## Check convergence if( iter &gt; 1 &amp; iter %% 5 == 0){## &amp; !local_adapt){ ## Calculate convergence criterion obj = check_converge(mu, rho, w, z, w_prev, z_prev, uw, uz, Dl, tDl, err_rel = err_rel, err_abs = err_abs) jj = (iter/ 5) resid_mat[jj,] = c( norm(obj$primal_resid1, &quot;F&quot;), ## Temp norm(obj$primal_resid2, &quot;F&quot;), ## Temp norm(obj$primal_resid, &quot;F&quot;), obj$primal_err, norm(obj$dual_resid,&quot;F&quot;), obj$dual_err) if(is.na(obj$converge)){ obj$converge &lt;- converge = FALSE warning(&quot;Convergence was NA&quot;) } if(obj$converge){ converge = TRUE break } else { converge = FALSE } } w_prev = w z_prev = z } if(FALSE){ ## Calculate optimization objective values for this cluster. obj.value &lt;- objective_per_cluster(y = y, mu = mu, resp = resp, Sigma_inv = sigmainv, TT = TT, d = dimdat, Dlp1 = Dlp1, Dl = Dl, l = l, maxdev = maxdev, lambda = lambda, rho = rho, N = N) ## This is very expensive to do within each ADMM iteration, so it&#39;s commented out for now. } obj.value = NA return(list(mu = mu, resid_mat = resid_mat, converge = converge, ## Other variables to return. Z = z, W = w, uz = uz, uw = uw, inner.iter = iter, single_admm_objective = obj.value)) } 8.4 Helpers for M step (\\(\\mu\\)) First, let’s start with a few R functions. #&#39; @param TT Number of time points. etilde_mat &lt;- function(TT){ mats &lt;- lapply(1:TT, FUN = function(t){ e_vec &lt;- rep(0, TT) e_vec[t] &lt;- 1 (e_vec - 1/TT) %*% t(e_vec - 1/TT) }) Reduce(&#39;+&#39;, mats) } get_AB_mats &lt;- function(y, resp, Sigma_inv, e_mat, Dlsqrd, N, Dlp1, Dl, rho, z, w, uz, uw){ # A matrix A &lt;- 1/N * Sigma_inv # B matrix sum_resp &lt;- sapply(resp, sum) #B &lt;- rho*(t(Dl)%*%Dl + e_mat)%*% diag(1/unlist(sum_resp)) B &lt;- rho*(Dlsqrd + e_mat) B &lt;- B/sum_resp[col(B)] #B &lt;- rho*(Dlsqrd + e_mat) %*% diag(1/unlist(sum_resp)) return(list(A = A, B = B)) } get_C_mat &lt;- function(C1, resp_sum, TT, dimdat, Sigma_inv, e_mat, N, Dlp1, Dl, rho, z, w, uz, uw, l){ C2 &lt;- t(uz - rho*z) # averaging C2 &lt;- C2 - rowMeans(C2) # third component C3 &lt;- do.call(rbind, lapply(1:dimdat, FUN = function(j){ ((uw[,j, drop=TRUE] - rho * w[,j,drop=TRUE]) %*% Dl) %&gt;% as.numeric() })) # combining C &lt;- (-1/N * C1 + C2 + C3) C &lt;- C/resp_sum[col(C)] return(C) } #&#39; @param mat Matrix to Schur-decompose. myschur &lt;- function(mat){ stopifnot(nrow(mat) == ncol(mat)) if(is.numeric(mat) &amp; length(mat)==1) mat = mat %&gt;% as.matrix() obj = Matrix::Schur(mat) obj$tQ = t(obj$Q) obj$orig = mat return(obj) } Here’s a function to check convergence of the ADMM with a fixed step size. #&#39; Check convergence of ADMM with a fixed step size (rho). check_converge &lt;- function(mu, rho, w, z, w_prev, z_prev, uw, uz, Dl, tDl, err_rel = 1E-4, err_abs = 0 ){ ## Constraints are: Ax + Bz =c, where x = mu, z =(w, z) prim1 = rbind(Dl %*% mu, mu - colMeans(mu)) prim2 = rbind(-w, -z) primal_resid = prim1 + prim2 ## Ax + Bz - c change_z = z - z_prev change_w = w - w_prev dual_resid = rho * (-(change_z - colMeans(change_z)) - tDl %*% change_w) tAU = (uz - colMeans(uz)) + tDl %*% uw ## Form primal and dual tolerances. primal_err = sqrt(length(primal_resid)) * err_abs + err_rel * max(norm(prim1, &quot;F&quot;), norm(prim2, &quot;F&quot;)) dual_err = sqrt(length(dual_resid)) * err_abs + err_rel * norm(tAU, &quot;F&quot;) ## Check convergence. primal_resid_size = norm(primal_resid, &quot;F&quot;) dual_resid_size = norm(dual_resid, &quot;F&quot;) primal_converge = ( primal_resid_size &lt;= primal_err ) dual_converge = ( dual_resid_size &lt;= dual_err ) ## Some checks (trying to address problems with |converge|). assertthat::assert_that(is.numeric(primal_resid_size)) assertthat::assert_that(is.numeric(primal_err)) assertthat::assert_that(is.numeric(dual_resid_size)) assertthat::assert_that(is.numeric(dual_err)) ## return(primal_converge &amp; dual_converge) converge = primal_converge &amp; dual_converge return(list( primal_resid1 = prim1, primal_resid2 = prim2, primal_resid = primal_resid, primal_err = primal_err, dual_resid = dual_resid, dual_err = dual_err, converge = converge)) } Here’s a function to compute the augmented Lagrangian of the M-step (this is not used for now). #&#39; computes the Augmented lagrangian. aug_lagr &lt;- function(y, TT, d, z, w, l, uz, uw, mu, resp, Sigma_inv, Dlp1, Dl, maxdev, lambda, rho, N){ mu_dd &lt;- rowMeans(mu) # Check the Z&#39;s for ball constraint, up to a tolerance of 1e-4 znorms &lt;- apply(z, FUN = function(zz) sqrt(sum(zz^2)), MARGIN = 1) if(any(is.na(znorms))) browser() if(any(znorms &gt; (maxdev + 1e-4))){ warning(&quot;||z||_2 &gt; maxdev, current iterate not feasible.&quot;) return(Inf) } aug1 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(t){ multmat &lt;- apply(y[[t]], FUN = function(yy){ t(yy - mu[,t]) %*% Sigma_inv %*% (yy - mu[,t])}, MARGIN = 1) sum(1/(2*N) * resp[[t]]*multmat) }))) aug2 &lt;- lambda*sum(do.call(rbind, lapply(1:d, FUN = function(j) sum(abs(diff(w[j,], differences = 1)))))) aug3 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(t){ uz[t,]%*%(mu[,t] - mu_dd - z[t,]) + rho/2 * sum((mu[,t] - mu_dd - z[t,])^2) }))) aug4 &lt;- sum(do.call(rbind, lapply(1:d, FUN = function(j){ uw[,j] %*% (Dl %*% mu[j,] - w[j,]) + rho/2 * sum((Dl %*% mu[j,] - w[j,])^2) }))) total &lt;- aug1 + aug2 + aug3 + aug4 return(total) } Here’s a function to calculate the objective value for the ADMM. \\[\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^T \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l+1)}\\mu_{j\\cdot }\\|_1\\] (This is the penalized surrogate objective \\(Q_{\\tilde \\theta}(\\mu, \\Sigma, \\pi) + \\lambda \\sum_{j=1}^d \\|D^{(l+1)} \\mu_{j \\cdot}\\|_1\\), taking only the parts, and leaving out a constant \\(C=-\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2} \\log det(\\Sigma_k)\\), since the constant \\(C\\) doesn’t change over ADMM iterations.) #&#39; computes the ADMM objective for one cluster objective_per_cluster &lt;- function(y, TT, d, l, mu, resp, Sigma_inv, Dlp1, Dl, maxdev, lambda, rho, N){ aug1 &lt;- sum(do.call(cbind, lapply(1:TT, FUN = function(tt){ multmat &lt;- apply(y[[tt]], FUN = function(yy){ t(yy - mu[tt,]) %*% Sigma_inv %*% (yy - mu[tt,])}, MARGIN = 1) sum(1/(2*N) * resp[[tt]] * multmat) }))) aug2 &lt;- lambda*sum(do.call(rbind, lapply(1:d, FUN = function(j) sum(abs(diff(mu[,j], differences = l)))))) total &lt;- aug1 + aug2 return(total) } 8.5 All Rcpp functions The main function we write in Rcpp is the “barebones” Sylvester equation solver that takes upper-triangular coefficient matrices. // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::depends(RcppEigen)]] #include &lt;RcppArmadillo.h&gt; #include &lt;RcppEigen.h&gt; #include &lt;numeric&gt; using namespace arma; using namespace Eigen; using Eigen::Map; // &#39;maps&#39; rather than copies using Eigen::MatrixXd; // variable size matrix, double precision //&#39; Solve &quot;barebones&quot; sylvester equation that takes upper triangular matrices as coefficients. //&#39; //&#39; @param TA Upper-triangular matrix //&#39; @param TB Upper-triangular matrix //&#39; @param C matrix //&#39; @export // [[Rcpp::export]] Eigen::MatrixXd matrix_function_solve_triangular_sylvester_barebonesC2(const Eigen::MatrixXd &amp; TA, const Eigen::MatrixXd &amp; TB, const Eigen::MatrixXd &amp; C){ // Eigen::eigen_assert(TA.rows() == TA.cols()); // Eigen::eigen_assert(TA.Eigen::isUpperTriangular()); // Eigen::eigen_assert(TB.rows() == TB.cols()); // Eigen::eigen_assert(TB.Eigen::isUpperTriangular()); // Eigen::eigen_assert(C.rows() == TA.rows()); // Eigen::eigen_assert(C.cols() == TB.rows()); // typedef typename MatrixType::Index Index; // typedef typename MatrixType::Scalar Scalar; int m = TA.rows(); int n = TB.rows(); Eigen::MatrixXd X(m, n); for (int i = m - 1; i &gt;= 0; --i) { for (int j = 0; j &lt; n; ++j) { // Compute T_A X = \\sum_{k=i+1}^m T_A_{ik} X_{kj} double TAX; if (i == m - 1) { TAX = 0; } else { MatrixXd TAXmatrix = TA.row(i).tail(m-1-i) * X.col(j).tail(m-1-i); TAX = TAXmatrix(0,0); } // Compute X T_B = \\sum_{k=1}^{j-1} X_{ik} T_B_{kj} double XTB; if (j == 0) { XTB = 0; } else { MatrixXd XTBmatrix = X.row(i).head(j) * TB.col(j).head(j); XTB = XTBmatrix(0,0); } X(i,j) = (C(i,j) - TAX - XTB) / (TA(i,i) + TB(j,j)); } } return X; } Also, we define a faster dmvnorm function written in C++. // [[Rcpp::depends(RcppArmadillo)]] #include &lt;RcppArmadillo.h&gt; static double const log2pi = std::log(2.0 * M_PI); /* C++ version of the dtrmv BLAS function */ void inplace_tri_mat_mult(arma::rowvec &amp;x, arma::mat const &amp;trimat){ arma::uword const n = trimat.n_cols; for(unsigned j = n; j-- &gt; 0;){ double tmp(0.); for(unsigned i = 0; i &lt;= j; ++i) tmp += trimat.at(i, j) * x[i]; x[j] = tmp; } } // [[Rcpp::export]] arma::vec dmvnorm_arma_fast(arma::mat const &amp;x, arma::rowvec const &amp;mean, arma::mat const &amp;sigma, bool const logd = false) { using arma::uword; uword const n = x.n_rows, xdim = x.n_cols; arma::vec out(n); arma::mat const rooti = arma::inv(trimatu(arma::chol(sigma))); double const rootisum = arma::sum(log(rooti.diag())), constants = -(double)xdim/2.0 * log2pi, other_terms = rootisum + constants; arma::rowvec z; for (uword i = 0; i &lt; n; i++) { z = (x.row(i) - mean); inplace_tri_mat_mult(z, rooti); out(i) = other_terms - 0.5 * arma::dot(z, z); } if (logd) return out; return exp(out); } // All credit goes to https://gallery.rcpp.org/articles/dmvnorm_arma/ We need this blob (from https://github.com/jacobbien/litr-project/blob/main/examples/make-an-r-package-with-armadillo/create-witharmadillo.Rmd ): usethis::use_rcpp_armadillo(name = &quot;code&quot;) usethis::use_rcpp_eigen(name = &quot;code&quot;) ## ✔ Leaving &#39;src/code.cpp&#39; unchanged ## • Edit &#39;src/code.cpp&#39; ## ✔ Adding &#39;RcppArmadillo&#39; to LinkingTo field in DESCRIPTION ## ✔ Created &#39;src/Makevars&#39; and &#39;src/Makevars.win&#39; with requested compilation settings. ## ✔ Leaving &#39;src/code.cpp&#39; unchanged ## • Edit &#39;src/code.cpp&#39; ## ✔ Adding &#39;RcppEigen&#39; to LinkingTo field in DESCRIPTION ## ✔ Adding &#39;@import RcppEigen&#39; to &#39;R/flowtrend-package.R&#39; ## • Run `devtools::document()` to update &#39;NAMESPACE&#39; #&#39; Testing against \\code{Mstep_mu()}, for ONE cluster. #&#39; @param ylist #&#39; @param resp #&#39; @param lambda #&#39; @param l #&#39; @param Sigma_inv inverse of Sigma #&#39; @param x covariates Mstep_mu_cvxr &lt;- function(ylist, resp, lambda, l, Sigma_inv, x = NULL, thresh = 1E-8, maxdev = NULL, dimdat, N, ecos_thresh = 1E-8, scs_eps = 1E-5){ ## Define dimensions TT = length(ylist) ## Responsibility Weighted Data ytildes &lt;- lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- resp[[tt]] yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) yrow = matrix(c(NA, NA), nrow=1, ncol=dimdat) yrow[1,] = colSums(yy) yrow }) %&gt;% do.call(rbind, .) ## Auxiliary term, needed to make the objective interpretable aux.y &lt;- Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt){ yy &lt;- ylist[[tt]] g &lt;- sqrt(resp[[tt]]) yy &lt;- apply(yy, MARGIN = 2, FUN = function(x) x * g) sum(diag(yy %*% Sigma_inv %*% t(yy))) })) ## Mu, d x T matrix mumat &lt;- CVXR::Variable(cols=dimdat, rows=TT) ## Summed sqrt responsibilities - needed in the objective. resp.sum.sqrt &lt;- lapply(resp, FUN = function(x) sqrt(sum(x))) ## Differencing Matrix, (TT-(l+1)) x TT Dlp1 &lt;- gen_diff_mat(n = TT, l = l+1, x = x) # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso ## Forming the objective obj = 1/(2*N) *( Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) CVXR::quad_form(t(resp.sum.sqrt[[tt]]*mumat[tt,]), Sigma_inv))) -2 * Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% t(mumat[tt,]))) + aux.y) + lambda * sum(CVXR::sum_entries(abs(Dlp1 %*% mumat), axis = 1)) ##Reduce(&quot;+&quot;, lapply(1:TT, FUN = function(tt) t(ytildes[tt,]) %*% Sigma_inv %*% (mumat[tt,]))) + aux.y ## a = t(resp.sum.sqrt[[tt]]*mumat[tt,]) ## CVXR::quad_form(resp.sum.sqrt[[tt]]*mumat[tt,], Sigma_inv) ## resp.sum.sqrt[[tt]]*mumat[tt,] %&gt;% dim() ## dim(Sigma_inv) ## mumat %&gt;% dim() ## ( (resp.sum.sqrt[[tt]]) * mumat[tt,]) %&gt;% dim() ## Putting together the ball constraint rowmns &lt;- matrix(rep(1, TT^2), nrow = TT)/TT mu_dotdot &lt;- rowmns %*% mumat constraints = list() if(!is.null(maxdev)){ constraints = list(CVXR::sum_entries(CVXR::square(mumat - mu_dotdot), axis = 2) &lt;= rep(maxdev^2, TT) ) } ## Try all two CVXR solvers. prob &lt;- CVXR::Problem(CVXR::Minimize(obj), constraints) result = NULL result &lt;- tryCatch({ CVXR::solve(prob, solver=&quot;ECOS&quot;, FEASTOL = ecos_thresh, RELTOL = ecos_thresh, ABSTOL = ecos_thresh) }, error=function(err){ err$message = paste(err$message, &quot;\\n&quot;, &quot;Lasso solver using ECOS has failed.&quot; ,sep=&quot;&quot;) cat(err$message, fill=TRUE) return(NULL) }) ## If anything is wrong, flag to use SCS solver scs = FALSE if(is.null(result)){ scs = TRUE } else { if(result$status != &quot;optimal&quot;) scs = TRUE } ## Use the SCS solver if(scs){ result = CVXR::solve(prob, solver=&quot;SCS&quot;, eps = scs_eps) if(any(is.na(result$getValue(mumat)))){ ## A clumsy way to check. stop(&quot;Lasso solver using both ECOS and SCS has failed.&quot;, sep=&quot;&quot;) } } ## Record Interesting Parameters num_iters &lt;- result$num_iters status &lt;- result$status mumat &lt;- result$getValue(mumat) val &lt;- result$value return(list(mu = mumat, value = val, status = status, num_iters = num_iters)) } This function solves the following problem: \\[ \\begin{align*} &amp;\\text{minimize}_{\\mu} {\\frac{1}{2N} \\sum_{t=1}^T \\sum_{i=1}^{n_t} \\hat{\\gamma}_{it} (y_i^{(t)} - \\mu_{\\cdot t})^\\top \\hat{\\Sigma}^{-1} ( y_i^{(t)} - \\mu_{\\cdot t}) + \\lambda \\sum_{j=1}^d \\|D^{(l)}\\mu_{j\\cdot }\\|_1}\\\\ &amp;\\text{subject to}\\;\\; {\\| \\mu_{\\cdot t} - \\bar{\\mu}_{\\cdot \\cdot}\\|_2 \\le r \\;\\;\\forall t=1,\\cdots, T, } \\end{align*} \\] and is directly equivalent to Mstep_mu(). The resulting solution and the objective value should be the same. Let’s check that. First, set up some objects to run Mstep_mu() and Mstep_mu_cvxr(). numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .01 l = 1 x = 1:TT Dlp1 = gen_diff_mat(n = TT, l = l+1, x = x) Dl = gen_diff_mat(n = TT, l = l, x = x) Dlsqrd &lt;- t(Dl) %*% Dl maxdev = NULL Then, compare the result of the two implementations. They should look identical. ## overall ADMM res1 = Mstep_mu(resp, ylist, lambda, l=l, sigma=sigma, Dlsqrd = Dlsqrd, Dl=Dl, Dlp1=Dlp1, TT=TT, N=N, dimdat=dimdat, e_mat=etilde_mat(TT = TT), maxdev = maxdev) mn1 = res1$mns ## CVXR just ONE cluster res2list = lapply(1:numclust, function(iclust){ Sigma_inv_oneclust = solve(sigma[iclust,,]) resp_oneclist = lapply(resp, function(resp_onetime){resp_onetime[,iclust, drop=FALSE]}) N = sum(unlist(resp)) res2 = Mstep_mu_cvxr(ylist, resp_oneclist, lambda, l, Sigma_inv_oneclust, thresh = 1E-8, maxdev = maxdev, dimdat, N) res2$mu }) mn2 = array(NA, dim=c(100, dimdat, 3)) for(iclust in 1:numclust){ mn2[,,iclust] = res2list[[iclust]] %&gt;% as.matrix() } ## Plot them plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm (solid) vs cvxr (dashed)&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn1[,1,] %&gt;% matlines(lwd = 1, lty = 1) mn2[,1,] %&gt;% matlines(lwd = 3, lty = 3) Next, do this for uneven inputs. ## Setup numclust = 3 TT = 100 dimdat = 1 set.seed(0) dt = gendat_1d(TT = TT, ntlist = rep(TT, 100)) ylist = dt %&gt;% dt2ylist() ## Subset them ylist = ylist[-seq(from=10,to=100,by=10)] x = (1:100)[-seq(from=10,to=100,by=10)] sigma = init_sigma(ylist, numclust) ## (T x numclust x (dimdat x dimdat)) mn = init_mn(ylist, numclust, TT, dimdat)##, countslist = countslist) prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. resp = Estep(mn, sigma, prob, ylist = ylist, numclust) lambda = .1 l = 2 ##x = 1:TT Dlp1 = gen_diff_mat(n = length(x), l = l+1, x = x) Dl = gen_diff_mat(n = length(x), l = l, x = x) Dlsqrd &lt;- t(Dl) %*% Dl maxdev = NULL ## Try the algorithm itelf. set.seed(100) obj = flowtrend_once(ylist = ylist, x = x, lambda = .1, lambda_prob = .1, l = 2, l_prob = 2, maxdev = 5, numclust = 3, rho_init = 0.01, verbose = TRUE) plot_1d(ylist=ylist, obj=obj) plot(obj$objectives, type =&#39;l&#39;) mn1 = obj$mn obj$mn[,1,] %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% matplot(type=&#39;l&#39;) mn[,1,] %&gt;% diff() %&gt;% diff() %&gt;% diff() %&gt;% matplot(type=&#39;l&#39;) mn[,1,] %&gt;% matplot(type=&#39;l&#39;) ## Okay, so cluster 3 has a very irregular mean. plot(x = dt$time, y=dt$Y, col = rgb(0,0,0,0.04), main = &#39;admm (solid) vs cvxr (dashed)&#39;, ylab = &quot;&quot;, xlab = &quot;time&quot;) mn_old[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 1) mn_less_old[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 2) mn[,1,] %&gt;% matlines(lwd = 1, x=x, lty = 3) TODO: We just need to bundle this into testthat style tests, and make sure to test several lambda values. TODO: maybe do this for unevenly spaced inputs. CVXR needs to take a different D matrix. testthat::test_that(&quot;Test the M step of \\mu against CVXR&quot;, {}) "],["flowtrend.html", "9 flowtrend", " 9 flowtrend Now we’ve assembled all ingredients we need, we’ll build the main function flowtrend_once() to estimate a flowtrend model. Here goes: #&#39; Estimate flowtrend model once. #&#39; #&#39; @param ylist Data. #&#39; @param countslist Counts corresponding to multiplicities. #&#39; @param x Times, if points are not evenly spaced. Defaults to NULL, in which #&#39; case the value becomes \\code{1:T}, for the $T==length(ylist)$. #&#39; @param numclust Number of clusters. #&#39; @param niter Maximum number of EM iterations. #&#39; @param l Degree of differencing for the mean trend filtering. l=0 will give #&#39; you piecewise constant means; l=1 is piecewise linear, and so forth. #&#39; @param l_prob Degree of differencing for the probability trend filtering. #&#39; @param mn Initial value for cluster means. Defaults to NULL, in which case #&#39; initial values are randomly chosen from the data. #&#39; @param lambda Smoothing parameter for means #&#39; @param lambda_prob Smoothing parameter for probabilities #&#39; @param verbose Loud or not? EM iteration progress is printed. #&#39; @param tol_em Relative numerical improvement of the objective value at which #&#39; to stop the EM algorithm #&#39; @param maxdev Maximum deviation of cluster means across time.. #&#39; @param countslist_overwrite #&#39; @param admm_err_rel #&#39; @param admm_err_abs #&#39; @param admm_local_adapt #&#39; @param admm_local_adapt_niter #&#39; #&#39; @return List object with flowtrend model estimates. #&#39; @export #&#39; #&#39; @examples flowtrend_once &lt;- function(ylist, countslist = NULL, x = NULL, numclust, niter = 1000, l, l_prob = NULL, mn = NULL, lambda = 0, lambda_prob = NULL, verbose = FALSE, tol_em = 1E-4, maxdev = NULL, countslist_overwrite = NULL, ## beta Mstep (ADMM) settings admm = TRUE, admm_err_rel = 1E-3, admm_err_abs = 1E-4, ## Mean M step (Locally Adaptive ADMM) settings admm_local_adapt = TRUE, admm_local_adapt_niter = if(admm_local_adapt) 10 else 1, rho_init = 0.1, ## Other options check_convergence = TRUE, ## Random seed seed = NULL){ ## Basic checks if(!is.null(maxdev)){ assertthat::assert_that(maxdev!=0) } else { maxdev = 1E10 } assertthat::assert_that(numclust &gt; 1) assertthat::assert_that(niter &gt; 1) if(is.null(countslist)){ ntlist = sapply(ylist, nrow) countslist = lapply(ntlist, FUN = function(nt) rep(1, nt)) } if(!is.null(seed)){ assertthat::assert_that(all((seed %&gt;% sapply(., class)) == &quot;integer&quot;)) assertthat::assert_that(length(seed) == 7) } ## Setup for EM algorithm TT = length(ylist) dimdat = ncol(ylist[[1]]) if(is.null(x)) x &lt;- 1:TT if(is.unsorted(x)) stop(&quot;x must be ordered!&quot;) # l = 2 is quadratic trend filtering # l = 1 is linear trend filtering # l = 0 is fused lasso # D^{(1)} is first differences, so it correponds to l=0 Dlp1 = gen_diff_mat(n = TT, l = l+1, x = x) if(l &gt; 1){ facmat = diag(l / diff(x, lag = l)) } else { facmat = diag(rep(1, TT-l)) } Dl = facmat %*% gen_diff_mat(n = TT, l = l, x = x) tDl = t(Dl) Dlsqrd &lt;- t(Dl) %*% Dl e_mat &lt;- etilde_mat(TT = TT) # needed to generate B Dlp1_prob = gen_diff_mat(n = TT, l = l_prob+1, x = x) H_tf &lt;- gen_tf_mat(n = TT, k = l_prob, x = x) if(is.null(mn)){ mn = init_mn(ylist, numclust, TT, dimdat, countslist = countslist, seed = seed) } ntlist = sapply(ylist, nrow) N = sum(ntlist) ## Initialize some objects prob = matrix(1/numclust, nrow = TT, ncol = numclust) ## Initialize to all 1/K. denslist_by_clust &lt;- NULL objectives = c(+1E20, rep(NA, niter-1)) sigma_fac &lt;- diff(range(do.call(rbind, ylist)))/8 sigma = init_sigma(ylist, numclust, sigma_fac) ## (T x numclust x (dimdat x dimdat)) sigma_eig_by_clust = NULL zero.betas = zero.alphas = list() ## The least elegant solution I can think of.. used only for blocked cv if(!is.null(countslist_overwrite)) countslist = countslist_overwrite #if(!is.null(countslist)) check_trim(ylist, countslist) vals &lt;- vector(length = niter) latest_rho = NA start.time = Sys.time() for(iter in 2:niter){ if(verbose){ print_progress(iter-1, niter-1, &quot;EM iterations.&quot;, start.time = start.time, fill = FALSE) } resp &lt;- Estep(mn, sigma, prob, ylist = ylist, numclust = numclust, denslist_by_clust = denslist_by_clust, first_iter = (iter == 2), countslist = countslist) ## M step (three parts) ## 1. Means res_mu = Mstep_mu(resp, ylist, lambda = lambda, first_iter = (iter == 2), l = l, Dlp1 = Dlp1, Dl = Dl, tDl = tDl, Dlsqrd = Dlsqrd, sigma_eig_by_clust = sigma_eig_by_clust, sigma = sigma, maxdev = maxdev, e_mat = e_mat, Zs = NULL, Ws = NULL, uws = NULL, uzs = NULL, x = x, err_rel = admm_err_rel, err_abs = admm_err_abs, local_adapt = admm_local_adapt, local_adapt_niter = admm_local_adapt_niter, rho_init = rho_init, iter = iter) ## rho_init = (if(iter == 2) rho_init else latest_rho)) ## latest_rho = res_mu$rho mn = res_mu$mns ## 2. Sigma sigma = Mstep_sigma(resp, ylist, mn, numclust) ## 3. Probabilities prob_link = Mstep_prob(resp, countslist = countslist, H_tf = H_tf, lambda_prob = lambda_prob, l_prob = l_prob, x = x) prob = softmax(prob_link) objectives[iter] = objective(ylist = ylist, mu = mn, sigma = sigma, prob = prob, prob_link = prob_link, lambda = lambda, Dlp1 = Dlp1, l = l, countslist = countslist, Dlp1_prob = Dlp1_prob, l_prob = l_prob, lambda_prob = lambda_prob) ## Check convergence if(iter &gt; 10){ if(check_convergence &amp; check_converge_rel(objectives[iter-1], objectives[iter], tol = tol_em) &amp; check_converge_rel(objectives[iter-2], objectives[iter-1], tol = tol_em)&amp; check_converge_rel(objectives[iter-3], objectives[iter-2], tol = tol_em)){ ## check_converge_rel(objectives[iter-4], objectives[iter-3], tol = tol_em)){ break } } } return(structure(list(mn = mn, prob = prob, prob_link = prob_link, sigma = sigma, objectives = objectives[2:iter], final.iter = iter, resp = resp, ## Above is output, below are data/algorithm settings. dimdat = dimdat, TT = TT, N = N, l = l, x = x, numclust = numclust, lambda = lambda, lambda_prob = lambda_prob, maxdev = maxdev, niter = niter ), class = &quot;flowtrend&quot;)) } Next, flowtrend() is the main user-facing function. #&#39; Main function. Repeats the EM algorithm (\\code{flowtrend_once()}) with |nrep| restarts (5 by default). #&#39; #&#39; @param ... : arguments for \\code{flowtrend_once()} #&#39; @param nrestart : number of random restarts #&#39; #&#39; @return #&#39; @export #&#39; #&#39; @examples flowtrend &lt;- function(..., nrestart = 5){ args = list(...) if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(&quot;EM will restart&quot;, nrestart, &quot;times&quot;, fill=TRUE) } } out_models &lt;- lapply(1:nrestart, FUN = function(irestart){ if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(&quot;EM restart:&quot;, irestart, fill=TRUE) } } model_temp &lt;- flowtrend_once(...) model_obj &lt;- tail(model_temp$objectives, n = 1) if(&quot;verbose&quot; %in% names(args)){ if(args$verbose){ cat(fill=TRUE) } } return(list(model = model_temp, final_objective = model_obj)) }) final_objectives &lt;- sapply(out_models, FUN = function(x) x$final_objective) best_model &lt;- which.min(final_objectives) final_model = out_models[[best_model]][[&quot;model&quot;]] ## Add the objectives final_model$all_objectives = lapply(1:nrestart, function(irestart){ one_model = out_models[[irestart]] data.frame(objective=one_model$model$objectives) %&gt;% mutate(iter=row_number(), irestart=irestart) %&gt;% select(irestart, iter, objective) }) %&gt;% bind_rows() return(final_model) } "],["reordering-clusters.html", "10 Reordering clusters", " 10 Reordering clusters It’s useful to be able to reorder, or permute, one model’s cluster labels (cluster 1,2,.. of newres which are arbitrary) to that of another model origres. The function reorder_kl() does this by (1) taking the posterior probabilities of the particles in ylist_particle (row-binded to be a \\(\\sum_t n_t \\times K\\) matrix), and then (2) using a Hungarian algorithm [@kuhn-hungarian] to best match the two elongated matrices \\(A\\) and \\(B\\) by measuring the (symmetric? TODO check) KL divergence between all permutations of the \\(K\\) columns (TODO: This is currently just copy-pasted from flowmix. But actually, this is an example of a function that can be directly borrowed; all the work is done in reorder_clust(). So we’ll deal with that!) #&#39; Reorder the cluster numbers for a new flowtrend object \\code{newres}; the best #&#39; permutation (reordering) is to match the original flowmix object #&#39; \\code{origres}. #&#39; #&#39; @param newres New flowtrend object to reorder. #&#39; @param origres Original flowtrend object. #&#39; @param ylist_particle The particle-level data. #&#39; @param fac Defaults to 100, to take 1/100&#39;th of the particles from each time point. #&#39; @param verbose Loud or not? #&#39; #&#39; @return Reordered res #&#39; #&#39; @export reorder_kl &lt;- function(newres, origres, ylist_particle, fac = 100, verbose = FALSE){ ## Randomly sample 1/100 of the original particles (mainly for memory reasons) TT = length(ylist_particle) N = sapply(ylist_particle, nrow) %&gt;% sum() ntlist = sapply(ylist_particle, nrow) indlist = lapply(1:TT, function(tt){ nt = ntlist[[tt]] ind = sample(1:nt, round(nt / fac), replace=FALSE) }) ## Sample responsibilities ylist_particle_small = Map(function(ind, y){ y[ind,,drop = FALSE] }, indlist, ylist_particle) ## Calculate new responsibilities resp_orig_small &lt;- Estep(origres$mn, origres$sigma, origres$prob, ylist = ylist_particle_small, numclust = origres$numclust, first_iter = TRUE) resp_new_small &lt;- Estep(newres$mn, newres$sigma, newres$prob, ylist = ylist_particle_small, numclust = newres$numclust, first_iter = TRUE) assertthat::assert_that(all(sapply(resp_orig_small, dim) == sapply(resp_new_small, dim))) ## Get best ordering (using symm. KL divergence and Hungarian algorithm for ## matching) best_ord &lt;- get_best_match_from_kl(resp_new_small, resp_orig_small) if(verbose) cat(&quot;New order is&quot;, best_ord, fill=TRUE) newres_reordered_kl = newres %&gt;% reorder_clust(ord = best_ord) ## Return the reordered object return(newres_reordered_kl) } This function uses get_best_match_from_kl(), which takes two lists containing responsibilities (posterior probabilities of particles) – one from each model – and returns the cluster ordering to apply to the model that produced resp_new. We define this function and a couple of helper functions next. #&#39; Compute KL divergence from responsibilities between two models&#39; #&#39; responsibilities \\code{resp_new} and \\code{resp_old}. #&#39; #&#39; @param resp_new New responsibilities #&#39; @param resp_orig Original responsiblities. #&#39; #&#39; @return Calculate reordering \\code{o} of the clusters in model represented #&#39; by \\code{resp_new}. To be clear, \\code{o[i]} of new model is the best #&#39; match with the i&#39;th cluster of the original model. #&#39; #&#39; @export #&#39; @importFrom clue solve_LSAP get_best_match_from_kl &lt;- function(resp_new, resp_orig){ ## Basic checks . = NULL ## Fixing check() assertthat::assert_that(all(sapply(resp_new, dim) == sapply(resp_orig , dim))) ## Row-bind all the responsibilities to make a long matrix distmat = form_symmetric_kl_distmat(resp_orig %&gt;% do.call(rbind,.), resp_new %&gt;% do.call(rbind,.)) ## Use Hungarian algorithm to solve. fit &lt;- clue::solve_LSAP(distmat) o &lt;- as.numeric(fit) ## Return the ordering return(o) } ##&#39; From two probability matrices, form a (K x K) distance matrix of the ##&#39; (n)-vectors. The distance between the vectors is the symmetric KL ##&#39; divergence. ##&#39; ##&#39; @param mat1 Matrix 1 of size (n x K). ##&#39; @param mat2 Matrix 2 of size (n x K). ##&#39; ##&#39; @return K x K matrix containing symmetric KL divergence of each column of ##&#39; \\code{mat1} and \\code{mat2}. form_symmetric_kl_distmat &lt;- function(mat1, mat2){ ## Manually add some small, in case some columns are all zero mat1 = (mat1 + 1E-10) %&gt;% pmin(1) mat2 = (mat2 + 1E-10) %&gt;% pmin(1) ## Calculate and return distance matrix. KK1 = ncol(mat1) KK2 = ncol(mat2) distmat = matrix(NA, ncol=KK2, nrow=KK1) for(kk1 in 1:KK1){ for(kk2 in 1:KK2){ mydist = symmetric_kl(mat1[,kk1, drop=TRUE], mat2[,kk2, drop=TRUE]) distmat[kk1, kk2] = mydist } } stopifnot(all(!is.na(distmat))) return(distmat) } ##&#39; Symmetric KL divergence, of two probability vectors. ##&#39; ##&#39; @param vec1 First probability vector. ##&#39; @param vec2 Second prbability vector. ##&#39; ##&#39; @return Symmetric KL divergence (scalar). symmetric_kl &lt;- function(vec1, vec2){ stopifnot(all(vec1 &lt;= 1) &amp; all(vec1 &gt;= 0)) stopifnot(all(vec2 &lt;= 1) &amp; all(vec2 &gt;= 0)) kl &lt;- function(vec1, vec2){ sum(vec1 * log(vec1 / vec2)) } return((kl(vec1, vec2) + kl(vec2, vec1))/2) } Finally, the function that actually performs the manual reordering the clusters of an estimated model obj is reorder_clust(). #&#39; Reorder the results of one object so that cluster 1 through #&#39; \\code{numclust} is in a particular order. The default is decreasing order of #&#39; the averages (over time) of the cluster means. #&#39; #&#39; @param res Model object. #&#39; @param ord Defaults to NULL. Use if you have an ordering in mind. #&#39; #&#39; @return Same object, but with clusters reordered. #&#39; #&#39; @export reorder_clust &lt;- function(res, ord = NULL){ ## Find an order by sums (averages) if(is.null(ord)) ord = res$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing = TRUE) if(!is.null(ord)) all(sort(ord) == 1:res$numclust) ## Reorder mean res$mn = res$mn[,,ord, drop=FALSE] ## Reorder sigma res$sigma = res$sigma[ord,,,drop=FALSE] ## Reorder prob res$prob = res$prob[,ord, drop=FALSE] ## Reorder the responsibilities if(&#39;resp&#39; %in% res){ resp_temp = list() for(tt in 1:TT){ rep_temp[[tt]] = res$resp[[tt]][,ord] } } return(res) } Here’s an example of how to use this. devtools::load_all(&quot;~/repos/FlowTF&quot;) set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45) dt_model &lt;- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model twice. set.seed(2) objlist &lt;- lapply(1:2, function(isim){ flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, nrestart = 1, verbose = TRUE)}) ## Perform the reordering, make three plots newres = objlist[[1]] origres = objlist[[2]] newres_reordered = reorder_kl(newres, origres, ylist, fac = 100, verbose = FALSE) plot_1d(ylist, newres, x = x, add_point = FALSE) + ggtitle(&quot;before reordering, model 1&quot;) plot_1d(ylist, origres, x = x, add_point = FALSE) + ggtitle(&quot;model 2&quot;) plot_1d(ylist, newres_reordered, x = x, add_point = FALSE) + ggtitle(&quot;reordered model 1&quot;) "],["tuning-the-regularization-parameters-for-flowtrend.html", "11 Tuning the regularization parameters for flowtrend 11.1 Predicting and evaluating on new time points 11.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test 11.3 Define CV data folds 11.4 CV = many single jobs 11.5 Running cross-validation 11.6 Summarizing the output 11.7 CV on your own computer", " 11 Tuning the regularization parameters for flowtrend We’re going to take a (huge) leap, and assume the flowtrend() function has been built. We need to build up quite a few functions before we’re able to do cross-validation. These include: Predicting out-of-sample, using predict_flowtrend(). Evaluating data fit (by likelihood) in an out-of-sample measurement, using objective(..., unpenalized = TRUE). Numerically estimating the maximum regularization values to test, using get_max_lambda(). Making data splits, using make_cv_folds(). 11.1 Predicting and evaluating on new time points First, let’s write a couple of functions interpolate_mn() and interpolate_prob() which linearly interpolate the means and probabilities at new time points. #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param mn length(x) by dimdat by numclust matrix. #&#39; #&#39; @return A dimdat-length vector. interpolate_mn &lt;- function(x, tt, iclust, mn){ ## Basic checks stopifnot(length(x) == dim(mn)[1]) stopifnot(iclust &lt;= dim(mn)[3]) if(tt %in% x) return(mn[which(x==tt),,iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation mn_t &lt;- mn[ceiling_t_ind,,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + mn[floor_t_ind,,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) ## Basic checks stopifnot(length(mn_t) == dim(mn)[2]) return(mn_t) } #&#39; Do a linear interpolation of the cluster means. #&#39; #&#39; @param x Training times. #&#39; @param tt Prediction time. #&#39; @param iclust Cluster number. #&#39; @param prob length(x) by numclust array or matrix. #&#39; #&#39; @return One probability. interpolate_prob &lt;- function(x, tt, iclust, prob){ ## Basic checks numdat = dim(prob)[1] numclust = dim(prob)[2] stopifnot(length(x) == numdat) stopifnot(iclust &lt;= numclust) if(tt %in% x) return(prob[which(x == tt),iclust,drop=TRUE]) ## Set up for linear interpolation floor_t &lt;- max(x[which(x &lt;= tt)]) ceiling_t &lt;- min(x[which(x &gt;= tt)]) floor_t_ind &lt;- which(x == floor_t) ceiling_t_ind &lt;- which(x == ceiling_t) ## Do the linear interpolation prob_t &lt;- prob[ceiling_t_ind,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) + prob[floor_t_ind,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t ## Basic checks stopifnot(length(prob_t) == 1) stopifnot(0 &lt;= prob_t &amp; prob_t &lt;= 1) return(prob_t) } Next, let’s build a prediction function predict_flowtrend() which takes the model object obj, and the new time points newtimes, and produces. #&#39; Prediction: Given new timepoints in the original time interval,generate a set #&#39; of means and probs (and return the same Sigma). #&#39; #&#39; @param obj Object returned from covariate EM flowtrend(). #&#39; @param newtimes New times at which to make predictions. #&#39; #&#39; @return List containing mean, prob, and sigma, and x. #&#39; #&#39; @export #&#39; predict_flowtrend &lt;- function(obj, newtimes = NULL){ ## Check the dimensions newx &lt;- newtimes if(is.null(newtimes)){ newx = obj$x } ## Check if the new times are within the time range of the original data stopifnot(all(sapply(newx, FUN = function(t) t &gt;= min(obj$x) &amp; t &lt;= max(obj$x)))) ## Setup some things x &lt;- obj$x TT_new = length(newx) numclust = obj$numclust dimdat = obj$dimdat ## Predict the means (manually). newmn_array = array(NA, dim = c(TT_new, dimdat, numclust)) for(iclust in 1:numclust){ newmn_oneclust &lt;- lapply(newx, function(tt){ interpolate_mn(x, tt, iclust, obj$mn) }) %&gt;% do.call(rbind, . ) newmn_array[,,iclust] = newmn_oneclust } ## Predict the probs. newprob = array(NA, dim = c(TT_new, numclust)) for(iclust in 1:numclust){ newprob_oneclust &lt;- lapply(newx, function(tt){ interpolate_prob(x, tt, iclust, obj$prob) }) %&gt;% do.call(c, .) newprob[,iclust] = newprob_oneclust } ## Basic checks stopifnot(all(dim(newprob) == c(TT_new,numclust))) stopifnot(all(newprob &gt;= 0)) stopifnot(all(newprob &lt;= 1)) ## Return the predictions return(list(mn = newmn_array, prob = newprob, sigma = obj$sigma, x = newx)) } Here’s a quick test (no new data) to make sure this function returns a list containing: the mean, probability, covariance, and new times. testthat::test_that(&quot;The prediction function returns the right things&quot;, { ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 1, niter = 3) predobj = predict_flowtrend(obj) testthat::expect_named(predobj, c(&quot;mn&quot;, &quot;prob&quot;, &quot;sigma&quot;, &quot;x&quot;)) }) Now, we try to make predictions at new held-out time points held_out=25:35, from a model that is estimated without those time points. ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() set.seed(686) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = 0.02, lambda_prob = .1, ## nrestart = 5, verbose = TRUE) obj$all_objectives %&gt;% mutate(irestart = as.factor(irestart)) %&gt;% ggplot() + geom_line(aes(x=iter, y=objective, group = irestart, col = irestart)) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster labels of the fitted model. obj = reorder_clust(obj) testthat::test_that(&quot;prediction function returns the right things&quot;, { predobj = predict_flowtrend(obj, newtimes = held_out) ## Check a few things testthat::expect_equal(predobj$x, held_out) testthat::expect_equal(rowSums(predobj$prob), rep(1, length(held_out))) testthat::expect_equal(dim(predobj$mn), c(length(held_out), 1, 3)) }) ## EM will restart 5 times ## EM restart: 1 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 21:59:53 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 21:59:53 EM iterations. 3 out of 999 with lapsed time 1 seconds and remaining time 332 seconds and will finish at 2024-02-08 22:05:26 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2024-02-08 22:04:03 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:03:13 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:02:40 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:02:16 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:01:59 EM iterations. 9 out of 999 with lapsed time 2 seconds and remaining time 220 seconds and will finish at 2024-02-08 22:03:35 EM iterations. 10 out of 999 with lapsed time 2 seconds and remaining time 198 seconds and will finish at 2024-02-08 22:03:13 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:02:55 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:02:39 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:02:27 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:02:16 EM iterations. 15 out of 999 with lapsed time 3 seconds and remaining time 197 seconds and will finish at 2024-02-08 22:03:13 EM iterations. 16 out of 999 with lapsed time 3 seconds and remaining time 184 seconds and will finish at 2024-02-08 22:03:00 EM iterations. 17 out of 999 with lapsed time 3 seconds and remaining time 173 seconds and will finish at 2024-02-08 22:02:49 EM iterations. 18 out of 999 with lapsed time 3 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:02:40 EM iterations. 19 out of 999 with lapsed time 3 seconds and remaining time 155 seconds and will finish at 2024-02-08 22:02:31 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2024-02-08 22:02:23 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2024-02-08 22:02:17 EM iterations. 22 out of 999 with lapsed time 4 seconds and remaining time 178 seconds and will finish at 2024-02-08 22:02:55 EM iterations. 23 out of 999 with lapsed time 4 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:02:47 EM iterations. 24 out of 999 with lapsed time 4 seconds and remaining time 162 seconds and will finish at 2024-02-08 22:02:39 EM iterations. 25 out of 999 with lapsed time 4 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:02:33 EM iterations. 26 out of 999 with lapsed time 4 seconds and remaining time 150 seconds and will finish at 2024-02-08 22:02:28 EM iterations. 27 out of 999 with lapsed time 5 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:02:58 EM iterations. 28 out of 999 with lapsed time 5 seconds and remaining time 173 seconds and will finish at 2024-02-08 22:02:51 EM iterations. 29 out of 999 with lapsed time 5 seconds and remaining time 167 seconds and will finish at 2024-02-08 22:02:45 EM iterations. 30 out of 999 with lapsed time 5 seconds and remaining time 162 seconds and will finish at 2024-02-08 22:02:40 EM iterations. 31 out of 999 with lapsed time 5 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:02:34 EM iterations. 32 out of 999 with lapsed time 6 seconds and remaining time 181 seconds and will finish at 2024-02-08 22:03:00 EM iterations. 33 out of 999 with lapsed time 6 seconds and remaining time 176 seconds and will finish at 2024-02-08 22:02:55 EM iterations. 34 out of 999 with lapsed time 6 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:02:49 ## EM restart: 2 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 21:59:59 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:00 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:00 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2024-02-08 22:04:09 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:03:19 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:02:46 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:02:22 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:02:04 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2024-02-08 22:01:51 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2024-02-08 22:01:40 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:03:01 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:02:45 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:02:33 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:02:22 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2024-02-08 22:02:12 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2024-02-08 22:02:04 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2024-02-08 22:01:58 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2024-02-08 22:01:51 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2024-02-08 22:01:45 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2024-02-08 22:02:29 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2024-02-08 22:02:22 EM iterations. 22 out of 999 with lapsed time 3 seconds and remaining time 133 seconds and will finish at 2024-02-08 22:02:15 EM iterations. 23 out of 999 with lapsed time 3 seconds and remaining time 127 seconds and will finish at 2024-02-08 22:02:09 EM iterations. 24 out of 999 with lapsed time 3 seconds and remaining time 122 seconds and will finish at 2024-02-08 22:02:04 EM iterations. 25 out of 999 with lapsed time 3 seconds and remaining time 117 seconds and will finish at 2024-02-08 22:01:59 EM iterations. 26 out of 999 with lapsed time 3 seconds and remaining time 112 seconds and will finish at 2024-02-08 22:01:54 EM iterations. 27 out of 999 with lapsed time 3 seconds and remaining time 108 seconds and will finish at 2024-02-08 22:01:51 EM iterations. 28 out of 999 with lapsed time 3 seconds and remaining time 104 seconds and will finish at 2024-02-08 22:01:47 EM iterations. 29 out of 999 with lapsed time 3 seconds and remaining time 100 seconds and will finish at 2024-02-08 22:01:43 EM iterations. 30 out of 999 with lapsed time 3 seconds and remaining time 97 seconds and will finish at 2024-02-08 22:01:40 ## EM restart: 3 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:03 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:03 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:03 EM iterations. 4 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:03 EM iterations. 5 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:03 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:02:49 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:02:26 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:02:08 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2024-02-08 22:01:54 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2024-02-08 22:01:43 EM iterations. 11 out of 999 with lapsed time 1 seconds and remaining time 90 seconds and will finish at 2024-02-08 22:01:34 EM iterations. 12 out of 999 with lapsed time 1 seconds and remaining time 82 seconds and will finish at 2024-02-08 22:01:26 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:02:36 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:02:26 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2024-02-08 22:02:16 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2024-02-08 22:02:08 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2024-02-08 22:02:01 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2024-02-08 22:01:54 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2024-02-08 22:01:48 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2024-02-08 22:02:32 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2024-02-08 22:02:26 EM iterations. 22 out of 999 with lapsed time 3 seconds and remaining time 133 seconds and will finish at 2024-02-08 22:02:19 EM iterations. 23 out of 999 with lapsed time 3 seconds and remaining time 127 seconds and will finish at 2024-02-08 22:02:13 EM iterations. 24 out of 999 with lapsed time 3 seconds and remaining time 122 seconds and will finish at 2024-02-08 22:02:08 EM iterations. 25 out of 999 with lapsed time 4 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:02:42 EM iterations. 26 out of 999 with lapsed time 4 seconds and remaining time 150 seconds and will finish at 2024-02-08 22:02:37 EM iterations. 27 out of 999 with lapsed time 4 seconds and remaining time 144 seconds and will finish at 2024-02-08 22:02:31 EM iterations. 28 out of 999 with lapsed time 4 seconds and remaining time 139 seconds and will finish at 2024-02-08 22:02:26 EM iterations. 29 out of 999 with lapsed time 5 seconds and remaining time 167 seconds and will finish at 2024-02-08 22:02:54 EM iterations. 30 out of 999 with lapsed time 5 seconds and remaining time 162 seconds and will finish at 2024-02-08 22:02:50 EM iterations. 31 out of 999 with lapsed time 5 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:02:44 EM iterations. 32 out of 999 with lapsed time 5 seconds and remaining time 151 seconds and will finish at 2024-02-08 22:02:39 EM iterations. 33 out of 999 with lapsed time 5 seconds and remaining time 146 seconds and will finish at 2024-02-08 22:02:34 EM iterations. 34 out of 999 with lapsed time 6 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:02:59 EM iterations. 35 out of 999 with lapsed time 6 seconds and remaining time 165 seconds and will finish at 2024-02-08 22:02:54 EM iterations. 36 out of 999 with lapsed time 6 seconds and remaining time 160 seconds and will finish at 2024-02-08 22:02:49 EM iterations. 37 out of 999 with lapsed time 6 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:02:45 EM iterations. 38 out of 999 with lapsed time 7 seconds and remaining time 177 seconds and will finish at 2024-02-08 22:03:07 EM iterations. 39 out of 999 with lapsed time 7 seconds and remaining time 172 seconds and will finish at 2024-02-08 22:03:02 EM iterations. 40 out of 999 with lapsed time 7 seconds and remaining time 168 seconds and will finish at 2024-02-08 22:02:58 EM iterations. 41 out of 999 with lapsed time 7 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:02:54 EM iterations. 42 out of 999 with lapsed time 8 seconds and remaining time 182 seconds and will finish at 2024-02-08 22:03:13 EM iterations. 43 out of 999 with lapsed time 8 seconds and remaining time 178 seconds and will finish at 2024-02-08 22:03:09 EM iterations. 44 out of 999 with lapsed time 8 seconds and remaining time 174 seconds and will finish at 2024-02-08 22:03:05 EM iterations. 45 out of 999 with lapsed time 8 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:03:01 EM iterations. 46 out of 999 with lapsed time 9 seconds and remaining time 186 seconds and will finish at 2024-02-08 22:03:18 EM iterations. 47 out of 999 with lapsed time 9 seconds and remaining time 182 seconds and will finish at 2024-02-08 22:03:14 EM iterations. 48 out of 999 with lapsed time 9 seconds and remaining time 178 seconds and will finish at 2024-02-08 22:03:10 ## EM restart: 4 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:12 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:12 EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:13 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2024-02-08 22:04:22 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:03:32 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:02:59 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:02:35 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:02:18 EM iterations. 9 out of 999 with lapsed time 2 seconds and remaining time 220 seconds and will finish at 2024-02-08 22:03:54 EM iterations. 10 out of 999 with lapsed time 2 seconds and remaining time 198 seconds and will finish at 2024-02-08 22:03:32 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:03:14 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:02:58 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:02:46 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:02:35 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2024-02-08 22:02:25 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2024-02-08 22:02:17 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2024-02-08 22:02:10 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2024-02-08 22:02:04 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2024-02-08 22:01:58 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2024-02-08 22:02:42 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2024-02-08 22:02:35 EM iterations. 22 out of 999 with lapsed time 3 seconds and remaining time 133 seconds and will finish at 2024-02-08 22:02:28 EM iterations. 23 out of 999 with lapsed time 3 seconds and remaining time 127 seconds and will finish at 2024-02-08 22:02:22 ## EM restart: 5 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:15 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:15 EM iterations. 3 out of 999 with lapsed time 1 seconds and remaining time 332 seconds and will finish at 2024-02-08 22:05:48 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2024-02-08 22:04:25 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:03:35 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:03:02 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:02:38 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:02:20 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2024-02-08 22:02:06 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2024-02-08 22:01:56 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:03:17 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:03:01 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:02:49 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:02:38 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2024-02-08 22:02:28 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2024-02-08 22:02:20 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2024-02-08 22:02:13 EM iterations. 18 out of 999 with lapsed time 3 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:03:02 EM iterations. 19 out of 999 with lapsed time 3 seconds and remaining time 155 seconds and will finish at 2024-02-08 22:02:53 EM iterations. 20 out of 999 with lapsed time 3 seconds and remaining time 147 seconds and will finish at 2024-02-08 22:02:45 EM iterations. 21 out of 999 with lapsed time 3 seconds and remaining time 140 seconds and will finish at 2024-02-08 22:02:38 EM iterations. 22 out of 999 with lapsed time 3 seconds and remaining time 133 seconds and will finish at 2024-02-08 22:02:31 EM iterations. 23 out of 999 with lapsed time 4 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:03:09 EM iterations. 24 out of 999 with lapsed time 4 seconds and remaining time 162 seconds and will finish at 2024-02-08 22:03:01 EM iterations. 25 out of 999 with lapsed time 4 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:02:55 EM iterations. 26 out of 999 with lapsed time 4 seconds and remaining time 150 seconds and will finish at 2024-02-08 22:02:49 EM iterations. 27 out of 999 with lapsed time 4 seconds and remaining time 144 seconds and will finish at 2024-02-08 22:02:43 ## Test passed Plot the predicted means \\(\\mu\\) and probabilities \\(\\pi\\), with purple points at the interpolated means. We can see that it works as expected. predobj = predict_flowtrend(obj, newtimes = held_out) g = plot_1d(ylist = ylist, obj=obj, x = x) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = held_out, cluster = iclust) }) %&gt;% bind_rows() g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = 2)##, alpha = .8) The estimated probabilities are shown here, with purple points showing the interpolation. It works as expected. plot_prob(obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = 3) Let’s now try to space inputs unevenly, by x. set.seed(100) TT = 100 dt &lt;- gendat_1d(TT, rep(100, TT)) dt_model &lt;- gendat_1d(TT, rep(100, TT), return_model = TRUE) ylist_orig = dt %&gt;% dt2ylist() plot_1d(ylist_orig) x = sample(1:TT, floor(TT/2)) %&gt;% sort() ylist = ylist_orig[x] set.seed(55) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = .5, lambda_prob = .5, ## rho_init = .1, nrestart = 1, verbose = TRUE) obj$all_objectives %&gt;% mutate(irestart = as.factor(irestart)) %&gt;% ggplot() + geom_line(aes(x=iter, y=objective, group = irestart, col = irestart)) ## Make mean predictions newtimes = seq(from=min(x),to=max(x),length=10000) predobj = predict_flowtrend(obj, newtimes = newtimes) ## Plot the predicted means preds = lapply(1:3, function(iclust){ tibble(mn = predobj$mn %&gt;% .[,,iclust, drop = TRUE], prob = predobj$prob %&gt;% .[,iclust, drop = TRUE], time = newtimes, cluster = iclust) }) %&gt;% bind_rows() ## EM will restart 1 times ## EM restart: 1 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:21 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:21 ## Warning in la_admm_oneclust(K = (if (local_adapt) local_adapt_niter else 1), : ADMM didn&#39;t converge for one cluster. ## Warning in la_admm_oneclust(K = (if (local_adapt) local_adapt_niter else 1), : ADMM didn&#39;t converge for one cluster. ## EM iterations. 3 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:21 EM iterations. 4 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:00:21 ## Warning in la_admm_oneclust(K = (if (local_adapt) local_adapt_niter else 1), : ADMM didn&#39;t converge for one cluster. ## EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:03:40 ## Warning in la_admm_oneclust(K = (if (local_adapt) local_adapt_niter else 1), : ADMM didn&#39;t converge for one cluster. ## EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:03:07 ## Warning in la_admm_oneclust(K = (if (local_adapt) local_adapt_niter else 1), : ADMM didn&#39;t converge for one cluster. ## EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:02:43 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:02:26 EM iterations. 9 out of 999 with lapsed time 1 seconds and remaining time 110 seconds and will finish at 2024-02-08 22:02:12 EM iterations. 10 out of 999 with lapsed time 1 seconds and remaining time 99 seconds and will finish at 2024-02-08 22:02:01 EM iterations. 11 out of 999 with lapsed time 1 seconds and remaining time 90 seconds and will finish at 2024-02-08 22:01:52 EM iterations. 12 out of 999 with lapsed time 1 seconds and remaining time 82 seconds and will finish at 2024-02-08 22:01:44 EM iterations. 13 out of 999 with lapsed time 1 seconds and remaining time 76 seconds and will finish at 2024-02-08 22:01:38 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:02:43 EM iterations. 15 out of 999 with lapsed time 2 seconds and remaining time 131 seconds and will finish at 2024-02-08 22:02:33 EM iterations. 16 out of 999 with lapsed time 2 seconds and remaining time 123 seconds and will finish at 2024-02-08 22:02:25 EM iterations. 17 out of 999 with lapsed time 2 seconds and remaining time 116 seconds and will finish at 2024-02-08 22:02:18 EM iterations. 18 out of 999 with lapsed time 2 seconds and remaining time 109 seconds and will finish at 2024-02-08 22:02:11 EM iterations. 19 out of 999 with lapsed time 2 seconds and remaining time 103 seconds and will finish at 2024-02-08 22:02:05 EM iterations. 20 out of 999 with lapsed time 2 seconds and remaining time 98 seconds and will finish at 2024-02-08 22:02:01 EM iterations. 21 out of 999 with lapsed time 2 seconds and remaining time 93 seconds and will finish at 2024-02-08 22:01:56 EM iterations. 22 out of 999 with lapsed time 2 seconds and remaining time 89 seconds and will finish at 2024-02-08 22:01:52 EM iterations. 23 out of 999 with lapsed time 2 seconds and remaining time 85 seconds and will finish at 2024-02-08 22:01:48 EM iterations. 24 out of 999 with lapsed time 2 seconds and remaining time 81 seconds and will finish at 2024-02-08 22:01:44 EM iterations. 25 out of 999 with lapsed time 2 seconds and remaining time 78 seconds and will finish at 2024-02-08 22:01:41 The estimated means \\(\\mu\\) in the training data are shown as solid triangle points. The out-of-sample \\(\\mu\\) predictions made on a fine grid of time points (shown by the yellow lines) look fine. g = plot_1d(ylist=ylist, obj = obj, x=x)##, x = x, add_point = FALSE) g + ggtitle(&quot;Fitted model&quot;) g + geom_line(aes(x=time, y=mn, group = cluster), data = preds, col = &#39;yellow&#39;, size = rel(1), alpha = .7) + ggtitle(&quot;Predictions on fine grid of times&quot;) The out-of-sample \\(\\pi\\) predictions are the lines that connect the points. They look great as well. plot_prob(obj, x=x) + ## geom_line(aes(x = time, y = prob, group = cluster, color = cluster), ## data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) + geom_line(aes(x = time, y = prob), data = preds, col = &#39;yellow&#39;, size = rel(.5), alpha = .7) Next, we’ll try evaluating an estimated model’s prediction in an out-of-sample measurement. This will be measured by the model prediction’s out-of-sample objective (negative log-likelihood). ## Generate data set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100)) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE) held_out = 25:35 dt_subset = dt %&gt;% subset(time %ni% held_out) ylist = dt_subset %&gt;% dt2ylist() x = dt_subset %&gt;% pull(time) %&gt;% unique() obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = .005, ## nrestart = 5) ## Make prediction predobj = predict_flowtrend(obj, newtimes = held_out) ## Use the predicted (interpolated) model parameters obj_pred = objective(mu = predobj$mn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) truemn = array(NA, dim = dim(predobj$mn)) truemn[,1,] = dt_model %&gt;% select(time, cluster, mean) %&gt;% pivot_wider(names_from = cluster, values_from = mean) %&gt;% subset(time %in% held_out) %&gt;% select(-time) %&gt;% as.matrix() ## Use the true mean obj_better = objective(mu = truemn, prob = predobj$prob, sigma = predobj$sigma, ylist = ylist[held_out], unpenalized = TRUE) ## Here is the estimated model plot_1d(ylist=ylist, obj=obj, x= (1:100)[-held_out], add_point = FALSE) The out-of-sample prediction is similar for the predicted model and the estimated model. Below, we’re showing just the predicted means at the held-out points, overlaid with data. (This is measured by the objective (= negative log likelihood), so lower is better! Red is worse than black, naturally.) {r fit, fig.width = 7, fig.height = 5}) g = plot_1d(ylist = dt %&gt;% subset(time %in% held_out) %&gt;% dt2ylist(), x= held_out) + xlim(c(0,100)) g + geom_line(aes(x=time, y = value, group = name), data = data.frame(truemn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time)) + geom_line(aes(x=time, y = value, group = name), data = data.frame(predobj$mn[,1,]) %&gt;% add_column(time = held_out) %&gt;% pivot_longer(-time), col = 'red') + ggtitle(paste0(round(obj_pred,3), \" (red, predicted) vs. \", round(obj_better, 3), \"(black, truth)\")) 11.2 Maximum \\((\\lambda_\\mu, \\lambda_\\pi)\\) values to test What should the maximum value of regularization parameters to use? It’s useful to be able to calculate the smallest value of regularization parameters that result in fully “simple” \\(\\mu\\) and \\(\\pi\\) over time, in all clusters. Call these \\(\\lambda_\\mu^{\\text{max}}\\) and \\(\\lambda_{\\pi}^{\\text{max}}\\). We use these to form a 2d grid of candidate \\(\\lambda\\) values – logarithmically-spaced pairs of values between starting at \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\) decreasing to some small pair of values. The function get_max_lambda() numerically estimates this maximum pair \\((\\lambda_{\\mu}^{\\text{max}}, \\lambda_{\\pi}^{\\text{max}})\\). It proceeds by first running flowtrend() on a very large pair \\((\\lambda_\\mu, \\lambda_\\pi)\\), then sequentially halving both values while checking if the resulting estimated \\(\\mu\\) and \\(\\pi\\) are all as simple over time. The simplest, most regularized model of an \\(l\\)’th order trend filter estimate will be a single \\(l\\)’th order polynomial; this means that there will be no discontinuities in the \\(l-1\\)’th order differences. (For example, a linear trend filter is \\(l=1\\); the first differences should be piecewise constant, and second differences should be zero except for at the knots. A quadratic trend filter is \\(l=2\\); the first differences should be piecewise linear, the second differences should be piecewise constant, and the third differences should be zero except for at the knots.) As soon as they cease to be simple, we stop and take the immediately previous pair of values of \\((\\lambda_\\mu, \\lambda_\\pi)\\). get_max_lambda() is a wrapper around the workhorse calc_max_lambda(). It obtains the value and saves it to a maxres_file (which defaults to maxres.Rdata) in the destin directory. #&#39; A wrapper for \\code{calc_max_lambda}. Saves the two maximum lambda values in #&#39; a file. #&#39; #&#39; @param destin Where to save the output (A two-lengthed list called #&#39; &quot;maxres&quot;). #&#39; @param maxres_file Filename for output. Defaults to maxres.Rdata. #&#39; @param ... Additional arguments to \\code{flowtrend()}. #&#39; @inheritParams calc_max_lambda #&#39; #&#39; @return No return #&#39; #&#39; @export get_max_lambda &lt;- function(destin, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist, numclust, maxdev, max_lambda_mean, max_lambda_prob, ...){ if(file.exists(file.path(destin, maxres_file))){ load(file.path(destin, maxres_file)) cat(&quot;Maximum regularization values are loaded.&quot;, fill=TRUE) return(maxres) } else { print(Sys.time()) cat(&quot;Maximum regularization values being calculated.&quot;, fill = TRUE) cat(&quot;with initial lambda values (prob and mu):&quot;, fill = TRUE) print(c(max_lambda_prob, max_lambda_mean)); maxres = calc_max_lambda(ylist = ylist, countslist = countslist, numclust = numclust, maxdev = maxdev, ## This function&#39;s settings max_lambda_prob = max_lambda_prob, max_lambda_mean = max_lambda_mean, ...) print(maxres) save(maxres, file = file.path(destin, maxres_file)) cat(&quot;file was written to &quot;, file.path(destin, maxres_file), fill=TRUE) cat(&quot;maximum regularization value calculation done.&quot;, fill = TRUE) print(Sys.time()) return(maxres) } } The aforementioned workhorse calc_max_lambda() is here. #&#39; Estimate maximum lambda values numerically. First starts with a large #&#39; initial value \\code{max_lambda_mean} and \\code{max_lambda_prob}, and runs #&#39; the EM algorithm on decreasing set of values (sequentially halved). This #&#39; stops once you see non-simple probabilities or means, and returns the *smallest* #&#39; regularization (lambda) value pair that gives full sparsity. #&#39; #&#39; Note that the \\code{zero_stabilize=TRUE} option is used in #&#39; \\code{flowtrend()}, which basically means the EM algorithm runs only until #&#39; the zero pattern stabilizes. #&#39; #&#39; @param ylist List of responses. #&#39; @param numclust Number of clusters. #&#39; @param max_lambda_mean Defaults to 4000. #&#39; @param max_lambda_prob Defaults to 1000. #&#39; @param iimax Maximum value of x for 2^{-x} factors to try. #&#39; @param ... Other arguments to \\code{flowtrend_once()}. #&#39; #&#39; @return list containing the two maximum values to use. #&#39; #&#39; @export calc_max_lambda &lt;- function(ylist, countslist = NULL, numclust, max_lambda_mean = 4000, max_lambda_prob = 1000, verbose = FALSE, iimax = 16, ...){ ## Basic setup: in each dimension, the data should only vary by a relatively ## small amount (say 1/100) dimdat = ncol(ylist[[1]]) toler_by_dim = sapply(1:dimdat, function(idim){ datrange = ylist %&gt;% sapply(FUN = function(y) y %&gt;% .[,idim] %&gt;% range()) %&gt;% range() toler = (datrange[2] - datrange[1])/1E3 }) toler_prob = 1E-3 args = list(...) l = args$l l_prob = args$l_prob ## Get range of regularization parameters. facs = sapply(1:iimax, function(ii) 2^(-ii+1)) ## DECREASING order print(&quot;running the models once&quot;) for(ii in 1:iimax){ cat(&quot;###############################################################&quot;, fill=TRUE) cat(&quot;#### lambda_prob = &quot;, max_lambda_prob * facs[ii], &quot; and lambda = &quot;, max_lambda_mean * facs[ii], &quot;being tested. &quot;, fill=TRUE) cat(&quot;###############################################################&quot;, fill=TRUE) res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], verbose = verbose, ...) ## In each dimension, the data should only vary by a relatively small amount (say 1/100) mean_is_simple = sapply(1:dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,], differences = l+1)) &lt; toler_by_dim[idim] * 2^l) }) prob_is_simple = all(abs(diff(res$prob, differences = l_prob+1)) &lt; toler_prob * 2^l_prob) all_are_simple = (all(mean_is_simple) &amp; prob_is_simple) if(!all_are_simple){ ## If there are *any* nonzero values at the first iter, prompt a restart ## with higher initial lambda values. if(ii == 1){ stop(paste0(&quot;Max lambdas: &quot;, max_lambda_mean, &quot; and &quot;, max_lambda_prob, &quot; were too small as maximum reg. values. Go up and try again!!&quot;)) ## If there are *any* nonzero values, return the immediately preceding ## lambda values -- these were the smallest values we had found that gives ## full sparsity. } else { ## Check one more time whether the model was actually zero, by fully running it; res = flowtrend_once(ylist = ylist, countslist = countslist, numclust = numclust, lambda_prob = max_lambda_prob * facs[ii], lambda = max_lambda_mean * facs[ii], ...) ## Check if both curves are maximally simple mean_is_simple = sapply(1:dimdat, FUN = function(idim){ all(abs(diff(res$mn[,idim,], differences = l+1)) &lt; toler_by_dim[idim]) }) prob_is_simple = all(abs(diff(res$prob, differences = l_prob+1)) &lt; toler_prob) all_are_simple = (all(mean_is_simple) &amp; prob_is_simple) ## If there are *any* nonzero values, stop. ## (Otherwise, just proceed to try a smaller set of lambdas.) if(!all_are_simple){ return(list(mean = max_lambda_mean * facs[ii-1], prob = max_lambda_prob * facs[ii-1])) } } } cat(fill=TRUE) } } 11.3 Define CV data folds make_cv_folds() makes the cross-validation “folds”, which are the \\(K\\) (nfold) list of data indices. These are not times! They simply split of 1:length(ylist). #&#39; Define the time folds cross-validation. #&#39; #&#39; @param nfold Number of folds. #&#39; @return List of fold indices. #&#39; @export #&#39; make_cv_folds &lt;- function(ylist=NULL, nfold, TT=NULL){ ## Make hour-long index list if(is.null(TT)) TT = length(ylist) folds &lt;- rep(1:nfold, ceiling( (TT-2)/nfold))[1:(TT-2)] inds &lt;- lapply(1:nfold, FUN = function(k) (2:(TT-1))[folds == k]) names(inds) = paste0(&quot;Fold&quot;, 1:nfold) return(inds) } We can visualize how the data is to be split. In the following plot, vertical lines mark data indices in each fold, using different colors . For nfold = 5, the first fold is every 5th point starting at 2, \\(\\{2,7,\\dots\\}\\), and the second fold is \\(\\{3,8,\\dots\\}\\), and so forth. Note: the first index \\(1\\) and the last \\(TT\\) are left out at this stage, and instead made available to all folds at training time (in cv_flowtrend()). This is because, otherwise, it would be impossible to make predictions at either ends of the data. nfold = 5 TT = 100 inds = make_cv_folds(nfold = nfold, TT = TT) print(inds) plot(NA, xlim = c(0,TT), ylim=1:2, ylab = &quot;&quot;, xlab = &quot;Data index of ylist&quot;, yaxt = &quot;n&quot;, xaxt=&quot;n&quot;) axis(1, at = c(1, seq(10, 100,10))) for(ifold in 1:nfold){ abline(v = inds[[ifold]], col = ifold, lwd = 2) } ## $Fold1 ## [1] 2 7 12 17 22 27 32 37 42 47 52 57 62 67 72 77 82 87 92 97 ## ## $Fold2 ## [1] 3 8 13 18 23 28 33 38 43 48 53 58 63 68 73 78 83 88 93 98 ## ## $Fold3 ## [1] 4 9 14 19 24 29 34 39 44 49 54 59 64 69 74 79 84 89 94 99 ## ## $Fold4 ## [1] 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 ## ## $Fold5 ## [1] 6 11 16 21 26 31 36 41 46 51 56 61 66 71 76 81 86 91 96 11.4 CV = many single jobs Next, we build the immediate elements needed for cross-validation. There are two applications of flowtrend() on data for cross-validation; one is when estimating models from held-in data folds, and the other is when re-estimating models on the full data. Estimating models on the held-in data is done by one_job(). Re-estimating models on the entire dataset is done by one_job_refit(). Here is one_job(). #&#39; Helper function to run ONE job for CV, in iprob, imu, ifold, irestart. #&#39; #&#39; @param iprob Index for prob. #&#39; @param imu Index for beta. #&#39; @param ifold Index for CV folds. #&#39; @param irestart Index for 1 through nrestart. #&#39; @param folds CV folds (from \\code{make_cv_folds()}). #&#39; @param destin Destination directory. #&#39; @param lambda_means List of regularization parameters for mean model. #&#39; @param lambda_probs List of regularization parameters for prob model. #&#39; @param ylist Data. #&#39; @param countslist Counts or biomass. #&#39; @param ... Rest of arguments for \\code{flowtrend_once()}. #&#39; #&#39; @return Nothing is returned. Instead, a file named &quot;1-1-1-1-cvscore.Rdata&quot; #&#39; is saved in \\code{destin}. (The indices here are iprob-imu-ifold-irestart). #&#39; #&#39; @export one_job &lt;- function(iprob, imu, ifold, irestart, folds, destin, lambda_means, lambda_probs, seedtab = NULL, ## The rest that is needed explicitly for flowtrend() ylist, countslist, l, l_prob, ...){ ## Get the train/test data TT &lt;- length(ylist) test.inds = unlist(folds[ifold]) %&gt;% sort() test.dat = ylist[test.inds] test.count = countslist[test.inds] train.inds = c(1, unlist(folds[-ifold]), TT) %&gt;% sort() train.dat = ylist[train.inds] train.count = countslist[train.inds] ## Check whether this job has been done already. filename = make_cvscore_filename(iprob, imu, ifold, irestart) if(file.exists(file.path(destin, filename))){ cat(fill=TRUE) cat(filename, &quot;already done&quot;, fill=TRUE) return(NULL) } ## Get the seed ready if(!is.null(seedtab)){ seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob, imu == !!imu, ifold == !!ifold, irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } lambda_prob = lambda_probs[iprob] lambda_mean = lambda_means[imu] ## Run the algorithm (all this trouble because of |nrestart|) args = list(...) args$ylist = train.dat args$countslist = train.count args$x = train.inds args$lambda = lambda_mean args$lambda_prob = lambda_prob args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)){ args = args[-which(names(args) %in% &quot;nrestart&quot;)] ## remove |nrestart| prior to feeding to flowtrend_once(). } tryCatch({ ## Estimate model argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res.train = eval(call, args) ## Assign mn and prob pred = predict_flowtrend(res.train, newtimes = test.inds) stopifnot(all(pred$prob &gt;= 0)) ## Build Dl ## ## Evaluate on test data, by calculating objective (penalized likelihood with penalty parameters set to 0) cvscore = objective(mu = pred$mn, prob = pred$prob, sigma = pred$sigma, ylist = test.dat, countslist = test.count, unpenalized = TRUE) ## Dl = diag(rep(1, length(test.count))), ## TODO: what is wrong here? ## lambda_prob = 0, ## lambda = 0) ## prob = res.train$prob, ## beta = res.train$beta) ## Store (temporarily) the run times time_per_iter = res.train$time_per_iter final_iter = res.train$final.iter total_time = res.train$total_time ## Store the results. mn = res.train$mn prob = res.train$prob objectives = res.train$objectives ## Save the CV results save(cvscore, ## Time time_per_iter, final_iter, total_time, ## Results lambda_mean, lambda_prob, lambda_means, lambda_probs, mn, prob, objectives, ## Save the file file = file.path(destin, filename)) return(NULL) }, error = function(err) { err$message = paste(err$message, &quot;\\n(No file will be saved for lambdas (&quot;, signif(lambda_probs[iprob],3), &quot;, &quot;, signif(lambda_means[imu],3), &quot;) whose indices are: &quot;, iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot; .)&quot;,sep=&quot;&quot;) cat(err$message, fill=TRUE) warning(err)}) } Here is one_job_refit(). #&#39; Refit model for one pair of regularization parameter values. Saves to #&#39; \\code{nrestart} files named like &quot;1-4-3-fit.Rdata&quot;, for #&#39; &quot;(iprob)-(imu)-(irestart)-fit.Rdata&quot;. #&#39; #&#39; (Note, \\code{nrestart} is not an input to this function.) #&#39; #&#39; @inheritParams one_job #&#39; #&#39; @export one_job_refit &lt;- function(iprob, imu, destin, lambda_means, lambda_probs, l, l_prob, seedtab = NULL, ## The rest that is needed explicitly for flowtrend_once() ylist, countslist, ...){ args = list(...) nrestart = args$nrestart assertthat::assert_that(!is.null(nrestart)) for(irestart in 1:nrestart){ ## Writing file filename = make_refit_filename(iprob = iprob, imu = imu, irestart = irestart) if(file.exists(file.path(destin, filename))){ cat(filename, &quot;already done&quot;, fill=TRUE) next } else { ## Get the seed ready if(!is.null(seedtab)){ ifold = 0 seed = seedtab %&gt;% dplyr::filter(iprob == !!iprob, imu == !!imu, ifold == !!ifold, irestart == !!irestart) %&gt;% dplyr::select(seed1, seed2, seed3, seed4, seed5, seed6, seed7) %&gt;% unlist() %&gt;% as.integer() } else { seed = NULL } ## Get the fitted results on the entire data args = list(...) args$ylist = ylist args$countslist = countslist args$lambda_prob = lambda_probs[iprob] args$lambda = lambda_means[imu] args$l = l args$l_prob = l_prob args$seed = seed if(&quot;nrestart&quot; %in% names(args)) args = args[-which(names(args) %in% &quot;nrestart&quot;)] ## remove |nrestart| prior to feeding ## Call the function. argn &lt;- lapply(names(args), as.name) names(argn) &lt;- names(args) call &lt;- as.call(c(list(as.name(&quot;flowtrend_once&quot;)), argn)) res = eval(call, args) ## Save the results cat(&quot;Saving file here:&quot;, file.path(destin, filename), fill=TRUE) save(res, file=file.path(destin, filename)) } } } Since cross-validation entails running many jobs, we need to index individual “jobs” carefully. Here are some more helpers for indexing: make_iimat(): Make a table whose rows index each “job” (iprob, imu, ifold, irestart), to be used by one_job(). make_iimat_small(): Make a table whose rows index each (iprob, imu, irestart) for re-estimating models, to be used by one_job_refit(). #&#39; Indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; #&#39; ind iprob imu ifold irestart #&#39; 55 6 1 2 1 #&#39; 56 7 1 2 1 #&#39; 57 1 2 2 1 #&#39; 58 2 2 2 1 #&#39; 59 3 2 2 1 #&#39; 60 4 2 2 1 #&#39; @param cv_gridsize CV grid size. #&#39; @param nfold Number of CV folds. #&#39; @param nrestart Number of random restarts of EM algorithm. #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat &lt;- function(cv_gridsize, nfold, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, ifold = 1:nfold, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } #&#39; 2d indices for the cross validation jobs. #&#39; #&#39; The resulting iimat looks like this: #&#39; (#, iprob, imu, irestart) #&#39; 1, 1, 1, 1 #&#39; 2, 1, 2, 1 #&#39; 3, 1, 3, 1 #&#39; #&#39; @inheritParams make_iimat #&#39; #&#39; @return Integer matrix. #&#39; #&#39; @export make_iimat_small &lt;- function(cv_gridsize, nrestart){ iimat = expand.grid(iprob = 1:cv_gridsize, imu = 1:cv_gridsize, irestart = 1:nrestart) iimat = cbind(ind = as.numeric(rownames(iimat)), iimat) return(iimat) } Let’s see the integer matrices that these functions make. make_iimat(cv_gridsize = 5, nfold = 5, nrestart = 10) %&gt;% head() make_iimat_small(cv_gridsize = 5, nrestart = 10) %&gt;% head() ## ind iprob imu ifold irestart ## 1 1 1 1 1 1 ## 2 2 2 1 1 1 ## 3 3 3 1 1 1 ## 4 4 4 1 1 1 ## 5 5 5 1 1 1 ## 6 6 1 2 1 1 ## ind iprob imu irestart ## 1 1 1 1 1 ## 2 2 2 1 1 ## 3 3 3 1 1 ## 4 4 4 1 1 ## 5 5 5 1 1 ## 6 6 1 2 1 Next, the functions make_cvscore_filename() and make_refit_filename() are used to form the names of the numerous output files. #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_cvscore_filename &lt;- function(iprob, imu, ifold, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-&quot;, irestart, &quot;-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for cross-validation results. #&#39; @param iprob #&#39; @param imu #&#39; @param ifold #&#39; @param irestart #&#39; #&#39; @export make_best_cvscore_filename &lt;- function(iprob, imu, ifold){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, ifold, &quot;-best-cvscore.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; @param irestart #&#39; #&#39; @export make_refit_filename &lt;- function(iprob, imu, irestart){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-&quot;, irestart, &quot;-fit.Rdata&quot;) return(filename) } #&#39; Create file name (a string) for re-estimated models for the lambda values #&#39; indexed by \\code{iprob} and \\code{imu}. #&#39; @param iprob #&#39; @param imu #&#39; #&#39; @export make_best_refit_filename &lt;- function(iprob, imu){ filename = paste0(iprob, &quot;-&quot;, imu, &quot;-best-fit.Rdata&quot;) return(filename) } Here’s a useful helper logspace(max, min) to make logarithmically spaced set of numbers, given min and max. We can use this to make a grid of lambda pairs to be used for cross-validation. #&#39; Helper function to logarithmically space out R. \\code{length} values linear #&#39; on the log scale from \\code{max} down to \\code{min}. #&#39; #&#39; @param max Maximum value. #&#39; @param min Minimum value. #&#39; @param length Length of the output string. #&#39; @param min.ratio Factor to multiply to \\code{max}. #&#39; #&#39; @return Log spaced #&#39; #&#39; @export logspace &lt;- function(max, min=NULL, length, min.ratio = 1E-4){ if(is.null(min)) min = max * min.ratio vec = 10^seq(log10(min), log10(max), length = length) stopifnot(abs(vec[length(vec)] - max) &lt; 1E10) return(vec) } 11.5 Running cross-validation Putting the helpers all together, you get the main user-facing function cv_flowtrend(). #&#39; Cross-validation for flowtrend(). Saves results to separate files in #&#39; \\code{destin}. #&#39; #&#39; @param destin Directory where output files are saved. #&#39; @param nfold Number of cross-validation folds. Defaults to 5. #&#39; @param nrestart Number of repetitions. #&#39; @param save_meta If TRUE, save meta data. #&#39; @param lambda_means Regularization parameters for means. #&#39; @param lambda_probs Regularization parameters for probs. #&#39; @param folds Manually provide CV folds (list of time points of data to use #&#39; as CV folds). Defaults to NULL. #&#39; @param mc.cores Use this many CPU cores. #&#39; @param blocksize Contiguous time blocks from which to form CV time folds. #&#39; @param refit If TRUE, estimate the model on the full data, for each pair of #&#39; regularization parameters. #&#39; @param ... Additional arguments to flowtrend(). #&#39; @inheritParams flowtrend_once #&#39; #&#39; @return No return. #&#39; #&#39; @export cv_flowtrend &lt;- function(## Data ylist, countslist, ## Define the locations to save the CV. destin = &quot;.&quot;, ## Regularization parameter values lambda_means, lambda_probs, l, l_prob, iimat = NULL, ## Other settings maxdev, numclust, nfold, nrestart, verbose = FALSE, refit = FALSE, save_meta = FALSE, mc.cores = 1, folds = NULL, seedtab = NULL, ...){ ## Basic checks stopifnot(length(lambda_probs) == length(lambda_means)) cv_gridsize = length(lambda_means) ## There&#39;s an option to input one&#39;s own iimat matrix. if(is.null(iimat)){ ## Make an index of all jobs if(!refit) iimat = make_iimat(cv_gridsize, nfold, nrestart) if(refit) iimat = make_iimat_small(cv_gridsize, nrestart) } ## Define the CV folds ## folds = make_cv_folds(ylist = ylist, nfold = nfold, blocksize = 1) if(is.null(folds)){ folds = make_cv_folds(ylist = ylist, nfold = nfold) } else { stopifnot(length(folds) == nfold) } ## Save meta information, once. if(save_meta){ ##if(!refit){ if(file.exists(file = file.path(destin, &#39;meta.Rdata&#39;))){ ## Put aside the current guys cat(fill=TRUE) print(&quot;Meta data already exists!&quot;) folds_current = folds nfold_current = nfold nrestart_current = nrestart cv_gridsize_current = cv_gridsize lambda_means_current = lambda_means lambda_probs_current = lambda_probs ylist_current = ylist countslist_current = countslist ## Load the saved metadata and check if they are all the same as the current guys load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) stopifnot(identical(folds, folds_current)) stopifnot(nfold == nfold_current) stopifnot(nrestart == nrestart_current) ## Added recently stopifnot(cv_gridsize == cv_gridsize_current) stopifnot(all(lambda_means == lambda_means_current)) stopifnot(all(lambda_probs == lambda_probs_current)) stopifnot(identical(ylist, ylist_current)) stopifnot(identical(countslist, countslist_current)) cat(fill=TRUE) cat(&quot;Successfully checked that the saved metadata is identical to the current one.&quot;, fill = TRUE) } else { save(folds, nfold, nrestart, ## Added recently cv_gridsize, lambda_means, lambda_probs, ylist, countslist, ## Save the file file = file.path(destin, &#39;meta.Rdata&#39;)) print(paste0(&quot;wrote meta data to &quot;, file.path(destin, &#39;meta.Rdata&#39;))) } ## } } ## Run the EM algorithm many times, for each value of (iprob, imu, ifold, irestart) start.time = Sys.time() parallel::mclapply(1:nrow(iimat), function(ii){ print_progress(ii, nrow(iimat), &quot;Jobs (EM replicates) assigned on this computer&quot;, start.time = start.time) if(!refit){ iprob = iimat[ii,&quot;iprob&quot;] imu = iimat[ii,&quot;imu&quot;] ifold = iimat[ii,&quot;ifold&quot;] irestart = iimat[ii,&quot;irestart&quot;] ## if(verbose) cat(&#39;(iprob, imu, ifold, irestart)=&#39;, c(iprob, imu, ifold, irestart), fill=TRUE) } else { iprob = iimat[ii, &quot;iprob&quot;] imu = iimat[ii, &quot;imu&quot;] ifold = 0 } if(!refit){ one_job(iprob = iprob, imu = imu, l = l, l_prob = l_prob, ifold = ifold, irestart = irestart, folds = folds, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments for flowtrend() ylist = ylist, countslist = countslist, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, verbose = FALSE, seedtab = seedtab) } else { one_job_refit(iprob = iprob, imu = imu, l = l, l_prob = l_prob, destin = destin, lambda_means = lambda_means, lambda_probs = lambda_probs, ## Arguments to flowtrend() ylist = ylist, countslist = countslist, ## Additional arguments for flowtrend(). numclust = numclust, maxdev = maxdev, nrestart = nrestart, verbose = FALSE, seedtab = seedtab) } return(NULL) }, mc.cores = mc.cores) } 11.6 Summarizing the output Once the cross-validation is finished (and saved into many files called e.g. 1-1-1-1-cvscore.Rdata or 1-1-1-fit.Rdata), we can use cv_summary() to summarize the results. If you look closely, you’ll notice that cv_aggregate() is the workhorse. #&#39; Main function for summarizing the cross-validation results. #&#39; #&#39; @inheritParams cv_flowtrend #&#39; @param save If TRUE, save to \\code{file.path(destin, filename)}. #&#39; @param filename File name to save to. #&#39; #&#39; @return List containing summarized results from cross-validation. Here are #&#39; some objects in this list: \\code{bestres} is the the overall best model #&#39; chosen from the cross-validation; \\code{cvscoremat} is a 2d matrix of CV #&#39; scores from all pairs of regularization parameters; \\code{bestreslist} is a #&#39; list of all the best models (out of \\code{nrestart} EM replications) from the #&#39; each pair of lambda values. If \\code{isTRUE(save)}, nothing is returned. #&#39; #&#39; @export cv_summary &lt;- function(destin = &quot;.&quot;, save = FALSE, filename = &quot;summary.RDS&quot; ){ ## Load data load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things: nrestart, nfold, cv_gridsize stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) ## Get the results of the cross-validation. a = cv_aggregate(destin) cvscore.mat = a$cvscore.mat min.inds = a$min.inds ## Get results from refitting bestreslist = cv_aggregate_res(destin = destin) bestres = bestreslist[[paste0(min.inds[1] , &quot;-&quot;, min.inds[2])]] if(is.null(bestres)){ if(min.inds[1]==2 &amp; min.inds[2]==3) browser() stop(paste0(&quot;The model with lambda indices (&quot;, min.inds[1], &quot;,&quot;, min.inds[2], &quot;) is not available.&quot;)) } out = list(bestres = bestres, cvscore.mat = cvscore.mat, min.inds = min.inds, ## List of all best models for all lambda pairs. bestreslist = bestreslist, destin = destin) if(save){ saveRDS(out, file=file.path(destin, filename))} return(out) } #&#39; From the results saved in \\code{destin}, aggregate all |nrestart| files to retain only the &quot;best&quot; restart, and delete the rest. #&#39; #&#39; All meta information (|nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, |lambda_probs|) comes from \\code{meta.Rdata}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_makebest &lt;- function(destin){ ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ for(ifold in 1:nfold){ ## If the &quot;best&quot; flowtrend object has already been created, do nothing. best_filename = make_best_cvscore_filename(iprob, imu, ifold) if(file.exists(file.path(destin, best_filename))){ next ## Otherwise, attempt to load from all |nrestart| replicates } else { objectives = load_all_objectives(destin, iprob, imu, ifold, nrestart) ## If all |nrestart| files exist, delete all files but the best model. if(all(!is.na(objectives))){ best_irestart = which(objectives == min(objectives)) keep_only_best(destin, iprob, imu, ifold, nrestart, best_irestart) } else { print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; ifold=&quot;, ifold, &quot; had objectives: &quot;, objectives)) } } } } } ## Also go over the &quot;refit&quot; files for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## If the &quot;best&quot; flowtrend object has already been created, do nothing. best_filename = make_best_refit_filename(iprob, imu) if(file.exists(file.path(destin, best_filename))){ ## ## Check if any more jobs have been done since before ## objectives = load_all_refit_objectives(destin, iprob, imu, nrestart) ## if(any(!is.na(objectives))){ ## nonmissing_irestart = which(!is.na(objectives))## == min(objectives)) ## ## keep_only_best_refit(destin, iprob, imu, nrestart, best_irestart) ## load(file.path(destin, best_filename)) ## } else { ## print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; had /refit/ objectives: &quot;, objectives)) ## } ## } next ## Otherwise, attempt to load from all |nrestart| replicates } else { objectives = load_all_refit_objectives(destin, iprob, imu, nrestart) if(all(!is.na(objectives))){ best_irestart = which(objectives == min(objectives)) keep_only_best_refit(destin, iprob, imu, nrestart, best_irestart) } else { print(paste0(&quot;iprob=&quot;, iprob, &quot; imu=&quot;, imu, &quot; had /refit/ objectives: &quot;, objectives)) } } } } } #&#39; Loading all objectives, with NA&#39;s for missing files load_all_objectives &lt;- function(destin, iprob, imu, ifold, nrestart){ objectives = sapply(1:nrestart, function(irestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) return(objectives[length(objectives)]) }, error = function(e){ NA }) }) return(objectives) } #&#39; Loading all objectives, with NA&#39;s for missing files load_all_refit_objectives &lt;- function(destin, iprob, imu, nrestart){ objectives = sapply(1:nrestart, function(irestart){ filename = make_refit_filename(iprob, imu, irestart) ## filename = make_best_cvscore_filename(iprob, imu, ifold) tryCatch({ load(file.path(destin, filename), verbose = FALSE) return(res$objectives[length(res$objectives)]) }, error = function(e){ NA }) }) return(objectives) } #&#39; Keeping only the output files for the &quot;best&quot; restart, and deleting the rest. keep_only_best &lt;- function(destin, iprob, imu, ifold, nrestart, best_irestart){ for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) if(irestart == best_irestart){ best_filename = make_best_cvscore_filename(iprob, imu, ifold) file.rename(from = file.path(destin, filename), to = file.path(destin, best_filename)) } else { file.remove(file.path(destin, filename)) } } } #&#39; Keeping only the output files (among refit models) for the &quot;best&quot; restart, #&#39; and deleting the rest. keep_only_best_refit &lt;- function(destin, iprob, imu, nrestart, best_irestart){ for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) if(irestart == best_irestart){ best_filename = make_best_refit_filename(iprob, imu) file.rename(from = file.path(destin, filename), to = file.path(destin, best_filename)) } else { file.remove(file.path(destin, filename)) } } } #&#39; Aggregate CV scores from the results, saved in \\code{destin}. #&#39; #&#39; @param destin Directory with cross-validation output. #&#39; #&#39; @export cv_aggregate &lt;- function(destin){ ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|, |lambda_means|, ## ## |lambda_probs|) load(file = file.path(destin, &#39;meta.Rdata&#39;), verbose = FALSE) ## This loads all the necessary things; just double-checking. stopifnot(exists(&quot;nrestart&quot;)) stopifnot(exists(&quot;nfold&quot;)) stopifnot(exists(&quot;cv_gridsize&quot;)) stopifnot(exists(c(&quot;lambda_probs&quot;))) stopifnot(exists(c(&quot;lambda_means&quot;))) ## Aggregate the results cvscore.array = array(NA, dim = c(cv_gridsize, cv_gridsize, nfold, nrestart)) cvscore.mat = matrix(NA, nrow = cv_gridsize, ncol = cv_gridsize) for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ obj = matrix(NA, nrow=nfold, ncol=nrestart) for(ifold in 1:nfold){ ## If the &quot;best&quot; flowtrend object has already been created, use it. best_filename = make_best_cvscore_filename(iprob, imu, ifold) if(file.exists(file.path(destin, best_filename))){ load(file.path(destin, best_filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, 1] = cvscore obj[ifold, 1] = objectives[length(objectives)] ## Otherwise, aggregate directly from the individual files } else { for(irestart in 1:nrestart){ filename = make_cvscore_filename(iprob, imu, ifold, irestart) tryCatch({ load(file.path(destin, filename), verbose = FALSE) cvscore.array[iprob, imu, ifold, irestart] = cvscore obj[ifold, irestart] = objectives[length(objectives)] }, error = function(e){}) } } } ## Pick out the CV scores with the *best* (lowest) objective value cvscores = cvscore.array[iprob, imu , , ] best.models = apply(obj, 1, function(myrow){ ind = which(myrow == min(myrow, na.rm=TRUE)) if(length(ind)&gt;1) ind = ind[1] ## Just choose one, if there is a tie. return(ind) }) %&gt;% as.numeric() final.cvscores = sapply(1:nfold, function(ifold){ cvscores[ifold, best.models[ifold]] ## Why did we get rid of this? ## cvscores[ifold] }) cvscore.mat[iprob, imu] = mean(final.cvscores) } } ## Clean a bit cvscore.mat[which(is.nan(cvscore.mat), arr.ind = TRUE)] = NA ## ## Read the meta data (for |nfold|, |cv_gridsize|, |nrestart|) rownames(cvscore.mat) = signif(lambda_probs,3) colnames(cvscore.mat) = signif(lambda_means,3) ## Find the minimum mat = cvscore.mat min.inds = which(mat == min(mat, na.rm = TRUE), arr.ind = TRUE) ## Return the results out = list(cvscore.array = cvscore.array, cvscore.mat = cvscore.mat, lambda_means = lambda_means, lambda_probs = lambda_probs, min.inds = min.inds) return(out) } #&#39; Helper to aggregate CV results and obtain the |res| object, all saved in #&#39; |destin|. #&#39; #&#39; @inheritParams cv_aggregate #&#39; #&#39; @return List containing, for every (iprob, imu), the &quot;best&quot; estimated model #&#39; out of the |nrestart| replicates (best in the sense that it had the best #&#39; likelihood value out of the |nrestart| replicates.) cv_aggregate_res &lt;- function(destin){ load(file.path(destin, &quot;meta.Rdata&quot;)) ## df.mat = matrix(NA, ncol=cv_gridsize, nrow=cv_gridsize) res.list = list() for(iprob in 1:cv_gridsize){ for(imu in 1:cv_gridsize){ ## If the &quot;best&quot; object has already been created, use it. best_refit_filename = make_best_refit_filename(iprob, imu) if(file.exists(file.path(destin, best_refit_filename))){ load(file.path(destin, best_refit_filename), verbose = FALSE) bestres = res ## Otherwise, aggregate directly from the individual files } else { obj = rep(NA, nrestart) res.list.inner = list() for(irestart in 1:nrestart){ filename = make_refit_filename(iprob, imu, irestart) tryCatch({ load(file.path(destin, filename)) res.list.inner[[irestart]] = res obj[irestart] = res$objectives[length(res$objectives)] }, error = function(e){ NULL }) } if(!all(is.na(obj))){ bestres = res.list.inner[[which.min(obj)]] ## which.min? } } ## Add the &quot;best&quot; object to a list. res.list[[paste0(iprob, &quot;-&quot;, imu)]] = bestres } } return(res.list) } 11.7 CV on your own computer Using all this functionality, we’d like to be able to cross-validate on our own laptop, using cv_flowtrend(). Let’s try it out. cv_gridsize = 3 l = 1 l_prob = 1 set.seed(332) ylist = gendat_1d(10, rep(100,10)) %&gt;% dt2ylist() plot_1d(ylist) folds = make_cv_folds(lapply(1:100, function(ii) cbind(runif(10))), nfold = 5, TT = length(ylist)) ## What is this doing? FISHY lambda_means = lambda_probs = logspace(min = 1E-5, max = 1, length = cv_gridsize) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = &quot;~/repos/flowtrend/inst/output/test&quot;, lambda_means = lambda_means, lambda_probs = lambda_probs, l = l, l_prob = l_prob, maxdev = NULL, numclust = 3, nfold = 3, nrestart = 3, niter = 3, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 6) } Once all jobs are run, use cv_makebest() to go over all the output, and (1) retain the best model out of the 5 replicates (random restarts), and (2) delete 4 of the the 5 replicates. ## Clean the output directory to keep only the &quot;best&quot; results cv_makebest(destin = &quot;~/repos/flowtrend/inst/output/test&quot;) ## Plot the final result obj = cv_summary(destin = &quot;~/repos/flowtrend/inst/output/test&quot;) plot_1d(ylist, obj$bestres, add_point=FALSE) Okay, now we know it’s possible to run on (say) a laptop. A more realistic application would go something like this: litr::load_all(&quot;~/repos/flowtrend/index.Rmd&quot;) get_max_lambda(&quot;~/repos/flowtrend/inst/output/test&quot;, maxres_file = &quot;maxres.Rdata&quot;, ylist, countslist = NULL, numclust = 3, maxdev = 1, max_lambda_prob = 3000000, max_lambda_mean = 3000000, verbose = TRUE, l = 1, l_prob = 1) load(&quot;maxres.Rdata&quot;, verbose = TRUE) ## Loads &quot;maxres&quot; object lambda_means = logspace(max = maxres$means, length = 5) lambda_probs = logspace(max = maxres$prob, length = 5) for(refit in c(FALSE, TRUE)){ cv_flowtrend(ylist = ylist, countslist = NULL, destin = &quot;~/repos/flowtrend/tempoutput&quot;, lambda_means = lambda_means, lambda_probs = lambda_probs, l = 2, l_prob = 1, maxdev = 2, numclust = 3, nfold = 5, nrestart = 5, verbose = TRUE, refit = refit, save_meta = TRUE, mc.cores = 6) } "],["testing-the-flowtrend-method.html", "12 Testing the flowtrend method 12.1 1d example 12.2 Testing monotonicity of objective values 12.3 2d example 12.4 Working with “binned” dataset 12.5 Unevenly spaced inputs (x)", " 12 Testing the flowtrend method We’re going to assume the flowtrend() function has been built, and test it now. 12.1 1d example Generate data. set.seed(100) dt &lt;- gendat_1d(100, rep(100, 100), offset = 4) dt_model &lt;- gendat_1d(100, rep(100, 100), return_model = TRUE, offset=4) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() plot_1d(ylist) dt_model %&gt;% select(time, cluster, prob) %&gt;% ggplot() + geom_line(aes(x=time, y=prob, group=cluster, col = cluster)) Next, we fit the model. set.seed(18) obj &lt;- flowtrend(ylist = ylist, x = x, maxdev = 1, numclust = 3, l = 2, l_prob = 1, lambda = .1, lambda_prob = .1, verbose = TRUE, nrestart = 5) ## Also reorder the cluster labels of the truth, to match the fitted model. ord = obj$mn[,1,] %&gt;% colSums() %&gt;% order(decreasing=TRUE) lookup &lt;- setNames(c(1:obj$numclust), ord) dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %&gt;% as.factor() ## Reorder the cluster lables of the fitted model. obj = reorder_clust(obj) The data and estimated model are shown here. The dashed lines are the true means. plot_1d(ylist = ylist, obj = obj, x = x, add_point = FALSE) + geom_line(aes(x = time, y = mean, group = cluster), data = dt_model,## %&gt;% subset(time %ni% held_out), linetype = &quot;dashed&quot;, size=2, alpha = .7) The estimated probabilities are shown here. plot_prob(obj=obj, x=x) + geom_line(aes(x = time, y = prob, group = cluster, color = cluster), data = dt_model, linetype = &quot;dashed&quot;) + facet_wrap(~cluster) 12.2 Testing monotonicity of objective values The objective value (that is, the penalized log likelihood) should be monotone decreasing across EM algorithm iterations. testthat::test_that(&quot;Objective value decreases over EM iterations.&quot;,{ for(iseed in 1:5){ print(iseed) ## Generate synthetic data set.seed(iseed*100) dt &lt;- gendat_1d(100, rep(10, 100)) dt_model &lt;- gendat_1d(100, rep(10, 100), return_model = TRUE) ylist = dt %&gt;% dt2ylist() x = dt %&gt;% pull(time) %&gt;% unique() ## Fit model obj &lt;- flowtrend_once(ylist = ylist, x = x, maxdev = 5, numclust = 3, lambda = 0.02, l = 1, l_prob = 2, lambda_prob = 0.05) ## Test objective monotonicity niter_end = length(obj$objective) testthat::expect_true(all(diff(obj$objective) &lt; 1E-4)) ## Make a plot g = ggplot(tibble(iter=1:niter_end, objective=obj$objectives)) + geom_point(aes(x=iter, y=objective)) + geom_line(aes(x=iter, y=objective)) + ggtitle(paste0(&quot;Seed=&quot;, iseed*100)) print(g) } }) ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## Test passed 12.3 2d example Next, we try out flowtrend on a synthetic 2d data example. set.seed(100) TT = 100 dt &lt;- gendat_2d(TT, rep(100, times = TT)) x = 1:TT set.seed(10) obj &lt;- flowtrend_once(ylist = dt$ylist, x = x, maxdev = 3, numclust = 3, l = 2, l_prob = 2, lambda = 0.01, lambda_prob = .01, rho_init = 0.01) g1 = plot_1d(ylist = dt$ylist, obj = obj, idim = 2) + ylim(c(-7, 4)) g2 = plot_1d(ylist = dt$ylist, obj = obj, idim = 1) + ylim(c(-7, 4)) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) The plot above shows 1d projections in the two dimensions of the response data. And the plot below are some snapshots in time \\(t\\). timelist = c(10, 20, 30, 40, 60, 80) plist = lapply(timelist, function(tt){ plot_2d(dt$ylist, obj, tt = tt) + coord_fixed() + ylim(-6, 3) + xlim(-6, 3) }) do.call(ggpubr::ggarrange, c(plist, ncol=3, nrow=2)) ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Coordinate system already present. Adding new coordinate system, which will replace the existing one. ## Warning: Removed 1 rows containing missing values (`geom_point()`). 12.4 Working with “binned” dataset Recall that “binning” means we will use binned frequency histogram estimates of the original particle-level dataset; this is useful when there are many particles. ## library(flowmix) set.seed(10232) TT = 100 dt &lt;- gendat_2d(TT, rep(100, times = TT)) manual_grid = flowmix::make_grid(dt$ylist, gridsize = 40) binres = flowmix::bin_many_cytograms(dt$ylist, manual.grid = manual_grid) set.seed(100) obj = flowtrend(ylist = binres$ybin_list, countslist = binres$counts_list, maxdev = 5, numclust = 3, l = 2, l_prob = 2, lambda = .01, lambda_prob = .005, rho_init = .01, nrestart = 1) obj$objectives %&gt;% plot(type=&#39;l&#39;) g1 = plot_1d(ylist = dt$ylist, obj = obj, idim = 2) + ylim(c(-7, 4)) g2 = plot_1d(ylist = dt$ylist, obj = obj, idim = 1) + ylim(c(-7, 4)) do.call(ggpubr::ggarrange, c(list(g1, g2), ncol=1, nrow=2)) ## Warning: replacing previous import &#39;RcppArmadillo::fastLmPure&#39; by &#39;RcppEigen::fastLmPure&#39; when loading &#39;flowmix&#39; ## Warning: replacing previous import &#39;RcppArmadillo::fastLm&#39; by &#39;RcppEigen::fastLm&#39; when loading &#39;flowmix&#39; 12.5 Unevenly spaced inputs (x) Let’s try the EM algorithm out with unevenly spaced inputs. set.seed(100) dt &lt;- gendat_1d(TT=100, rep(100, times=100)) dt_model &lt;- gendat_1d(TT=100, rep(100, times=100), return_model = TRUE) ylist_orig = dt %&gt;% dt2ylist() ind_rm_list = list(seq(from=10, to=100, by=10), ind_rm = 30:50) for(ind_rm in ind_rm_list){ x = (1:100)[-ind_rm] ylist = ylist_orig[x] set.seed(100) obj &lt;- flowtrend_once(ylist = ylist, x = x, maxdev = 100, numclust = 3, l = 2, l_prob = 2, lambda = 0.1, lambda_prob = 0.1, ## verbose = TRUE, admm_local_adapt = TRUE, rho_init = 0.01) obj$objectives %&gt;% plot(type=&#39;o&#39;, ylab = &quot;EM objectives&quot;) plot_1d(obj = obj, ylist = ylist, x = x) } ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:02:20 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:02:21 EM iterations. 3 out of 999 with lapsed time 1 seconds and remaining time 332 seconds and will finish at 2024-02-08 22:07:53 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2024-02-08 22:06:30 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:05:40 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:05:07 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:04:44 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:04:26 EM iterations. 9 out of 999 with lapsed time 2 seconds and remaining time 220 seconds and will finish at 2024-02-08 22:06:02 EM iterations. 10 out of 999 with lapsed time 2 seconds and remaining time 198 seconds and will finish at 2024-02-08 22:05:40 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:05:22 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:05:07 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:04:55 EM iterations. 14 out of 999 with lapsed time 3 seconds and remaining time 211 seconds and will finish at 2024-02-08 22:05:54 EM iterations. 15 out of 999 with lapsed time 3 seconds and remaining time 197 seconds and will finish at 2024-02-08 22:05:40 EM iterations. 16 out of 999 with lapsed time 3 seconds and remaining time 184 seconds and will finish at 2024-02-08 22:05:27 EM iterations. 17 out of 999 with lapsed time 3 seconds and remaining time 173 seconds and will finish at 2024-02-08 22:05:17 EM iterations. 18 out of 999 with lapsed time 3 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:05:08 EM iterations. 19 out of 999 with lapsed time 4 seconds and remaining time 206 seconds and will finish at 2024-02-08 22:05:50 EM iterations. 20 out of 999 with lapsed time 4 seconds and remaining time 196 seconds and will finish at 2024-02-08 22:05:40 EM iterations. 21 out of 999 with lapsed time 4 seconds and remaining time 186 seconds and will finish at 2024-02-08 22:05:31 EM iterations. 22 out of 999 with lapsed time 4 seconds and remaining time 178 seconds and will finish at 2024-02-08 22:05:23 EM iterations. 23 out of 999 with lapsed time 5 seconds and remaining time 212 seconds and will finish at 2024-02-08 22:05:57 EM iterations. 24 out of 999 with lapsed time 5 seconds and remaining time 203 seconds and will finish at 2024-02-08 22:05:49 EM iterations. 25 out of 999 with lapsed time 5 seconds and remaining time 195 seconds and will finish at 2024-02-08 22:05:41 EM iterations. 26 out of 999 with lapsed time 6 seconds and remaining time 225 seconds and will finish at 2024-02-08 22:06:11 EM iterations. 27 out of 999 with lapsed time 6 seconds and remaining time 216 seconds and will finish at 2024-02-08 22:06:02 EM iterations. 28 out of 999 with lapsed time 6 seconds and remaining time 208 seconds and will finish at 2024-02-08 22:05:55 ## EM iterations. 1 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:02:27 EM iterations. 2 out of 999 with lapsed time 0 seconds and remaining time 0 seconds and will finish at 2024-02-08 22:02:27 EM iterations. 3 out of 999 with lapsed time 1 seconds and remaining time 332 seconds and will finish at 2024-02-08 22:07:59 EM iterations. 4 out of 999 with lapsed time 1 seconds and remaining time 249 seconds and will finish at 2024-02-08 22:06:37 EM iterations. 5 out of 999 with lapsed time 1 seconds and remaining time 199 seconds and will finish at 2024-02-08 22:05:47 EM iterations. 6 out of 999 with lapsed time 1 seconds and remaining time 166 seconds and will finish at 2024-02-08 22:05:14 EM iterations. 7 out of 999 with lapsed time 1 seconds and remaining time 142 seconds and will finish at 2024-02-08 22:04:50 EM iterations. 8 out of 999 with lapsed time 1 seconds and remaining time 124 seconds and will finish at 2024-02-08 22:04:32 EM iterations. 9 out of 999 with lapsed time 2 seconds and remaining time 220 seconds and will finish at 2024-02-08 22:06:09 EM iterations. 10 out of 999 with lapsed time 2 seconds and remaining time 198 seconds and will finish at 2024-02-08 22:05:47 EM iterations. 11 out of 999 with lapsed time 2 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:05:29 EM iterations. 12 out of 999 with lapsed time 2 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:05:13 EM iterations. 13 out of 999 with lapsed time 2 seconds and remaining time 152 seconds and will finish at 2024-02-08 22:05:01 EM iterations. 14 out of 999 with lapsed time 2 seconds and remaining time 141 seconds and will finish at 2024-02-08 22:04:50 EM iterations. 15 out of 999 with lapsed time 3 seconds and remaining time 197 seconds and will finish at 2024-02-08 22:05:47 EM iterations. 16 out of 999 with lapsed time 3 seconds and remaining time 184 seconds and will finish at 2024-02-08 22:05:34 EM iterations. 17 out of 999 with lapsed time 3 seconds and remaining time 173 seconds and will finish at 2024-02-08 22:05:23 EM iterations. 18 out of 999 with lapsed time 3 seconds and remaining time 164 seconds and will finish at 2024-02-08 22:05:14 EM iterations. 19 out of 999 with lapsed time 3 seconds and remaining time 155 seconds and will finish at 2024-02-08 22:05:05 EM iterations. 20 out of 999 with lapsed time 4 seconds and remaining time 196 seconds and will finish at 2024-02-08 22:05:47 EM iterations. 21 out of 999 with lapsed time 4 seconds and remaining time 186 seconds and will finish at 2024-02-08 22:05:37 EM iterations. 22 out of 999 with lapsed time 4 seconds and remaining time 178 seconds and will finish at 2024-02-08 22:05:29 EM iterations. 23 out of 999 with lapsed time 4 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:05:21 EM iterations. 24 out of 999 with lapsed time 4 seconds and remaining time 162 seconds and will finish at 2024-02-08 22:05:13 EM iterations. 25 out of 999 with lapsed time 4 seconds and remaining time 156 seconds and will finish at 2024-02-08 22:05:07 EM iterations. 26 out of 999 with lapsed time 5 seconds and remaining time 187 seconds and will finish at 2024-02-08 22:05:39 EM iterations. 27 out of 999 with lapsed time 5 seconds and remaining time 180 seconds and will finish at 2024-02-08 22:05:32 EM iterations. 28 out of 999 with lapsed time 5 seconds and remaining time 173 seconds and will finish at 2024-02-08 22:05:25 EM iterations. 29 out of 999 with lapsed time 5 seconds and remaining time 167 seconds and will finish at 2024-02-08 22:05:19 EM iterations. 30 out of 999 with lapsed time 5 seconds and remaining time 162 seconds and will finish at 2024-02-08 22:05:14 EM iterations. 31 out of 999 with lapsed time 6 seconds and remaining time 187 seconds and will finish at 2024-02-08 22:05:40 EM iterations. 32 out of 999 with lapsed time 6 seconds and remaining time 181 seconds and will finish at 2024-02-08 22:05:34 EM iterations. 33 out of 999 with lapsed time 6 seconds and remaining time 176 seconds and will finish at 2024-02-08 22:05:29 EM iterations. 34 out of 999 with lapsed time 6 seconds and remaining time 170 seconds and will finish at 2024-02-08 22:05:23 "],["documenting-the-package-and-building.html", "13 Documenting the package and building", " 13 Documenting the package and building We finish by running commands that will document, build, and install the package. It may also be a good idea to check the package from within this file. litr::document() # &lt;-- use instead of devtools::document() ## ℹ Updating flowtrend documentation ## ℹ Loading flowtrend ## ℹ Re-compiling flowtrend (debug build) ## Warning: [Estep.R:5] @param requires name and description ## Warning: [Estep.R:6] @param requires name and description ## Warning: [Estep.R:7] @param requires name and description ## Warning: [Estep.R:8] @param requires name and description ## Warning: [Estep.R:9] @param requires name and description ## Warning: [Estep.R:10] @param requires name and description ## Warning: [Estep.R:11] @param requires name and description ## Warning: [Estep.R:14] @return requires a value ## Warning: [Mstep_mu.R:9] @param requires name and description ## Warning: [Mstep_mu.R:10] @param requires name and description ## Warning: [Mstep_mu.R:11] @param requires name and description ## Warning: [Mstep_mu.R:12] @param requires name and description ## Warning: [Mstep_mu.R:13] @param requires name and description ## Warning: [Mstep_mu.R:14] @param requires name and description ## Warning: [Mstep_mu.R:15] @param requires name and description ## Warning: [Mstep_mu.R:16] @param requires name and description ## Warning: [Mstep_mu.R:17] @param requires name and description ## Warning: [Mstep_mu.R:18] @param requires name and description ## Warning: [Mstep_mu.R:19] @param requires name and description ## Warning: [Mstep_mu.R:20] @param requires name and description ## Warning: [Mstep_mu.R:21] @param requires name and description ## Warning: [Mstep_mu.R:22] @param requires name and description ## Warning: [Mstep_mu.R:23] @param requires name and description ## Warning: [Mstep_mu.R:24] @param requires name and description ## Warning: [Mstep_mu.R:25] @param requires name and description ## Warning: [Mstep_mu.R:26] @param requires name and description ## Warning: [Mstep_mu.R:27] @param requires name and description ## Warning: [Mstep_mu.R:28] @param requires name and description ## Warning: [Mstep_mu.R:29] @param requires name and description ## Warning: [Mstep_mu.R:30] @param requires name and description ## Warning: [Mstep_mu.R:33] @return requires a value ## Warning: [Mstep_mu.R:36] @examples requires a value ## Warning: [Mstep_mu_cvxr.R:4] @param requires name and description ## Warning: [Mstep_mu_cvxr.R:5] @param requires name and description ## Warning: [Mstep_mu_cvxr.R:6] @param requires name and description ## Warning: [Mstep_mu_cvxr.R:7] @param requires name and description ## Warning: [Mstep_sigma.R:13] @examples requires a value ## Warning: [flowtrend.R:8] @return requires a value ## Warning: [flowtrend.R:11] @examples requires a value ## Warning: [flowtrend_once.R:22] @param requires name and description ## Warning: [flowtrend_once.R:23] @param requires name and description ## Warning: [flowtrend_once.R:24] @param requires name and description ## Warning: [flowtrend_once.R:25] @param requires name and description ## Warning: [flowtrend_once.R:31] @examples requires a value ## Warning: [gen_diff_mat.R:12] @examples requires a value ## Warning: [la_admm_oneclust.R:5] @param requires name and description ## Warning: [la_admm_oneclust.R:8] @return requires a value ## Warning: [la_admm_oneclust.R:11] @examples requires a value ## Warning: [make_cvscore_filename.R:4] @param requires name and description ## Warning: [make_cvscore_filename.R:5] @param requires name and description ## Warning: [make_cvscore_filename.R:6] @param requires name and description ## Warning: [make_cvscore_filename.R:16] @param requires name and description ## Warning: [make_cvscore_filename.R:17] @param requires name and description ## Warning: [make_cvscore_filename.R:18] @param requires name and description ## Warning: [make_cvscore_filename.R:30] @param requires name and description ## Warning: [make_cvscore_filename.R:31] @param requires name and description ## Warning: [make_cvscore_filename.R:42] @param requires name and description ## Warning: [objective.R:5] @param requires name and description ## Warning: [objective.R:6] @param requires name and description ## Warning: [objective.R:7] @param requires name and description ## Warning: [objective.R:8] @param requires name and description ## Warning: [objective.R:9] @param requires name and description ## Warning: [objective.R:10] @param requires name and description ## Warning: [objective.R:11] @param requires name and description ## Warning: [objective.R:12] @param requires name and description ## Warning: [objective.R:13] @param requires name and description ## Warning: [objective.R:14] @param requires name and description ## Warning: [objective.R:15] @param requires name and description ## Warning: [objective.R:16] @param requires name and description ## Warning: [objective.R:17] @param requires name and description ## Warning: [objective.R:18] @param requires name and description ## Warning: [objective.R:19] @param requires name and description ## Warning: [objective.R:22] @return requires a value ## Warning: [objective.R:25] @examples requires a value ## Writing &#39;NAMESPACE&#39; ## Warning: Skipping &#39;U_update_Z.Rd&#39; ## ℹ File lacks name and/or title ## Warning: Skipping &#39;U_update_W.Rd&#39; ## ℹ File lacks name and/or title ## Warning: Skipping &#39;etilde_mat.Rd&#39; ## ℹ File lacks name and/or title ## Warning: Skipping &#39;myschur.Rd&#39; ## ℹ File lacks name and/or title ## Writing &#39;NAMESPACE&#39; ## Writing &#39;Estep.Rd&#39; ## Writing &#39;Mstep_mu.Rd&#39; ## Writing &#39;Mstep_mu_cvxr.Rd&#39; ## Writing &#39;Mstep_prob.Rd&#39; ## Writing &#39;Mstep_sigma.Rd&#39; ## Writing &#39;matrix_function_solve_triangular_sylvester_barebonesC2.Rd&#39; ## Writing &#39;W_update_fused.Rd&#39; ## Writing &#39;admm_oneclust.Rd&#39; ## Writing &#39;aug_lagr.Rd&#39; ## Writing &#39;calc_max_lambda.Rd&#39; ## Writing &#39;check_converge.Rd&#39; ## Writing &#39;check_converge_rel.Rd&#39; ## Writing &#39;cv_aggregate.Rd&#39; ## Writing &#39;cv_aggregate_res.Rd&#39; ## Writing &#39;cv_flowtrend.Rd&#39; ## Writing &#39;cv_makebest.Rd&#39; ## Writing &#39;load_all_objectives.Rd&#39; ## Writing &#39;load_all_refit_objectives.Rd&#39; ## Writing &#39;keep_only_best.Rd&#39; ## Writing &#39;keep_only_best_refit.Rd&#39; ## Writing &#39;cv_summary.Rd&#39; ## Writing &#39;dt2ylist.Rd&#39; ## Writing &#39;flowtrend-package.Rd&#39; ## Writing &#39;flowtrend.Rd&#39; ## Writing &#39;flowtrend_once.Rd&#39; ## Writing &#39;gen_diff_mat.Rd&#39; ## Writing &#39;gen_tf_mat.Rd&#39; ## Writing &#39;gen_tf_mat_equalspace.Rd&#39; ## Writing &#39;gendat_1d.Rd&#39; ## Writing &#39;gendat_2d.Rd&#39; ## Writing &#39;get_best_match_from_kl.Rd&#39; ## Writing &#39;form_symmetric_kl_distmat.Rd&#39; ## Writing &#39;symmetric_kl.Rd&#39; ## Writing &#39;get_max_lambda.Rd&#39; ## Writing &#39;init_mn.Rd&#39; ## Writing &#39;init_sigma.Rd&#39; ## Writing &#39;interpolate_mn.Rd&#39; ## Writing &#39;interpolate_prob.Rd&#39; ## Writing &#39;la_admm_oneclust.Rd&#39; ## Writing &#39;loglik_tt.Rd&#39; ## Writing &#39;logspace.Rd&#39; ## Writing &#39;make_cv_folds.Rd&#39; ## Writing &#39;make_cvscore_filename.Rd&#39; ## Writing &#39;make_best_cvscore_filename.Rd&#39; ## Writing &#39;make_refit_filename.Rd&#39; ## Writing &#39;make_best_refit_filename.Rd&#39; ## Writing &#39;make_iimat.Rd&#39; ## Writing &#39;make_iimat_small.Rd&#39; ## Writing &#39;objective.Rd&#39; ## Writing &#39;objective_per_cluster.Rd&#39; ## Writing &#39;one_job.Rd&#39; ## Writing &#39;one_job_refit.Rd&#39; ## Writing &#39;plot_1d.Rd&#39; ## Writing &#39;plot_2d.Rd&#39; ## Writing &#39;plot_prob.Rd&#39; ## Writing &#39;predict_flowtrend.Rd&#39; ## Writing &#39;print_progress.Rd&#39; ## Writing &#39;reorder_clust.Rd&#39; ## Writing &#39;reorder_kl.Rd&#39; ## Writing &#39;softmax.Rd&#39; "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
