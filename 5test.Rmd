# Testing the `flowsmooth()` method

We're going to take a huge leap, and assume the `flowsmooth()` function has been
built. We're going to test it now.

```{r, eval = FALSE}
library(tidyverse)
ll <- function(){
  litr::render("index.Rmd", output_format = litr::litr_gitbook())
  la('flowsmooth')
  devtools::load_all("~/repos/FlowTF")
}
ll()
```

## 1d example

```{r 1d-example, eval = FALSE}
devtools::load_all("~/repos/FlowTF")
## Generate data
set.seed(100)
dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.45)
dt_model <- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE)
ylist = dt %>% dt2ylist()
x = dt %>% pull(time) %>% unique()

## Fit model
set.seed(0)
obj <- flowsmooth(ylist = ylist,
                  x = x,
                  maxdev = 5,
                  numclust = 3,
                  lambda = 0.02,
                  l = 1,
                  l_prob = 2,
                  lambda_prob = .005,
                  ## nrestart = 3)
                  nrestart = 1)

## Also reorder the cluster labels of the truth, to match the fitted model.
ord = obj$mn[,1,] %>% colSums() %>% order(decreasing=TRUE)
lookup <- setNames(c(1:obj$numclust), ord)
dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %>% as.factor()

## Reorder the cluster lables of the fitted model.
obj = reorder_clust(obj)
```

The objective value (that is, the penalized log likelihood) should be monotone
across EM algorithm iterations.

```{r, eval=FALSE}
testthat::test_that("Objective value decreases over EM iterations.",{
  
  devtools::load_all("~/repos/FlowTF")
  for(iseed in 1:5){
    
    ## Generate increasingly noisy data
    set.seed(iseed*100)
    dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.2)
    dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2)
    ylist = dt %>% dt2ylist()
    x = dt %>% pull(time) %>% unique()
    
    ## Fit model
    set.seed(0)
    obj <- flowsmooth(ylist = ylist,
                      x = x,
                      maxdev = 5,
                      numclust = 3,
                      lambda = 0.02,
                      l = 1,
                      l_prob = 2,
                      ## lambda_prob = .5,
                      lambda_prob = 0.05,
                      nrestart = 1)
    
    ## Test objective monotonicity
    testthat::expect_true(all(diff(obj$objective) < 0))
  }
})
```

While the slight rise in objective value is not egregious, I would like to get
to the bottom of this. The next code block shows a self-contained example of the
objective value rising. I didn't stop the algorithm (allowing the full
`niter=200` iterations) to see if it increases. Now I wonder -- is it due to
glmnet? If we use CVXR, will it go away?

```{r, eval = FALSE}
iseed = 1
set.seed(iseed * 100)
dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.2)
dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2)
ylist = dt %>% dt2ylist()
x = dt %>% pull(time) %>% unique()

## Fit model
set.seed(0)
obj <- flowsmooth(ylist = ylist,
                  x = x,
                  maxdev = 5,
                  numclust = 3,
                  lambda = 0.02,
                  l = 1,
                  l_prob = 2,
                  lambda_prob = 0.05,
                  nrestart = 1,
                  niter = 200,
                  verbose = TRUE) ## TODO: remember to comment out the convergence check!!

objectives = obj$objective
for(iter in 2:200){
  if(check_converge_rel(objectives[iter-1], objectives[iter], tol = 1E-4)) break
}
stopping_iter = iter

print(range(diff(obj$objective)))

plot(obj$objective, type ='l', xlim = c(0,50)) ## Ok did it rise?
abline(v=iter, lty = 'dotted')
abline(h=min(obj$objective), lty = 'dotted', col = 'blue')

plot(diff(obj$objective), type ='l', ylim = c(-0.001, 0.0005)) ## Ok did it rise?
abline(h=0, col = 'blue', lty = 'dotted')
```

The data and estimated model are shown here.

```{r 1d-example-plot-mean, eval = FALSE, fig.width = 7, fig.height = 5}
plot_tf_1d(ylist, obj, x = x) +
  geom_line(aes(x = time, y = mean, group = cluster),
            data = dt_model,## %>% subset(time %ni% held_out),
            linetype = "dashed", size=2, alpha = .7)
```

The estimated probabilities are shown here.

```{r 1d-example-plot-prob, eval = FALSE, fig.width = 7, fig.height = 5}
plot_tf_prob(obj) +
  geom_line(aes(x = time, y = prob, group = cluster, color = cluster),
            data = dt_model, linetype = "dashed")  +
  facet_wrap(~cluster)
```


## 1d example with gap

Repeating this exercise with data that has a gap (between times 25 and 35).

```{r 1d-example-with-gap, eval = FALSE}
held_out = 25:35
dt_subset = dt %>% subset(time %ni% held_out)
ylist = dt_subset %>% dt2ylist()
x = dt_subset %>% pull(time) %>% unique()

## Fit model
set.seed(0)
obj <- flowsmooth(ylist = ylist,
                  x = x,
                  maxdev = 5,
                  numclust = 3,
                  lambda = 0.02,
                  l = 1,
                  l_prob = 2,
                  lambda_prob = .005,
                  nrestart = 2)

## Also reorder the cluster labels of the truth, to match the fitted model.
ord = obj$mn[,1,] %>% colSums() %>% order(decreasing=TRUE)
lookup <- setNames(c(1:obj$numclust), ord)
dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %>% as.factor()

## Reorder the cluster lables of the fitted model.
obj = reorder_clust(obj)
```

The model estimates (the solid colored lines) at the gap (between 25 and 35) are
automatically generated by `ggplot::geom_line()`, and no model estiamtes are
made here. But actually, this is what we want to do when making predictions at
new time points. More about this shortly.


```{r 1d-example-gap-plot, eval = FALSE, fig.width = 7, fig.height = 5}
plot_tf_1d(ylist, obj, x = x) +
  geom_line(aes(x = time, y = mean, group = cluster),
            data = dt_model,## %>% subset(time %ni% held_out),
            linetype = "dashed", size=2, alpha = .7)
```

## Predicting and evaluating on new time points

first, let's write a couple of functions `interpolate_mn()` and
`interpolate_prob()` which do a linear interpolation of the means and
probabilities at new time points.

```{r interpolate_mn}
#' Do a linear interpolation of the cluster means.
#'
#' @param x Training times.
#' @param tt Prediction time.
#' @param iclust Cluster number.
#' @param mn length(x) by dimdat by numclust matrix.
#'
#' @return A dimdat-length vector.
interpolate_mn <- function(x, tt, iclust, mn){

  ## Basic checks
  stopifnot(length(x) == dim(mn)[1])
  stopifnot(iclust <= dim(mn)[3])
  if(tt %in% x) return(mn[which(x==tt),,iclust,drop=TRUE])

  ## Set up for linear interpolation
  floor_t <- max(x[which(x <= tt)])
  ceiling_t <- min(x[which(x >= tt)])
  floor_t_ind <- which(x == floor_t)
  ceiling_t_ind <- which(x == ceiling_t)

  ## Do the linear interpolation
  mn_t <-
    mn[ceiling_t_ind,,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) +
    mn[floor_t_ind,,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t

  ## Basic checks
  stopifnot(length(mn_t) == dim(mn)[2])

  return(mn_t)
}
```

```{r interpolate_prob}
#' Do a linear interpolation of the cluster means.
#'
#' @param x Training times.
#' @param tt Prediction time.
#' @param iclust Cluster number.
#' @param prob length(x) by numclust array or matrix.
#'
#' @return One probability.
interpolate_prob <- function(x, tt, iclust, prob){

  ## Basic checks
  numdat = dim(prob)[1]
  numclust = dim(prob)[2]
  stopifnot(length(x) == numdat)
  stopifnot(iclust <= numclust)
  if(tt %in% x) return(prob[which(x == tt),iclust,drop=TRUE])

  ## Set up for linear interpolation
  floor_t <- max(x[which(x <= tt)])
  ceiling_t <- min(x[which(x >= tt)])
  floor_t_ind <- which(x == floor_t)
  ceiling_t_ind <- which(x == ceiling_t)

  ## Do the linear interpolation
  prob_t <-
    prob[ceiling_t_ind,iclust,drop=TRUE]*(ceiling_t - tt)/(ceiling_t - floor_t) +
    prob[floor_t_ind,iclust,drop=TRUE]*(tt - floor_t)/(ceiling_t - floor_t) # linear interpolation between floor_t and ceiling_t

  ## Basic checks
  stopifnot(length(prob_t) == 1)
  stopifnot(0 <= prob_t & prob_t <= 1)

  return(prob_t)
}
```

Next, let's build a prediction function `predict_flowsmooth()` which takes the
model object `obj`, and the new time points `newtimes`, and produces.

```{r predict_flowsmooth}
#' Prediction: Given new timepoints in the original time interval,generate a set
#' of means and probs (and return the same Sigma).
#'
#' @param obj Object returned from covariate EM flowmix().
#' @param newtimes New times at which to make predictions.
#'
#' @return List containing mean, prob, and sigma, and x.
#'
#' @export
#'
predict_flowsmooth <- function(obj, newtimes = NULL){

  ## Check the dimensions
  newx <- newtimes
  if(is.null(newtimes)){ newx = obj$x }

  ## Check if the new times are within the time range of the original data (why is this important)?
  if(FALSE) stopifnot(all(sapply(newx, FUN = function(t) t >= min(obj$x) & t <= max(obj$x))))

  ## Setup some things
  x <- obj$x
  TT_new = length(newx)
  numclust = obj$numclust
  dimdat = obj$dimdat

  ## Predict the means (manually).
  newmn_array = array(NA, dim = c(TT_new, dimdat, numclust))
  for(iclust in 1:numclust){
    newmn_oneclust <- lapply(newx, function(tt){
      interpolate_mn(x, tt, iclust, obj$mn)
    }) %>% do.call(rbind, . )
    newmn_array[,,iclust] = newmn_oneclust
  }

  ## Predict the probs.
  newprob = array(NA, dim = c(TT_new, numclust))
  for(iclust in 1:numclust){
    newprob_oneclust <- lapply(newx, function(tt){
      interpolate_prob(x, tt, iclust, obj$prob)
    }) %>% do.call(c, .)
    newprob[,iclust] = newprob_oneclust
  }

  ## Basic checks
  stopifnot(all(dim(newprob) == c(TT_new,numclust)))
  stopifnot(all(newprob >= 0))
  stopifnot(all(newprob <= 1))

  ## Return the predictions
  return(list(mn = newmn_array,
              prob = newprob,
              sigma = obj$sigma,
              x = newx))
}
```

Here's a quick test (no new data) to make sure this function returns a list
containing: the mean, probability, covariance, and new times.

```{r, eval = FALSE}
testthat::test_that("The prediction function returns the right things", {
  ## Generate data
  ## ll()
  ## devtools::load_all('~/repos/FlowTF')
  set.seed(100)
  dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.45)
  ylist = dt %>% dt2ylist()
  x = dt %>% pull(time) %>% unique()
  obj <- flowsmooth(ylist = ylist,
                    x = x,
                    maxdev = 5,
                    numclust = 3,
                    lambda = 0.02,
                    l = 1,
                    l_prob = 2,
                    lambda_prob = .005, ## 
                    nrestart = 1,
                    niter = 3)

  predobj = predict_flowsmooth(obj)
  testthat::expect_named(predobj, c("mn", "prob", "sigma", "x"))
})
```

Now, try to make predictions at new points..

```{r test-predict_flowsmooth, eval = FALSE}
testthat::test_that("prediction function returns the right things", {
  ## Generate data
  ## ll()
  ## devtools::load_all('~/repos/FlowTF')
  set.seed(100)
  dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.45)
  held_out = 25:35
  dt_subset = dt %>% subset(time %ni% held_out)
  ylist = dt_subset %>% dt2ylist()
  x = dt_subset %>% pull(time) %>% unique()
  obj <- flowsmooth(ylist = ylist,
                    x = x,
                    maxdev = 5,
                    numclust = 3,
                    lambda = 0.02,
                    l = 1,
                    l_prob = 2,
                    lambda_prob = .005, ## 
                    nrestart = 1,
   

  predobj = predict_flowsmooth(obj, newtimes = held_out)

  ## Check a few things
  testthat::expect_equal(predobj$x,  held_out)
  testthat::expect_equal(rowSums(predobj$prob),  rep(1, length(held_out)))
  testthat::expect_equal(dim(predobj$mn),  c(length(held_out), 1, 3))
})
```

## Objective value


## Evaluating data fit (by likelihood) in an out-of-sample measurement. 

```{r predict-example eval = FALSE}
predobj = predict_flowsmooth(obj, newtimes = held_out)
predobj
```


## 1d example with ends cut off

```{r, eval = FALSE}
set.seed(100)
dt <-       gendat_1d(TT, ntlist, die_off_time = 0.45)
dt1 = dt %>% subset(time <= 50)
dt2 = dt %>% subset(time > 50)
```




## 1d example with cross-validation

Thoroughly test the selected lambda.
