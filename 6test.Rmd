# Testing the flowsmooth method

We're going to take a huge leap, and assume the `flowsmooth()` function has been
built. We're going to test it now.

```{r temp-block, eval = FALSE}
library(tidyverse)
devtools::load_all("~/repos/FlowTF")
litr::load_all("index.Rmd")
```

## 1d example {#1d-example}

```{r 1d-example, eval = TRUE}
## Generate data
set.seed(100)
dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.45)
dt_model <- gendat_1d(100, rep(100, 100), die_off_time = 0.45, return_model = TRUE)
ylist = dt %>% dt2ylist()
x = dt %>% pull(time) %>% unique()

## Fit model
set.seed(18)
obj <- flowsmooth(ylist = ylist,
                  x = x,
                  maxdev = 5,
                  numclust = 3,
                  lambda = 0.02,
                  l = 1,
                  l_prob = 2,
                  lambda_prob = .005,
                  nrestart = 10)

## Also reorder the cluster labels of the truth, to match the fitted model.
ord = obj$mn[,1,] %>% colSums() %>% order(decreasing=TRUE)
lookup <- setNames(c(1:obj$numclust), ord)
dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %>% as.factor()

## Reorder the cluster lables of the fitted model.
obj = reorder_clust(obj)
```

The objective value (that is, the penalized log likelihood) should be monotone
across EM algorithm iterations.

```{r test-objective, eval = FALSE}
testthat::test_that("Objective value decreases over EM iterations.",{
  
  devtools::load_all("~/repos/FlowTF")
  for(iseed in 1:5){
    
    ## Generate increasingly noisy data
    set.seed(iseed*100)
    dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.2)
    dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2)
    ylist = dt %>% dt2ylist()
    x = dt %>% pull(time) %>% unique()
    
    ## Fit model
    set.seed(0)
    obj <- flowsmooth(ylist = ylist,
                      x = x,
                      maxdev = 5,
                      numclust = 3,
                      lambda = 0.02,
                      l = 1,
                      l_prob = 2,
                      ## lambda_prob = .5,
                      lambda_prob = 0.05,
                      nrestart = 1)
    
    ## Test objective monotonicity
    testthat::expect_true(all(diff(obj$objective) < 0))
  }
})
```

(TODO: resolve the issue of the objective value sometimes rising!)

While the slight rise in objective value is not egregious, I would like to get
to the bottom of this. The next code block shows a self-contained example of the
objective value rising. I didn't stop the algorithm (allowing the full
`niter=200` iterations) to see if it increases. Now I wonder -- is it due to
glmnet? If we use CVXR, will it go away?

```{r test-objective-rise, eval = FALSE}
iseed = 1
set.seed(iseed * 100)
dt       <- gendat_1d(100, rep(100, 100), die_off_time = 0.2)
dt$Y = dt$Y + rnorm(nrow(dt), 0, iseed/2)
ylist = dt %>% dt2ylist()
x = dt %>% pull(time) %>% unique()

## Fit model
set.seed(0)
obj <- flowsmooth(ylist = ylist,
                  x = x,
                  maxdev = 5,
                  numclust = 3,
                  lambda = 0.02,
                  l = 1,
                  l_prob = 2,
                  lambda_prob = 0.05,
                  nrestart = 1,
                  niter = 200,
                  verbose = TRUE) ## TODO: remember to comment out the convergence check!!

objectives = obj$objective
for(iter in 2:200){
  if(check_converge_rel(objectives[iter-1], objectives[iter], tol = 1E-4)) break
}
stopping_iter = iter

print(range(diff(obj$objective)))

plot(obj$objective, type ='l', xlim = c(0,50))
abline(v=iter, lty = 'dotted')
abline(h=min(obj$objective), lty = 'dotted', col = 'blue')

plot(diff(obj$objective), type ='l', ylim = c(-0.001, 0.0005)) 
abline(h=0, col = 'blue', lty = 'dotted')
```

The data and estimated model are shown here.

```{r 1d-example-plot-mean, eval = TRUE, fig.width = 7, fig.height = 5}
plot_1d(ylist, obj, x = x) +
  geom_line(aes(x = time, y = mean, group = cluster),
            data = dt_model,## %>% subset(time %ni% held_out),
            linetype = "dashed", size=2, alpha = .7)
```

The estimated probabilities are shown here.

```{r 1d-example-plot-prob, eval = TRUE, fig.width = 7, fig.height = 5}
plot_prob(obj) +
  geom_line(aes(x = time, y = prob, group = cluster, color = cluster),
            data = dt_model, linetype = "dashed")  +
  facet_wrap(~cluster)
```


## 1d example with gap

Repeating this exercise with data that has a gap (between times 25 and 35).

```{r 1d-example-with-gap, eval = FALSE}
held_out = 25:35
dt_subset = dt %>% subset(time %ni% held_out)
ylist = dt_subset %>% dt2ylist()
x = dt_subset %>% pull(time) %>% unique()

## Fit model
set.seed(0)
obj <- flowsmooth(ylist = ylist,
                  x = x,
                  maxdev = 5,
                  numclust = 3,
                  lambda = 0.02,
                  l = 1,
                  l_prob = 2,
                  lambda_prob = .005,
                  nrestart = 2)

## Also reorder the cluster labels of the truth, to match the fitted model.
ord = obj$mn[,1,] %>% colSums() %>% order(decreasing=TRUE)
lookup <- setNames(c(1:obj$numclust), ord)
dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %>% as.factor()

## Reorder the cluster lables of the fitted model.
obj = reorder_clust(obj)
```

No model estimates are being made at the gap (between 25 and 35)! The model
estimates (the solid colored lines) are automatically generated by
`ggplot::geom_line()`. But actually, linear interpolation is what we'll want to
do when making predictions at new time points. There's more about this shortly.

TODO: This blob is a bit outdated. We've already built the `predict_flowsmooth()` function.

```{r 1d-example-gap-plot, eval = TRUE, fig.width = 7, fig.height = 5}
plot_1d(ylist, obj, x = x) +
  geom_line(aes(x = time, y = mean, group = cluster),
            data = dt_model,## %>% subset(time %ni% held_out),
            linetype = "dashed", size=2, alpha = .7)
```


# 2d to 1d


```{r}
litr::load_all("~/repos/flowsmooth/index.Rmd")
set.seed(100)
dt       <- gendat_2d(100, rep(100, 100))
ylist = dt$ylist

## collapse to 1d
ylist_1d = lapply(ylist, function(a) a[,1, drop = FALSE])
x = 1:length(ylist_1d)

## Data
plot_1d(ylist_1d)

## Fit model
set.seed(100)
obj = flowsmooth(ylist = ylist, numclust = 3, l = 2, l_prob = 2,
                 lambda = .001, lambda_prob = .0001,  
                 verbose = TRUE)

# Also reorder the cluster labels of the truth, to match the fitted model.
ord = obj$mn[,1,] %>% colSums() %>% order(decreasing=TRUE)
lookup <- setNames(c(1:obj$numclust), ord)
dt_model$cluster = lookup[as.numeric(dt_model$cluster)] %>% as.factor()

## Reorder the cluster lables of the fitted model.
obj = reorder_clust(obj)

## Show the estimated model overlaid with data
mns = dt$means %>% .[,,1] %>% reshape2::melt() 
colnames(mns) = c("time", "cluster", "mean")
mns = mns %>% as_tibble()
plot_1d(ylist_1d, obj, add_point = FALSE) +
  geom_line(aes(x = time, y = mean, group = cluster),
            data = mns, size=3) +
  facet_wrap(~cluster)

## Plot the probabilities
trueprobs = dt$probs
colnames(trueprobs) = 1:3##paste0("clust", 1:3)
trueprobs = trueprobs %>% as_tibble() %>% add_column(time = 1:nrow(trueprobs))
trueprobs = trueprobs %>% pivot_longer(-time, names_to = "cluster", values_to = "prob")

## Compare the true probabilities
plot_prob(obj) + facet_wrap(~cluster) +
  geom_line(aes(x = time, y = prob), data = trueprobs)##, alpha = .5, linetype = "dashed") +
```

Notes: look into literature for how to do diagnostics of misidentified mixture
model.
